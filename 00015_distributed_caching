Caching solves:

Avoid network calls
Avoid repeated computataions
Avoid DB load

Why cant store everything in cache?
hardware of cache is SSD costly
also if store tons a data , search time will increase why dont we go and hit the database.


So we need to keep only the relevant data in cache
to do so , we need cache policy to load and evict data
And cache performance entirely depends on cache policy

eviction policy
LRU
LFU

Poor  eviction policy increases the DB calls
With small cache , cache Thrashing: you are inputting and outputting the elements without using it, it is hurtful situation for cache.

cache write policy

write through => write to cache and then update DB at the same time 
write back => write to database first and then write to cache
hybrid of write through and write back => write to cache then update DB in blocks ensuring network failure also so retrying or kafka or WAL
in case of one server failure => also write to cache of other servers to ensure data consistency

for write through -> might have cache invalidated in other servers

so can keep global cache

write back is expensive

so choose hybrid approach:

write through and later on save network calls in blocks



Cache write policies define how data is written to main memory and the cache when a write operation occurs. The choice of policy affects performance, data consistency, and cache coherency.

üîÅ Major Cache Write Policies
1. Write-Through
‚úÖ Data is written to both the cache and main memory simultaneously.

Ensures data consistency between cache and memory.

Slower due to frequent memory writes.

Pros:

Simpler and consistent.

Memory is always up to date.

Cons:

Slower performance due to main memory write latency.

2. Write-Back (Copy-Back)
‚úÖ Data is written only to cache.

‚ùó Modified cache blocks are written to main memory only when evicted.

Requires a "dirty bit" to track modified blocks.

Pros:

Faster, reduces memory writes.

Good for repeated writes to the same block.

Cons:

More complex.

Risk of data loss on power failure.

Memory may be stale until the dirty block is flushed.

3. Write-Once (Hybrid)
First write: write-through.

Subsequent writes: write-back.

Tries to combine the benefits of both.

üß≠ Cache Write Allocation Policies
These determine what happens on a write miss:

A. Write Allocate (Fetch on Write)
On a write miss, the block is loaded into cache, and then the write is performed.

Usually used with write-back caches.

B. No-Write Allocate (Write-No-Allocate)
On a write miss, data is written directly to main memory, not loaded into the cache.

Typically used with write-through caches.

üîÑ Summary Table


| Write Policy  | Memory Updated on Write | Cache Updated on Write | Common Pairing                      |
| ------------- | ----------------------- | ---------------------- | ----------------------------------- |
| Write-Through | ‚úÖ Yes                   | ‚úÖ Yes                  | No-Write Allocate or Write Allocate |
| Write-Back    | ‚ùå Only on eviction      | ‚úÖ Yes                  | Write Allocate                      |
| Write-Once    | Mixed                   | Mixed                  | Hybrid                              |


The write-around policy is a cache write policy designed to avoid polluting the cache with data that is not frequently accessed.

üîÅ Write-Around Policy ‚Äì Definition
On a write miss, data is written directly to main memory, not loaded into cache.

On a write hit, data is written to cache and memory (if combined with write-through).

üß† Key Characteristics
Feature	Behavior
Write Hit	Write to cache (may also update memory)
Write Miss	Bypass cache, write directly to memory
Cache Allocation	‚ùå No caching on write miss
Common Pairing	Often used with write-through cache
Goal	Avoid caching transient or rarely used data

‚úÖ Pros
Reduces cache pollution with write-only or rarely used data.

Saves cache space for frequently read data.

Can improve cache hit rate for read-intensive workloads.

‚ùå Cons
On future read, data might not be in the cache ‚Üí causes read miss.

Slightly more complex to implement.

üß≠ Comparison with Other Policies

| Policy            | Write Miss Behavior               | Data Cached? |
| ----------------- | --------------------------------- | ------------ |
| **Write-Through** | Write to cache + memory           | ‚úÖ Yes        |
| **Write-Back**    | Write to cache only, defer memory | ‚úÖ Yes        |
| **Write-Around**  | Write to memory only              | ‚ùå No         |


üìå When to Use Write-Around
When write operations are infrequent, or

When written data is unlikely to be read soon (e.g., logging, telemetry),

Useful in read-heavy systems where cache space is at a premium.


# Replacement Policies:

LRU , LFU, 

Cache replacement policies determine which cache block to evict when a new block needs to be loaded and the cache is already full. Choosing the right policy is crucial for maximizing cache hit rate and performance.



üîÅ Common Cache Replacement Policies
1. LRU (Least Recently Used)
Evicts the least recently accessed block.

Assumes that recently used data is likely to be reused soon.

Pros: Good for general-purpose workloads.
Cons: Requires tracking usage history (can be costly in hardware).

2. FIFO (First-In, First-Out)
Evicts the oldest loaded block, regardless of usage.

Pros: Simple to implement.
Cons: May evict frequently used data if it was loaded early.

3. LFU (Least Frequently Used)
Evicts the block with the fewest accesses over time.

Pros: Works well for highly skewed access patterns.
Cons: Requires access counters; poor with changing access patterns (can be solved using aging techniques).

4. Random Replacement
Evicts a randomly selected block.

Pros: Very simple, sometimes surprisingly effective.
Cons: Doesn't adapt to access patterns.

5. MRU (Most Recently Used)
Evicts the most recently accessed block.

Pros: Useful in specific workloads (e.g., some stack-based access patterns).
Cons: Usually worse than LRU for general workloads.

6. Clock (Second-Chance) Algorithm
Approximation of LRU using a circular buffer and a "reference bit".

Gives blocks a second chance if recently used.

Pros: Efficient in hardware, commonly used in virtual memory.
Cons: Not perfectly accurate like LRU.



| Policy     | Tracks Usage    | Complexity | Adaptivity | Notes                           |
| ---------- | --------------- | ---------- | ---------- | ------------------------------- |
| **LRU**    | Recent use      | Moderate   | High       | Common and effective            |
| **FIFO**   | Insertion order | Low        | Low        | Simple but may evict hot data   |
| **LFU**    | Frequency       | High       | Medium     | Good for stable access patterns |
| **Random** | None            | Very low   | None       | Low overhead                    |
| **MRU**    | Recent use      | Moderate   | Low        | Useful for LIFO-like patterns   |
| **Clock**  | Reference bit   | Low        | Medium     | Used in OS page replacement     |


üß† When to Use Which
General-purpose CPUs: LRU or approximations like pseudo-LRU or Clock.

Embedded systems: FIFO or Random (due to simplicity).

Workloads with strong reuse: LFU (with aging).

Virtual memory/page replacement: Clock algorithm.

Would you like C++/Java code to simulate these or visualize their behavior on a sequence of memory accesses?

