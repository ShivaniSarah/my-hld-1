######   The Future of Data Systems


CHAPTER 12 The Future of Data Systems

The chapter begins with a quotation from St. Thomas Aquinas, which explains that if something is designed for a higher purpose, its ultimate goal cannot just be its own preservation. For example:

A captain‚Äôs role is not just to preserve the ship but to use it for navigation.

Similarly, in data systems, the goal should not merely be to maintain existing systems but to use them in ways that achieve higher purposes (e.g., enabling better applications).

This chapter shifts from describing current systems (covered in earlier chapters) to thinking about the future‚Äîhow systems should be designed. The author presents personal opinions and speculations about future improvements in how we build applications.

The central idea: applications of the future should be robust, correct, evolvable, and beneficial to humanity.

The main themes of the book (reliability, scalability, maintainability) will now be brought together to imagine better systems.



######   Data Integration


A recurring idea throughout the book is that there are always multiple possible solutions to any given problem, each with its own trade-offs.

Examples:

Storage engines (Chapter 3): log-structured storage, B-trees, column-oriented storage.

Replication (Chapter 5): single-leader, multi-leader, leaderless.

üëâ The key insight:

There is no single correct answer (e.g., ‚Äúthe best way to store and retrieve data‚Äù).

Different approaches are appropriate for different circumstances.

Software implementations usually pick one path, since making every approach work well in one tool would result in poor performance.

Thus, the best choice of tool depends on context. Even ‚Äúgeneral-purpose‚Äù databases are optimized for particular usage patterns.

The first challenge, then, is mapping tools to the circumstances where they fit best.

Vendors rarely admit the weak points of their tools.

The book‚Äôs earlier chapters help you ask the right questions to evaluate trade-offs.

However, even if you perfectly understand the mapping, another problem arises:

In complex applications, data is used in many different ways.

One tool is rarely sufficient for all use cases.

As a result, applications often have to integrate multiple tools to achieve their goals.


######   Combining Specialized Tools by Deriving Data


A common real-world example:

You may need an OLTP (Online Transaction Processing) database for transactions.

But you also want full-text search for keyword queries.

Options:

Some databases (like PostgreSQL) provide built-in full-text search, which works for simple needs.

For more advanced needs, you must use specialized tools (e.g., Elasticsearch, Solr).

Why not just use the search index as the main system?

Search indexes are generally not reliable as durable systems of record.

Therefore, many applications must use both:

A database (for durability & transactions).

A search index (for keyword search).

This leads to the data integration problem:

As more tools are added, complexity increases.

Beyond databases and search indexes, you may also need:

Analytics systems (data warehouses, batch/stream processing).

Caches or denormalized objects derived from the original data.

Machine learning systems (classification, ranking, recommendation).

Notification systems (triggered by data changes).

The more specialized tools you combine, the harder it becomes to keep them all in sync.

üëâ Key point: Different people need different features.

Some engineers claim: ‚Äú99% of people only need X‚Äù or ‚Äúnobody really needs X.‚Äù

But such claims often reflect the speaker‚Äôs limited experience rather than the actual diversity of needs.

In reality, the range of possible data use cases is extremely wide.

What seems unnecessary for one developer may be mission-critical for another.

The need for data integration becomes clear only when you look at the entire dataflow of an organization, not just one application in isolation.

‚úÖ In summary:

Future systems must go beyond preserving existing tools; they should integrate specialized tools into robust ecosystems.

No single database or system can meet all needs‚Äîdifferent tools must be combined.

Integration and synchronization across these tools is the real challenge.

The diversity of data use cases means flexible, interoperable systems are essential for the future.



######   Reasoning About Dataflows



When the same data needs to live in multiple storage systems (e.g., a database and a search index), you must carefully decide:

Where data is written first (the system of record).

Which systems derive from which (so dependencies are clear).

How to move data into all the right places in the correct format.

Example:

Write data first to the database.

Use Change Data Capture (CDC) to capture changes from the database.

Apply the same changes to the search index.

If CDC is the only way to update the index ‚Üí index stays consistent with the database.

‚ö†Ô∏è Problem: If the application writes directly to both the database and the index ‚Üí conflicts may occur because the two systems might process writes in different orders ‚Üí permanent inconsistency.

‚úÖ Solution: Funnel all writes through one system that enforces an order, then derive all other systems from it. This follows the state machine replication model using Total Order Broadcast.

Key: The important part is not CDC vs event sourcing, but ensuring a total order of writes.



######   Derived Data Versus Distributed Transactions



Two approaches try to keep multiple systems consistent:

Distributed Transactions

Use locks (Two-Phase Locking, 2PL) to enforce write order.

Use atomic commit (Two-Phase Commit, 2PC) to ensure all changes happen once.

Provide linearizability (e.g., ‚Äúread your own writes‚Äù).

Derived Data (CDC/Event Sourcing)

Use a log to order writes.

Ensure correctness via deterministic retries + idempotence.

Updates are asynchronous, so timing guarantees like ‚Äúread your own writes‚Äù are not automatic.

Comparison:

Distributed transactions = strong guarantees (linearizability) but poor performance/fault tolerance (e.g., XA is slow, brittle).

Log-based derived data = scalable and fault-tolerant, but weaker guarantees.

üëâ Best path forward: log-based derived data (with guidance on handling eventual consistency), but research continues on stronger guarantees.



######   The Limits of Total Ordering



A totally ordered log (like in leader-based replication) works well at small scale. But scaling brings problems:

Single-leader bottleneck: One node must order all events ‚Üí limited throughput.

Geo-distribution: Each datacenter often has its own leader ‚Üí events from different regions have no defined global order.

Microservices: Each service has its own database ‚Üí events across services may not have a defined order.

Offline clients: Clients update local state immediately (even offline) ‚Üí servers and clients may see different event orders.

Formally, total ordering = total order broadcast = consensus.
Problem: Existing consensus algorithms don‚Äôt scale beyond a single node‚Äôs throughput. Designing scalable, geo-distributed consensus is still an open research problem.



######   Ordering Events to Capture Causality


When events are independent, total order isn‚Äôt necessary. But sometimes events have causal dependencies:

Example:

User A unfriends User B.

Then User A posts a rude message about B.

Intent: B should not see the message.

But if the system processes ‚Äúmessage‚Äù before ‚Äúunfriend,‚Äù B may still get the notification ‚Üí incorrect behavior.

This happens because friendship state and messages are in different systems ‚Üí ordering dependency is lost.

Approaches to Handle Causality

Logical Timestamps

Provide a consistent ordering without central coordination.

Still requires handling out-of-order events and passing metadata.

Event Dependencies via IDs

Log the system state the user saw before acting.

Later events reference that state‚Äôs ID ‚Üí makes dependencies explicit.

Conflict Resolution Algorithms

Help when events arrive in unexpected order.

Useful for state resolution, but not for external side effects (like sending notifications).

üëâ Hope: Over time, design patterns will emerge to capture causality efficiently and maintain correct derived states without requiring total order broadcast bottlenecks.

‚úÖ So in short:

Always pick a clear system of record.

Prefer log-based derived systems over distributed transactions for scalability.

Be aware of limits of total ordering (bottlenecks, partitions, geo-distribution).

Handle causal dependencies carefully (timestamps, event references, conflict resolution).



######   Batch and Stream Processing



The main goal of data integration is to ensure data ends up in the right form, in the right places. This involves consuming inputs, transforming, joining, filtering, aggregating, training models, evaluating, and writing to outputs.

Batch and stream processors are the main tools to achieve this.

The outputs of these processes are derived datasets such as:

Search indexes

Materialized views

User recommendations

Aggregate metrics

Key Difference

Batch processing ‚Üí works on datasets of known, finite size.

Stream processing ‚Üí works on unbounded datasets (continuous flow of data).

Even though they share many principles, implementation details differ. But today, those distinctions are blurring:

Apache Spark ‚Üí stream processing on top of a batch engine (via microbatches).

Apache Flink ‚Üí batch processing on top of a streaming engine.

‚ö†Ô∏è Performance may vary. Example: microbatching struggles with hopping/sliding windows.



######   Maintaining Derived State


Batch Processing

Has a functional programming flavor:

Deterministic, pure functions (output depends only on input).

Inputs treated as immutable.

Outputs as append-only.

Stream Processing

Similar, but extends operators with managed, fault-tolerant state.

Why Determinism Matters

Helps with fault tolerance (e.g., idempotence).

Makes dataflows easier to reason about.

Regardless of whether derived data is a search index, model, or cache, it‚Äôs useful to see them as pipelines that transform one dataset into another.

Synchronous vs Asynchronous Maintenance

In principle: derived data could be maintained synchronously (like relational DB updating secondary indexes).

In practice: asynchronous approaches are more robust.

Distributed transactions ‚Üí spread failures (bad).

Asynchrony ‚Üí isolates faults.

Secondary Indexes and Partitions

Secondary indexes often cross partition boundaries.

If term-partitioned index ‚Üí writes go to multiple partitions.

If document-partitioned index ‚Üí reads go to all partitions.

Maintaining asynchronously ‚Üí more reliable and scalable.



######   Reprocessing Data for Application Evolution



Both batch and stream processing are useful:

Stream processing ‚Üí reflects changes quickly (low latency).

Batch processing ‚Üí allows reprocessing historical data for new views.

Benefits of Reprocessing

Supports system evolution (new features, requirements).

Without it, schema changes are very limited (e.g., adding optional fields).

With it, you can restructure datasets entirely to meet new needs.



######   Schema Migrations on Railways (Analogy)



In 19th-century England, railways had different track gauges (distance between rails). Trains of one gauge couldn‚Äôt run on another.

When a standard gauge was chosen (1846), railways had to convert tracks.

Solution: dual gauge/mixed gauge (adding a third rail).

Allowed gradual conversion, trains of both gauges could run.

Once converted, old rail removed.

‚ö†Ô∏è Expensive ‚Üí some nonstandard gauges still exist (e.g., BART in San Francisco).

Lesson for Data Systems

Derived views allow gradual schema evolution.

You don‚Äôt need a sudden switch:

Maintain old and new schema in parallel.

Shift a small number of users to new schema.

Gradually migrate all users.

Drop the old schema later.

‚úÖ Advantages:

Reversible at every step ‚Üí lower risk.

Faster improvement because you‚Äôre more confident in migrating.



######   The Lambda Architecture



When using both batch (for history) and stream (for real-time) processing, the question is: How do you combine them?

Core Idea

Incoming data ‚Üí recorded as immutable events (like event sourcing).

From these ‚Üí derive read-optimized views.

Run two systems in parallel:

Batch system (e.g., Hadoop MapReduce).

Stream system (e.g., Apache Storm).

Workflow

Stream processor ‚Üí quickly produces an approximate update.

Batch processor ‚Üí later produces a corrected version.

Reasoning

Batch is simpler, less buggy.

Stream is harder to make reliable, but gives fast results.

Streams may use approximate algorithms, batch uses exact ones.

Problems with Lambda Architecture

Although influential, it has practical challenges:

Duplicated Logic

Same logic must run in both batch + stream ‚Üí lots of extra effort.

Abstractions (e.g., Summingbird) help, but operational complexity remains.

Merging Outputs

Stream pipeline + batch pipeline produce separate outputs ‚Üí must be merged.

Easy for simple aggregations (tumbling windows).

Hard for joins, sessionization, or non-time-series outputs.

Reprocessing Costs

Reprocessing full historical dataset is expensive.

Often only incremental batches are processed (e.g., hourly).

Raises issues:

Stragglers (late data).

Windows crossing batch boundaries.

Incrementalizing batch processing makes it more like streaming, which defeats the original purpose of keeping batch simple.



######   Unifying Batch and Stream Processing



Traditionally, the Lambda Architecture combined batch processing (for historical data) and stream processing (for real-time events), but it had downsides ‚Äî mainly the complexity of maintaining two separate systems.
More recent systems allow both to be handled together in one system, giving the benefits of Lambda without the drawbacks.

To achieve this, a system needs these key features:

1. Replay of Historical Events

The same engine that processes real-time data must also be able to reprocess historical data.

Example:

Log-based message brokers (like Kafka) allow you to replay old messages.

Some stream processors can also read input from distributed filesystems such as HDFS, meaning they can reprocess stored events in bulk.

2. Exactly-Once Semantics

When faults occur (like crashes or retries), the system should guarantee that results are the same as if no failure had ever happened.

Partial results from failed tasks must be discarded, just like in batch processing.

This ensures correctness and consistency in both batch and streaming modes.

3. Windowing by Event Time

Processing should be based on event time (the time the event actually happened), not processing time (when the system handled it).

This is important because when replaying historical events, processing time is meaningless.

Example: Apache Beam provides an API for event-time windowing, and the same programs can run on Apache Flink or Google Cloud Dataflow.



######   Unbundling Databases



At the most abstract level, systems like databases, Hadoop, and operating systems all do the same thing:

Store data

Allow you to process and query that data

Differences in abstraction:

Databases store data as structured records (tables, documents, graphs).

Operating systems store data as files in a filesystem.

Example Difference

A filesystem may struggle with 10 million small files, but a database easily manages 10 million small records.

So while both are ‚Äúinformation management systems,‚Äù they take very different approaches.

Unix vs. Relational Databases

Unix philosophy:

Provide a logical but low-level abstraction over hardware.

Files and pipes are just byte sequences.

Relational database philosophy:

Provide a high-level abstraction to hide internal complexities.

Features include SQL, transactions, query optimization, indexes, concurrency control, replication, etc.

Which is better?

Depends on your needs:

Unix is simpler as a thin wrapper over hardware.

Databases are simpler for developers since a short declarative query can leverage powerful built-in features.

Longstanding Tension

This debate has lasted since the 1970s.

The NoSQL movement can be seen as bringing the Unix-style philosophy (low-level abstractions) into distributed databases for OLTP workloads.

The author suggests trying to reconcile these two philosophies to combine the best of both worlds.



######   Composing Data Storage Technologies



Throughout the book, several database features have been discussed:

Secondary indexes
‚Üí Efficiently search records by field values.

Materialized views
‚Üí Precomputed caches of query results.

Replication logs
‚Üí Keep replicas in sync with updates from the primary database.

Full-text search indexes
‚Üí Enable keyword searches; sometimes built directly into relational databases.

Similar ideas show up in batch and stream processing systems:

Full-text indexes can be built in batch jobs.

Materialized views can be maintained via stream processors.

Change Data Capture (CDC) replicates database changes into other systems.

Thus, the features inside databases look very similar to the derived systems built using modern data pipelines.



######   Creating an Index



When you run CREATE INDEX in a relational database, this process happens:

Scan a consistent snapshot of the table.

Gather all field values to be indexed.

Sort them.

Write out the index.

Catch up on recent writes.

If the table was not locked during indexing, new writes may have occurred.

These changes must be applied to the index.

Keep the index up-to-date.

After catching up, the database must maintain the index for all future writes.

Similarity with Other Systems

This is much like setting up a new follower replica:

First take a snapshot.

Then apply the backlog of changes.

Finally, stay up-to-date.

It‚Äôs also like bootstrapping CDC in streaming systems:

Initial snapshot.

Then apply changes as they arrive.

So, every CREATE INDEX is essentially a reprocessing task:

It derives a new view (the index) from the existing dataset.

This mirrors the idea of reprocessing data for application evolution.

Even though databases usually work from a snapshot (state), the concept is very similar to working with a log of changes (stream).

‚úÖ In short:

Modern systems unify batch and stream processing with replay, exactly-once semantics, and event-time windowing.

Databases and operating systems both manage information but with different philosophies (high-level abstraction vs low-level abstraction).

Many database features (indexes, materialized views, replication) are conceptually similar to what batch/stream systems now do.

Creating an index is basically a mini data reprocessing job, much like setting up replication or CDC.



######   The Meta-Database of Everything



The author suggests that if we look at the way data flows across an organization, it starts to resemble one huge database.

Every time a batch job, stream, or ETL process moves data from one system to another, it‚Äôs essentially acting like a subsystem of a database‚Äîsimilar to how indexes or materialized views are kept up to date inside a relational database.

In this view, batch and stream processors look like sophisticated versions of triggers, stored procedures, or materialized view maintenance.

Different systems maintained by these processes resemble different index types (e.g., B-trees, hash indexes, spatial indexes).

Instead of having all of these features built into one integrated database product, modern architectures spread them across many different tools, running on different machines and managed by different teams.

Federated Databases: Unifying Reads

Federated databases (or polystores) allow us to query across different storage engines and processing systems with a unified query interface.

Example: PostgreSQL‚Äôs foreign data wrapper feature.

Benefits:

Applications can still access specialized storage engines directly.

Users can combine and query data from multiple systems in one place.

This follows the relational tradition: a high-level query language with elegant semantics but a complex internal implementation.

Unbundled Databases: Unifying Writes

Federation mainly helps with read-only querying, but it doesn‚Äôt solve the harder problem: synchronizing writes across different systems.

In a single database, index consistency is automatic. But across multiple systems, we need to make sure data changes reach the right places.

One solution: change data capture (CDC) + event logs ‚Üí this is like unbundling index-maintenance into standalone tools.

This follows the Unix philosophy:

Small tools that do one thing well.

Communicate through simple APIs (like pipes).

Composable into bigger systems using higher-level languages (like the shell).



######   Making Unbundling Work



Both federation (reads) and unbundling (writes) aim to create a cohesive, reliable, and scalable system from diverse components.

Federation (reads): requires mapping one data model into another ‚Üí relatively manageable.

Unbundling (writes): harder problem ‚Üí keeping writes synchronized across multiple systems reliably.

Traditional Approach: Distributed Transactions

Requires distributed transactions across heterogeneous systems.

Problem: brittle, complex, and not standardized across all technologies.

Better Approach: Event Logs + Idempotent Writes

Instead of distributed transactions, use asynchronous event logs.

Consumers apply events with idempotence (safe to reapply without duplicating effects).

Benefits:

System level: more robust. Event logs can buffer when a consumer is slow or down. Producers and other consumers keep working. Failures stay contained. Distributed transactions, in contrast, often escalate local faults into system-wide failures.

Human level: teams can work independently, building specialized tools with clean interfaces. Event logs provide durability + ordering = strong enough consistency but general enough for many use cases.



######   Unbundled versus Integrated Systems



Unbundling will not replace databases.

Databases will still:

Maintain state in stream processors.

Serve queries for batch/stream outputs.

Power specialized query engines (e.g., MPP warehouses for analytics).

Drawbacks of unbundling:

Complexity: more infrastructure pieces ‚Üí more learning curves, configuration, and quirks.

Integrated systems can sometimes give better performance for specific workloads.

Key principle: don‚Äôt build for scale you don‚Äôt need (avoids premature optimization).

Goal of unbundling: not beating specialized databases at their job, but allowing combination of multiple databases for broader workload support.

If one database product does everything you need, just use it. Unbundling only makes sense when no single product meets all requirements.



######   What‚Äôs Missing?



The author argues that one critical piece is still missing:

A high-level declarative language (like the Unix shell) for composing unbundled data systems.

Example

Imagine writing:

mysql | elasticsearch


This would mean:

Take everything in MySQL.

Continuously index it into Elasticsearch.

Keep Elasticsearch updated automatically as MySQL changes.

No custom glue code required.

Other Possibilities

Precomputing and updating caches (materialized views).

Supporting complex queries (e.g., recursive queries on graphs).

Early-stage research like differential dataflow is exploring this direction, but production-ready solutions don‚Äôt yet exist.

‚úÖ In summary:
The future of data systems may look like an ‚Äúunbundled meta-database‚Äù where:

Federation solves read queries across systems.

Unbundling + event logs solve write synchronization.

Event logs enable resilience and team independence.

But we still lack a simple, declarative ‚ÄúUnix shell for data‚Äù to compose everything easily.




######   Designing Applications Around Dataflow



The idea of unbundling databases means breaking apart the traditional ‚Äúmonolithic database‚Äù into specialized storage + processing systems that interact with application code. This is sometimes called the ‚Äúdatabase inside-out‚Äù approach. It‚Äôs not really a brand-new architecture, more of a design pattern‚Äîa way to think and talk about systems.

These concepts draw from:

Dataflow languages ‚Üí e.g., Oz, Juttle.

Functional Reactive Programming (FRP) ‚Üí e.g., Elm.

Logic programming ‚Üí e.g., Bloom.

Even spreadsheets, which already support automatic dataflow updates (changing an input cell automatically refreshes dependent cells).

üëâ At the database level, we want the same thing: if data changes, all indexes, caches, or views should auto-update without manual refresh logic.

The big challenge: unlike spreadsheets, databases must also be fault-tolerant, scalable, durable, and integrate with many systems written in different languages and frameworks.



######   Application Code as a Derivation Function



Whenever we build new data from existing data, we apply a transformation (derivation) function. Examples:

Secondary index ‚Üí Derived from base data by extracting certain fields and sorting them (e.g., B-tree, SSTable).

Full-text search index ‚Üí Derived using NLP techniques (language detection, tokenization, stemming, synonyms, etc.), then stored in inverted indexes.

Machine learning model ‚Üí Derived from training data through feature extraction + statistical modeling. Predictions are derived from input + model.

Cache for UI ‚Üí Derived by aggregating or formatting data in the way the UI needs.

üëâ Some derivations (like secondary indexes) are built into databases (e.g., CREATE INDEX).
üëâ Others (like ML feature engineering or specialized search) need custom application code.

Relational databases have tools like triggers, stored procedures, and UDFs, but these were bolted-on features and not very developer-friendly.



######   Separation of Application Code and State



In theory, databases could execute arbitrary app code (like an operating system). But in practice, databases are bad at being code execution environments because they lack modern dev tooling like:

Dependency/package management

Version control

Rolling upgrades

Monitoring/metrics

Service calls and integrations

On the other hand, tools like Mesos, YARN, Docker, Kubernetes are designed to run application code well.

üëâ So the best approach:

Databases specialize in durable state management.

Other platforms run application code.

Both remain independent but interact.

Modern web apps follow this model:

Stateless services ‚Üí App servers don‚Äôt keep state, so requests can be routed anywhere.

Databases hold the state ‚Üí Durable, consistent, shared across app servers.

Functional programmers joke about this as ‚Äúseparation of Church and state‚Äù (Church = Alonzo Church‚Äôs lambda calculus, which has no mutable state).

But one issue:

In most programming languages, you cannot subscribe to changes in variables (no built-in ‚Äúreact to changes‚Äù).

Similarly, databases don‚Äôt push changes to clients. Apps often poll (re-run queries repeatedly).

Only recently have change streams / subscriptions started emerging in databases.



######   Dataflow: Interplay Between State Changes and Application Code




If we think in terms of dataflow, we shift perspective:

Instead of treating the database as a passive variable,

We treat it as an active participant, where state changes trigger application code.

Examples:

Database triggers updating indexes.

Stream systems consuming database change logs to maintain derived views.

Actor/message-passing systems reacting to events.

Tuple spaces (1980s model) where distributed processes observe & react to changes.

üëâ Unbundling the database means moving this logic outside the database:

Caches

Search indexes

ML pipelines

Analytics systems

This relies on stream processing + messaging systems.

Key Challenges for Derived Data

Maintaining derived datasets isn‚Äôt just like running jobs‚Äîit has strict requirements:

Message ordering matters

Derived views must process events in the same order to stay consistent.

Many message brokers break ordering when retrying failed deliveries.

Dual writes (writing to DB + message broker separately) cause inconsistencies.

Fault tolerance is critical

Losing even one message permanently corrupts derived state.

Both message delivery and state updates must be reliable.

Actor systems often keep state in-memory only, losing it on crashes.

üëâ Modern stream processors solve this:

Ensure reliable ordering + delivery.

Provide fault-tolerant processing at scale.

Let you run application code as stream operators.

The Unix Philosophy for Data Systems

Stream operators are like Unix tools in a pipeline.

Each operator:

Takes a stream of changes as input.

Applies arbitrary logic.

Produces a new stream of state changes.

üëâ By chaining operators, we can build large-scale, reliable systems around dataflow.

‚úÖ In short:

Traditional databases are passive + monolithic.

The future lies in unbundled systems, where databases store durable state, stream processors handle dataflow logic, and app code reacts to changes.

This enables scalable, fault-tolerant, reactive applications.




######   Stream Processors and Services



Modern application development often uses service-oriented architecture (SOA), where functionality is split into independent services that talk to each other using synchronous network requests (like REST APIs).

Advantages of Microservices

Organizational scalability through loose coupling.

Different teams can develop, deploy, and update services independently.

Comparison with Dataflow Systems

Similarities: Both decompose tasks into smaller units (services in microservices, operators in dataflow).

Differences:

Microservices ‚Üí synchronous request/response (e.g., REST, RPC).

Dataflow ‚Üí asynchronous, one-directional message streams.

Benefits of Dataflow Systems

Fault tolerance (better than synchronous RPC).

Performance improvements through event-driven updates.

Example: Currency Conversion in Purchases

Microservices approach: Purchase service queries an exchange-rate service/database at the time of transaction.

Dataflow approach: Purchase service subscribes to a stream of exchange rate updates, maintains a local database, and uses it during purchase.

Result: Faster and more reliable because it avoids network requests at purchase time.

Essentially, instead of RPC, this is a stream join between:

Purchase events

Exchange rate updates

Time-dependence

Exchange rates change over time.

If reprocessing past purchases, you need historical rates.

Both service-query and stream-subscription approaches must handle this.

Spreadsheet Analogy

Like a spreadsheet: when one value changes, all dependent values update automatically.

Subscribing to streams makes systems reactive and continuously updated.


######   Observing Derived State



Dataflow systems help create and maintain derived datasets (indexes, materialized views, predictive models).

Write Path vs. Read Path

Write path: Processing that happens when new data is written. Updates flow through pipelines to keep derived datasets fresh.

Read path: Work done when users query the system. Reads data from derived datasets to respond.

Together, they form the full data journey: from collection ‚Üí processing ‚Üí consumption.

Analogy to Functional Programming

Write path ‚Üí eager evaluation (precompute when data arrives).

Read path ‚Üí lazy evaluation (compute only when queried).

The derived dataset (like a search index) is where write path and read path meet.



######   Materialized Views and Caching



A search index is a good example:

Write path ‚Üí update index with document changes.

Read path ‚Üí query the index for keywords.

Trade-offs in Precomputation

No index (grep-like scan)

Write path = light (just store documents).

Read path = heavy (must scan all documents).

Full precomputation of all queries

Read path = very light (just fetch precomputed result).

Write path = impossible (infinite queries, exponential storage).

Caching common queries

A middle ground: precompute results for frequent queries.

Uncommon queries still use the index.

This is effectively a materialized view of query results.

Key Point

Indexes, caches, and materialized views all shift the boundary between write work and read work.

More work at write time = faster reads.

Less work at write time = slower reads.

Twitter Example (from earlier in the book)

Celebrities vs. ordinary users ‚Üí boundary between write/read paths drawn differently depending on load.



######   Stateful, Offline-Capable Clients



Traditionally, client/server model:

Clients ‚Üí stateless.

Servers ‚Üí authoritative source of data.

But now, technology is moving beyond that.

Evolution of Clients

Old browsers: mostly stateless, only useful when online.

Newer single-page apps and mobile apps:

Maintain local state.

Use persistent local storage.

Reduce round-trips to the server.

Offline-First Applications

Apps work offline using local databases.

Sync changes to remote servers in the background when connected.

Particularly useful for mobile devices with slow/unreliable networks.

Reframing the Client-Server Model

On-device state = cache of server state.

Screen pixels = materialized view of local model objects.

Local model objects = replica of remote server state.

This unbundles databases, spreading responsibility between server and client.



######   Pushing State Changes to Clients


Traditional Web Model

In a normal web page:

The browser fetches data from the server once when the page loads.

If the data changes later on the server, the browser won‚Äôt know unless the user reloads.

So, the client‚Äôs view of data is a stale cache unless it explicitly polls the server.

Even protocols like RSS are just polling in disguise.

Moving Beyond Request/Response

Newer protocols exist:

Server-Sent Events (SSE) using EventSource

WebSockets

These allow the client to keep a persistent TCP connection with the server.

The server can push messages to the browser in real time ‚Üí reduces staleness.

Extending the Write Path to Clients

Normally:

Read path: client fetches data (snapshot of state).

Write path: updates propagate between servers/databases.

If servers push state changes directly to clients, then the write path extends all the way to the end user.

Flow:

Client first uses read path to get initial state.

After that ‚Üí client gets state-change events pushed continuously.

Handling Offline Devices

Devices may disconnect and miss updates.

But this problem is already solved in log-based message brokers:

Consumers store their offsets.

When they reconnect, they replay missed events.

Same principle works for individual user devices.



######   End-to-End Event Streams


Stateful Clients Today

Tools like Elm, React, Flux, Redux manage state internally by:

Subscribing to streams of events (user input + server responses).

This looks a lot like event sourcing.

Extending the Model

Naturally, servers could push state-change events into client event pipelines.

This creates a true end-to-end event stream:

Action on one device ‚Üí logged as an event ‚Üí processed by stream processors ‚Üí pushed to other devices in under 1 second.

Real-Time Applications

Already used in:

Instant messaging

Online games

But most apps don‚Äôt use it because:

The stateless request/response model is deeply ingrained in databases, frameworks, and protocols.

Most data stores don‚Äôt support subscriptions to changes.

Shift in Architecture

To make this work widely:

Move from request/response to publish/subscribe dataflow.

Benefits:

More responsive UIs.

Better offline support.

Key takeaway: when designing systems, support subscribing to changes, not just querying state.



######   Reads Are Events Too


Current Model

Write path:

Writes go through an event log before reaching a database.

Read path:

Clients query the database directly (transient network request).

Stream processors also maintain state for joins/aggregations:

Some allow external clients to query this state (like a simple database).

Alternative Model

Treat read requests as events too:

Send read events through the stream processor.

Processor responds with results on an output stream.

Conceptually:

Reads = stream of queries.

Writes = stream of updates.

Together ‚Üí like a stream-table join.

One-Off Reads vs Subscriptions

One-off read:

Query goes through join operator.

Result returned, then forgotten.

Subscribe:

Continuous join ‚Üí result updates whenever data changes.

Benefits of Logging Reads

Enables tracking causal dependencies and provenance:

Example: In online shopping ‚Üí user sees shipping date + stock info ‚Üí influences purchase decision.

By logging read results, we know what the user saw before making decisions.

Costs:

Extra storage + I/O.

Research challenge:

How to optimize logging efficiently.

But if you‚Äôre already logging reads for ops/monitoring, turning them into event logs is not a huge shift.



######   Multi-Partition Data Processing



When Queries Are Simple

For queries that touch only one partition, sending reads through streams may be overkill.

Complex Queries

For queries that combine many partitions, streams are useful:

Stream processors already handle:

Message routing

Partitioning

Joins

Example 1: Storm‚Äôs Distributed RPC

Used to compute:

Number of people who saw a URL on Twitter.

Requires joining follower sets across many partitions.

Example 2: Fraud Detection

To assess fraud risk for a purchase:

Check reputation of IP, email, billing/shipping address, etc.

Each database is partitioned.

Requires joins across differently partitioned datasets.

Parallel with MPP Databases

Massively Parallel Processing (MPP) databases do similar things internally.

If you need heavy multi-partition joins:

Usually easier with a database that supports it directly.

But treating queries as streams provides another scalable option for very large applications.

‚úÖ Summary

The future of data systems may move from request/response ‚Üí event-driven streams.

Clients could subscribe to state changes, making apps more real-time and resilient offline.

Even read requests can be treated as events, enabling causality tracking and powerful analytics.

Multi-partition event processing allows for large-scale distributed queries, blurring the line between databases and stream processors.



######   Aiming for Correctness



Stateless vs. Stateful Services

Stateless services (e.g., web servers that only read data) are easy to recover: if they crash, you fix the bug and restart them‚Äîno long-term harm.

Stateful systems (e.g., databases) are harder because they remember data permanently.

If something goes wrong, the mistake can also last forever.

This makes correctness much more critical.

Transactions as Tools for Correctness

For ~40 years, transactions (ACID: Atomicity, Isolation, Durability) have been the standard tools to ensure correctness.

But:

Their guarantees are often weaker than people think.

Example: weak isolation levels ‚Üí confusing, error-prone, and can break correctness.

Some systems are abandoning transactions in favor of:

Higher performance and scalability.

But at the cost of messier semantics and weaker consistency.

The Problem with Consistency

Consistency is widely discussed but poorly defined:

Some argue to ‚Äúembrace weak consistency‚Äù for availability.

But in practice, what this really means is often unclear.

Determining if an app is safe with a certain isolation level or replication config is very difficult.

Many systems seem fine under low load, but fail subtly under stress or failures.

Evidence of Flaws

Kyle Kingsbury‚Äôs Jepsen experiments showed:

Big differences between what databases claim (safety guarantees) vs. how they behave in real failures (network partitions, crashes).

Even if infrastructure were perfect:

Application code must still use features correctly.

This is error-prone because configs like weak isolation levels or quorum rules are hard to understand.

Trade-Offs in Correctness

If your app can tolerate occasional corruption or loss, life is simpler‚Äîyou may just ‚Äúhope for the best.‚Äù

But if you need strong correctness guarantees:

Serializability + atomic commit are established methods.

Downsides:

Usually only work in one datacenter.

Limit scale and fault tolerance.

Thus, while transactions are still important, they are not the ultimate solution.

We need new ways of thinking about correctness in modern, distributed, dataflow-based systems.



######   The End-to-End Argument for Databases


Strong Guarantees Are Not Enough

Even with strong guarantees like serializable transactions, apps are not automatically safe from corruption/loss.

Example:

If a buggy app writes wrong data or deletes rows, transactions won‚Äôt protect you.

Lesson: immutability + append-only data is safer.

You can recover from mistakes more easily if you don‚Äôt allow destructive updates.

But immutability alone is not a cure-all.

Exactly-Once Execution of an Operation

Problem: When a system processes a message and fails midway, you have two options:

Give up ‚Üí message lost (data loss).

Retry ‚Üí risk that the first attempt succeeded, causing duplicate processing.

Example of corruption:

Charging a customer twice.

Incrementing a counter twice.



######   Exactly-once semantics means:



Final outcome looks as if no faults occurred, even if retries happened.

How to achieve it:

Make operations idempotent (same result whether run once or multiple times).

Requires effort:

Keep metadata (e.g., processed operation IDs).

Use fencing when failing over nodes (to prevent double execution).



######   Duplicate Suppression



Many systems solve duplicates by tracking them:

TCP: Uses sequence numbers to order packets, detect loss, and remove duplicates.

Lost packets are retransmitted, duplicates discarded.

BUT: this only works within one TCP connection.

Problem: Nonidempotent Transactions

Example 12-1: Money transfer transaction

BEGIN TRANSACTION;
UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;
UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;
COMMIT;


Issue:

If client disconnects after sending COMMIT but before receiving confirmation, it doesn‚Äôt know if transaction succeeded.

Retrying could cause double execution ($22 moved instead of $11).

Lesson: Even ‚Äúclassic‚Äù examples of atomicity are not correct in reality.

Real banks use more robust mechanisms.

Two-Phase Commit (2PC)

2PC breaks the dependency between:

TCP connection and transaction.

A coordinator can reconnect after failure to decide commit/abort.

But still not enough to guarantee exactly-once execution.

Client-to-Server Problems

Even if DB duplicate suppression works, there‚Äôs still the network between user ‚Üí application server.

Example:

User submits a form via HTTP POST.

Network drops before receiving the response.

User sees an error ‚Üí retries manually.

Browser warns: ‚ÄúResubmit this form?‚Äù ‚Üí user says yes.

From server‚Äôs view: two separate requests.

From DB‚Äôs view: two separate transactions.

Result: Duplicate actions still happen.

Workaround like Post/Redirect/Get avoids some cases but doesn‚Äôt help with timeouts.



######   Operation Identifiers



To make operations idempotent (so that executing them multiple times has the same effect as executing them once), it‚Äôs not enough to just rely on database transactions. You need to think about the entire request flow, from the client all the way to the database.

Example: Using Unique Identifiers

You can generate a unique identifier (like a UUID) for each operation.

The client includes this identifier when submitting a request (e.g., as a hidden form field).

Alternatively, you can calculate a hash of all the form fields to derive an operation ID.

If the browser accidentally sends the same request twice, both will carry the same operation ID.

By propagating this ID through the system, you can ensure only one execution happens for that request.

Example 12-2: Suppressing Duplicate Requests Using a Unique ID

SQL schema and transaction:

ALTER TABLE requests ADD UNIQUE (request_id);

BEGIN TRANSACTION;
INSERT INTO requests
(request_id, from_account, to_account, amount)
VALUES('0286FDB8-D7E1-423F-B40B-792B3608036C', 4321, 1234, 11.00);

UPDATE accounts SET balance = balance + 11.00 WHERE account_id = 1234;
UPDATE accounts SET balance = balance - 11.00 WHERE account_id = 4321;
COMMIT;

Key points:

Uniqueness constraint on request_id: prevents duplicate inserts.

If the same ID is submitted again, the INSERT fails, aborting the transaction.

This mechanism works even with weak isolation levels in databases, whereas an application-level check-then-insert may fail under nonserializable isolation.

The requests table also acts like an event log, leaning towards event sourcing:

Balance updates could be derived later from the log.

What matters is that each request event is processed exactly once, which the request ID ensures.



######   The End-to-End Argument



This principle (Saltzer, Reed, Clark, 1984) states:

A function can only be implemented correctly with help from the endpoints (the application). The communication system itself cannot guarantee complete correctness‚Äîat most, it can offer partial support.

Duplicate suppression example:

TCP suppresses duplicate packets at the network layer.

Stream processors may promise "exactly-once" semantics.

But: these mechanisms cannot stop a user from re-sending a request if the first attempt timed out.

So, end-to-end solution is needed:
‚û°Ô∏è Use a transaction ID, carried from the client ‚Üí server ‚Üí database.

Other applications of the end-to-end argument:

Data integrity

Ethernet, TCP, TLS detect network corruption.

But they cannot detect corruption from software bugs or disk storage failures.

True safety requires end-to-end checksums.

Encryption

WiFi password protects against local snooping.

TLS/SSL protects against attackers on the network.

But only end-to-end encryption protects against server compromises.

Summary:

Low-level reliability features (TCP ordering, Ethernet checksums, WiFi encryption) are useful but not sufficient. They reduce probability of errors but don‚Äôt guarantee end-to-end correctness.



######   Applying End-to-End Thinking in Data Systems



Even if an application uses a database with strong guarantees (like serializable transactions), correctness is not guaranteed.

Applications must take end-to-end measures, e.g., duplicate suppression with operation IDs.

Challenge: Fault-tolerance is hard

Low-level mechanisms (e.g., TCP reliability) work well.

High-level issues (timeouts, retries, concurrency) occur less often but are much harder to handle.

Transactions simplify programming by collapsing complexity into commit/abort outcomes, but they‚Äôre:

Expensive (especially in distributed systems).

Insufficient for all correctness needs.

When distributed transactions are avoided for performance reasons, developers reimplement fault-tolerance logic in application code, which is:

Hard to reason about (concurrency + partial failures).

Often buggy, leading to data loss/corruption.

Research direction:

We need better abstractions that:

Support application-specific end-to-end correctness.

Still perform well in large-scale distributed environments.



######   Enforcing Constraints



Beyond duplicate suppression, other application-level constraints must be enforced.

Examples:

Uniqueness constraints:

Username or email must be unique.

File service cannot allow duplicate filenames.

No two users can book the same seat.

Other constraints:

Account balance must never go negative.

Stock levels must not go below zero.

Meeting rooms must not have overlapping bookings.

Techniques used for uniqueness can often be adapted to these constraints.



######   Uniqueness Constraints Require Consensus



In distributed systems, enforcing uniqueness needs consensus, because multiple clients may try to write conflicting values at the same time.

Common approaches:

Leader-based consensus

A single node is elected as leader.

It decides which request wins, ensuring uniqueness.

Downsides:

All requests must flow through that leader (latency issues).

If leader fails ‚Üí need leader election (consensus again).

Partitioning

Scale uniqueness checking by partitioning based on the unique value:

Example: route all requests with the same request_id to the same partition.

Example: partition by hash of username for enforcing unique usernames.

What doesn‚Äôt work

Asynchronous multi-master replication:

Different masters could accept conflicting writes at the same time.

Violates uniqueness.

To immediately reject violations, you need synchronous coordination (unavoidable).



######   Uniqueness in Log-Based Messaging



Guarantee of ordering:
Logs ensure that all consumers see messages in the same order. This is called total order broadcast, which is equivalent to consensus.

Using logs for uniqueness constraints:
In an unbundled database with log-based messaging, we can enforce uniqueness constraints (e.g., unique usernames) using stream processors.

How it works:

Message appending: Every request (e.g., for a username) is encoded as a message and appended to a log partition. Partitioning is done by hashing the unique value (e.g., username).

Stream processing: A stream processor consumes messages sequentially from that partition. It keeps a local database to track taken values.

If the username is available ‚Üí mark it as taken ‚Üí emit a success message.

If already taken ‚Üí emit a rejection message.

Client confirmation: The client waits for its success/rejection message from the output stream.

Key property:
Because all conflicting operations for the same key go to the same partition and are processed sequentially, the system can deterministically decide the outcome.

Scalability:
Throughput can be increased by adding more partitions (each partition processed independently).

Applicability beyond uniqueness:
This approach works for other constraints too.

Any writes that may conflict are routed to the same partition and processed sequentially.

The definition of conflict depends on the application.

Stream processors can run arbitrary validation logic.

This resembles Bayou‚Äôs conflict resolution approach (1990s).


######   Multi-Partition Request Processing



The challenge:
Some operations involve multiple independent partitions.
Example: Money transfer ‚Üí request ID, payer account, payee account.

These belong to different partitions.

Traditional databases solve this via atomic commit (forcing a total order across partitions).

This reduces throughput since partitions can no longer be processed independently.

Partitioned logs approach (without atomic commit):

Log request: The client creates a unique request ID and appends the transfer request to a partition (based on the request ID).

Derive instructions: A stream processor reads requests and emits:

A debit instruction to payer A (partitioned by A).

A credit instruction to payee B (partitioned by B).
Both carry the same request ID.

Apply updates: Further processors read debit/credit streams, deduplicate by request ID, and apply account balance changes.

Why this avoids atomic commit:

If the client sent debit & credit instructions directly, it would need an atomic commit across partitions.

By first logging the request as a single message, the system ensures the request either exists or doesn‚Äôt (single-object atomicity).

Then, all downstream actions derive deterministically from that log entry.

Fault tolerance:

If the step 2 processor crashes, it resumes from the checkpoint.

It may reprocess requests, but since it is deterministic, it produces the same debit/credit instructions.

Deduplication in step 3 ensures no double application.

Overdraft prevention (optional):

Another stream processor (partitioned by payer account) can maintain balances and validate before placing requests into the log.

Only valid requests are logged in step 1.

Result:

Same correctness property as an atomic transaction (each request applied exactly once).

Achieved without distributed atomic commit.

Uses multiple partitioning strategies across stages + unique request IDs.

Relation to earlier concepts:

This is similar to multi-partition data processing and concurrency control ideas.



######   Timeliness and Integrity



Transactions vs stream processing:

Traditional transactions are linearizable (writes visible immediately after commit).

Stream processing is asynchronous: producers don‚Äôt wait for consumers.

But clients can wait for derived results on output streams (e.g., username uniqueness).

Difference between timeliness and integrity:

Timeliness

Ensures users see up-to-date data.

If users read stale data (e.g., due to replication lag), the system may appear inconsistent.

But this inconsistency is temporary ‚Üí resolves automatically with time.

CAP theorem‚Äôs ‚Äúconsistency‚Äù = linearizability, a strong form of timeliness.

Weaker forms also exist: e.g., read-after-write consistency.

Integrity

Ensures no corruption or contradictions in data.

Views or derived data (like indexes) must faithfully represent underlying data.

If violated, the inconsistency is permanent (not fixed by waiting).

Requires checking and repair.

In ACID terms, ‚Äúconsistency‚Äù usually refers to integrity, enforced with atomicity & durability.

Slogan form:

Timeliness violations ‚Üí eventual consistency

Integrity violations ‚Üí perpetual inconsistency

Relative importance:

Integrity is more critical than timeliness in most applications.

Timeliness delays are tolerable; integrity failures are catastrophic.

Example (credit card statements):

Timeliness issue: Recent transactions may not appear immediately ‚Üí acceptable.

Integrity issue: Balance not matching transactions, or money disappearing ‚Üí unacceptable.



######   Correctness of Dataflow Systems



ACID Transactions: Timeliness and Integrity

ACID transactions (Atomicity, Consistency, Isolation, Durability) normally provide both:

Timeliness ‚Üí e.g., linearizability, ensuring that all operations appear to happen in order, in real-time.

Integrity ‚Üí e.g., atomic commit, ensuring that a transaction‚Äôs changes are applied entirely or not at all.

Because ACID transactions couple timeliness and integrity, you usually don‚Äôt distinguish between the two when reasoning about correctness.

Event-Based Dataflow Systems: Decoupling Timeliness and Integrity

In event-based streaming systems, timeliness and integrity are separated:

No guaranteed timeliness: Messages may arrive late unless consumers are explicitly designed to wait for them.

Integrity is central: Systems must prevent data corruption despite failures.

Exactly-once / effectively-once semantics:

Protect integrity by avoiding data loss and duplicate processing.

Achieved through fault-tolerant message delivery and idempotent operations (ignore duplicates).

Key mechanisms for integrity (without distributed transactions):

Atomic single-message writes ‚Üí fits well with event sourcing.

Deterministic derivation functions ‚Üí derive all state from the original event, like stored procedures.

Client-generated request IDs ‚Üí enable end-to-end duplicate suppression and idempotence.

Immutable messages ‚Üí allow reprocessing and recovery from bugs.

Result: Comparable correctness to ACID transactions, but with better performance and robustness.



######   Loosely Interpreted Constraints



Uniqueness constraints (e.g., unique usernames, one seat per passenger):

Normally require consensus ‚Üí all events for a partition go through a single node.

This limits scalability and requires coordination.

However, many real applications can tolerate temporary constraint violations if they can be fixed later using compensating transactions (apologies, refunds, corrections).

Examples of Looser Constraints

Duplicate username / double booking a seat

System apologizes and asks one user to pick another option.

Warehouse stock mismatch

If more items sold than available, order more or apologize for delay.

Same as handling accidents (e.g., damaged stock).

Airlines / hotels overbooking

They intentionally violate "one seat/room per customer" to optimize business.

Compensation (refunds, upgrades, alternative accommodations) resolves conflicts.

Bank overdrafts

Customer withdraws too much ‚Üí bank charges fees, requests repayment.

Risks are bounded by daily limits.

Key Idea

Many applications don‚Äôt need immediate constraint enforcement.

They can optimistically write data, then validate later.

Integrity (no data loss or mismatched credits/debits) remains crucial, but timeliness in enforcement is not always required.



######   Coordination-Avoiding Data Systems



Two Key Observations

Dataflow systems can preserve integrity without:

Atomic commits

Linearizability

Cross-partition synchronous coordination

Strict uniqueness constraints require coordination, but many applications can use weaker constraints with later fixes.

Benefits of Coordination Avoidance

Systems can manage data without requiring global coordination.

Achieve:

Better performance

Higher fault tolerance

Example:

Multi-datacenter, multi-leader replication.

Each datacenter operates independently.

No synchronous cross-region coordination needed.

Guarantees:

Strong integrity

Weak timeliness (not linearizable).

Role of Transactions

Serializable transactions still useful, but at small scope (e.g., within a single partition).

Global distributed transactions (e.g., XA) are not required.

Synchronous coordination should only be applied where recovery is impossible.

Trade-Off: Coordination vs. Apologies

Coordination reduces inconsistencies, but also reduces performance/availability.

No coordination increases availability/performance, but may require more apologies.

You can‚Äôt avoid apologies entirely ‚Üí the goal is to balance inconsistencies vs. outages for the best trade-off.

‚úÖ Summary:
Dataflow systems separate integrity (must always be maintained) from timeliness (not always necessary). By using techniques like event sourcing, idempotence, and immutable events, they avoid expensive coordination. Many real-world applications can tolerate temporary constraint violations and correct them later through compensations. Thus, coordination-avoiding data systems are a promising way forward, balancing correctness, performance, and operational robustness.




######   Trust, but Verify



This section discusses the idea that while we usually assume systems behave correctly, in reality things can go wrong‚Äîsometimes in rare and unexpected ways. Therefore, instead of relying purely on trust, systems should be built with verification and auditing mechanisms.

System Models and Assumptions

System model: Defines what we assume can and cannot fail in a system.

Example assumptions:

Processes can crash.

Machines can lose power.

Networks can drop or delay messages.

Data is not lost after fsync.

Memory and CPU instructions behave correctly.

These assumptions are usually reasonable and practical, but in reality, failures happen with certain probabilities.

Question: Do rare assumption violations occur often enough to matter in practice?

Data Corruption in Reality

Disk corruption: Data can degrade while sitting on disk.

Network corruption: TCP checksums don‚Äôt catch all errors.

Memory corruption:

Rare bit-flips can occur due to hardware faults or radiation.

Attack technique rowhammer can intentionally flip bits, breaking OS security.

Hardware is not a perfect abstraction‚Äîunexpected corruption is possible.

Even though rare, bit-flips deserve attention.



######   Maintaining Integrity in the Face of Software Bugs



Hardware isn‚Äôt the only risk‚Äîsoftware bugs also corrupt data.

Even robust databases (MySQL, PostgreSQL) have had:

Uniqueness constraint failures.

Serializable isolation anomalies.

Application code is more error-prone:

Often lacks reviews/testing.

Frequently misuses DB features (e.g., ignoring foreign keys).

ACID consistency assumes transactions are correct, but if application logic is buggy or unsafe isolation levels are used ‚Üí integrity is compromised.


######   Don‚Äôt Just Blindly Trust What They Promise



Data corruption is inevitable (hardware + software).

Need mechanisms to detect corruption (auditing).

Auditing importance:

Not just for finance ‚Üí useful for all systems.

Example: HDFS and Amazon S3 periodically read, compare replicas, and repair files to detect silent corruption.

Backups must be verified ‚Üí test restore processes, otherwise risk losing data when needed.

Lesson: Don‚Äôt blindly trust storage systems or backups.



######   A Culture of Verification



Mature systems (HDFS, S3) assume hardware is ‚Äúmostly correct‚Äù but still verify continuously.

Most systems don‚Äôt have this ‚Äútrust but verify‚Äù culture ‚Üí they assume guarantees are absolute.

ACID culture problem:

Led developers to blindly trust transaction guarantees.

Neglected auditing.

Shift to NoSQL:

Weaker consistency + less mature tech.

But still no auditing ‚Üí more dangerous.

Need self-auditing, self-validating systems in the future.



######   Designing for Auditability



Problem with transactions:

If a transaction mutates multiple objects, it‚Äôs hard to reconstruct why it happened from logs.

Application logic is transient and not reproducible.

Event sourcing as a solution:

Represent user input as immutable events.

State updates are derived deterministically from events.

Replaying events produces the same results.

Auditability benefits:

Easier to check provenance of data.

Use hashes to check event log integrity.

Can rerun batch/stream processors or run redundant derivations for verification.

Debugging benefits:

Deterministic dataflow = easier tracing.

‚ÄúTime-travel debugging‚Äù ‚Üí reproduce past state to understand issues.



######   The End-to-End Argument Again



Since we cannot fully trust every component (hardware, software), we must periodically check integrity.

Without checks ‚Üí corruption may propagate unnoticed and cause damage.

End-to-end checks:

Verify entire data pipelines, not just components.

Covers disks, networks, services, algorithms in one sweep.

Continuous end-to-end auditing:

Builds confidence in correctness.

Like automated testing ‚Üí catches bugs early.

Reduces fear of making changes ‚Üí allows faster evolution of applications.

‚úÖ Summary:
The main message is that systems should not rely purely on trust (in hardware, software, or databases). Failures are rare but inevitable. The solution is to adopt a ‚Äútrust, but verify‚Äù culture, with auditing, event sourcing, deterministic dataflows, and end-to-end checks to ensure long-term correctness and resilience.




######   Tools for Auditable Data Systems


Current Situation

Most data systems don‚Äôt focus on auditability as a primary feature.

Some applications add their own audit mechanisms, like logging all changes to an audit table.

But:

Ensuring both the audit log and database state are tamper-proof is still difficult.

Example: Transaction logs can be protected by digitally signing them (using a hardware security module), but this doesn‚Äôt guarantee that only the correct transactions were logged.

Cryptographic Integrity Tools

Cryptography could be used to prove the integrity of systems, making them resistant to hardware/software problems and even malicious actions.

Cryptocurrencies, blockchains, and distributed ledger technologies (e.g., Bitcoin, Ethereum, Ripple, Stellar) explore these ideas.

Data Systems Perspective

These technologies are essentially distributed databases with:

A data model

A transaction mechanism

Replicas hosted by mutually untrusting organizations

Replicas:

Continuously check each other‚Äôs integrity

Use a consensus protocol to agree on valid transactions

Limitations

Skepticism about Byzantine fault tolerance in these technologies.

Proof of work (e.g., Bitcoin mining) is wasteful.

Bitcoin‚Äôs throughput is low, but for political/economic reasons more than technical ones.

Interesting Aspects

Integrity checking is useful.

Cryptographic auditing often uses Merkle trees:

A tree of hashes that can prove a record exists in a dataset efficiently.

Example outside crypto hype: Certificate Transparency

Uses Merkle trees to verify TLS/SSL certificates‚Äô validity.

Future Possibility

Similar algorithms (like certificate transparency and distributed ledgers) could become more common in general data systems.

Challenges:

Making them scalable

Reducing the performance penalty



######   Doing the Right Thing



Big Picture

Every system has a purpose and produces intended and unintended consequences.

Systems may aim to make money, but effects often go far beyond that.

As engineers, we must think about consequences and decide the kind of world we want to create.

Respect for Data

Data often represents people (their behavior, interests, identity).

Must treat data with humanity and respect.

Users are humans ‚Üí dignity matters.

Ethical Responsibility

Software development increasingly requires ethical choices.

There are guidelines (e.g., ACM‚Äôs Code of Ethics), but they are rarely applied.

As a result, many engineers/product managers act carelessly about privacy and negative consequences.

Technology and Use

Technology is neutral ‚Üí its effect depends on how it‚Äôs used.

Engineers must not ignore consequences.

Ethical responsibility belongs to engineers too.


######   Predictive Analytics



Definition

A big part of the ‚ÄúBig Data‚Äù hype.

Used to predict things like weather or disease spread ‚Üí less controversial.

Used for things like:

Predicting if a convict will reoffend

Loan default likelihood

Insurance claims likelihood

Hiring risks

These directly affect individual lives.

Risk of Exclusion

Organizations (banks, airlines, employers, etc.) prefer to be cautious.

Safer to reject uncertain applicants than risk losses.

But: repeated rejections can trap someone in an ‚Äúalgorithmic prison‚Äù (systematic exclusion from jobs, loans, housing, etc.).

Unlike criminal law (innocent until proven guilty), algorithms can exclude people without proof and with little chance of appeal.



######   Bias and Discrimination


Algorithms vs Humans

Algorithms aren‚Äôt inherently better or worse than human decisions.

Humans have biases ‚Üí algorithms trained on biased data will learn and amplify bias.

Hope: data-driven decisions could be fairer than human intuition.

Reality: systems often encode and strengthen discrimination.

Protected Traits

Laws forbid discrimination based on traits like race, age, gender, disability, beliefs.

But other factors (like postal code, IP address) strongly correlate with these traits.

Believing biased inputs can magically produce fair outputs is false.

Satirized as: ‚Äúmachine learning is like money laundering for bias.‚Äù

Main Point

Predictive analytics extrapolate from the past.

If the past is discriminatory, the system will replicate discrimination.

To improve the future, moral imagination (human input) is essential.

Data and models should be tools, not masters.



######   Responsibility and Accountability


Accountability Gap

Humans can be held responsible for mistakes.

Algorithms make mistakes too‚Äîbut who is accountable?

Self-driving car accidents

Discriminatory credit scoring algorithms

Question: if reviewed by a court, can you explain how the algorithm made its decision?

Credit Scoring Example

Traditional credit scores:

Based on borrowing history (relevant facts).

Errors can (sometimes) be corrected.

Machine learning scores:

Use many more inputs.

Are opaque (hard to explain).

Hard to prove fairness or detect discrimination.

Statistical Nature of Data

Data is probabilistic: it can be wrong in individual cases even if the average is correct.

Example: average life expectancy doesn‚Äôt predict exact lifespan.

Prediction systems also make individual errors.

Dangers

Blind faith in data is dangerous.

Need to solve:

Algorithm accountability and transparency

Preventing bias reinforcement

Fixing mistakes when they occur

Dual-Use of Analytics

Analytics can help or harm:

Positive: target aid/support to those in need

Negative: exploited by predatory businesses (e.g., high-cost loans, fake degrees)

‚úÖ In summary:

Auditability in data systems may improve using cryptographic tools (Merkle trees, distributed ledgers).

Ethical responsibility in building systems is crucial since data affects real people.

Predictive analytics can be powerful but risks discrimination and exclusion.

Accountability for algorithmic decisions remains unresolved and urgently needs solutions.


######    Feedback Loops


Predictive systems like recommendation engines can create echo chambers, where people only see content they agree with.

This encourages stereotypes, misinformation, and political polarization (e.g., seen in election campaigns).

Predictive analytics in real life (like credit scores for hiring) can cause self-reinforcing cycles:

Misfortune ‚Üí missed payments ‚Üí lower credit score ‚Üí harder to get a job ‚Üí joblessness ‚Üí poverty ‚Üí further lower scores.

These feedback loops hide behind a false sense of objectivity (mathematical rigor).

We can‚Äôt always foresee such loops, but systems thinking (analyzing the whole system including human interactions) helps.

Key questions:

Does the system amplify inequality (rich get richer, poor get poorer)?

Or does it try to fight injustice?

Even with good intentions, unintended consequences must be considered.



######   Privacy and Tracking



Ethical problems come not just from predictive analytics but also data collection itself.

When users explicitly enter data, the system is serving them (user = customer).

But when behavior is tracked indirectly (e.g., clicks, browsing), the service develops its own interests, which may conflict with users‚Äô.

Tracking has benefits:

Improves search rankings (via clicks).

Enables recommendations (‚Äúpeople who liked X also liked Y‚Äù).

A/B tests & flow analysis help improve interfaces.

But if the service is ad-funded, advertisers (not users) are the real customers.

Leads to detailed profiling for marketing.

Users‚Äô engagement serves advertisers‚Äô needs, not their own.

This relationship resembles surveillance.


######   Surveillance



Thought experiment: replace ‚Äúdata‚Äù with ‚Äúsurveillance.‚Äù

E.g., ‚Äúsurveillance-driven organization,‚Äù ‚Äúsurveillance warehouse,‚Äù etc.

Suddenly it sounds sinister.

Modern digital infrastructure = mass surveillance system.

With the Internet of Things (IoT), microphones and sensors are everywhere (smartphones, TVs, assistants, baby monitors, toys).

Many devices have poor security, increasing risks.

Historically, even totalitarian regimes couldn‚Äôt force such constant surveillance‚Äîyet people now voluntarily adopt it.

Difference: surveillance is corporate, not governmental.

Why do people accept it?

Belief of ‚Äúnothing to hide‚Äù (privileged position).

Perceived as harmless (recommendations, ads).

But surveillance already affects insurance, employment, and daily life (e.g., driving trackers, fitness devices).

Data can be more intrusive than expected (e.g., wearables inferring what you type).

Algorithms will only get more powerful at extracting hidden insights.


######   Consent and Freedom of Choice



Common claim: ‚ÄúUsers consent by accepting terms of service.‚Äù

Problems with this argument:

Users don‚Äôt know what data is collected, how it‚Äôs processed, or how long it‚Äôs retained.

Privacy policies obscure more than they clarify.

Data from one user often reveals info about others who never consented.

Derived datasets (combined user + external data) are beyond user understanding.

Data extraction = one-way process, not reciprocal.

No negotiation; terms are set by the service.

Refusal = not real freedom:

Popular services (Google, Facebook, smartphones) are essential for social participation.

Network effects create social and professional costs for opting out.

Only a privileged minority can choose not to use such services.

For most people, surveillance is unavoidable.


######   Privacy and Use of Data



Claim ‚Äúprivacy is dead‚Äù is false.

Privacy ‚â† secrecy. It means choice: deciding what to reveal, to whom, and when.

With surveillance:

Privacy rights are transferred to companies.

Users must ‚Äútrust‚Äù companies to handle data responsibly.

Companies keep outcomes hidden (to avoid appearing creepy).

Info is revealed indirectly (e.g., targeted ads for people with illnesses).

Users lose control over sensitive disclosures‚Äîthe company decides based on profit.

Many companies focus on not looking creepy rather than limiting data use.

Perception is managed, often poorly (e.g., painful reminders in ads).

Data can be wrong, undesirable, or harmful.

Systems must plan for these failures.

Algorithms alone don‚Äôt understand what is ‚Äúinappropriate.‚Äù

Privacy settings (what other users can see) = small step forward.

But services themselves still have full access.

Privacy policies often grant broad rights for internal use.

This large-scale transfer of privacy rights is historically unprecedented.

Surveillance used to be manual, costly, limited.

Trust (e.g., doctor‚Äìpatient) was governed by strict ethical/legal rules.

Internet services enable massive-scale sensitive data collection without meaningful consent.



######   Data as Assets and Power


Data Exhaust vs. Core Asset

Behavioral data, created when people interact with digital services, is often called ‚Äúdata exhaust‚Äù, implying it is just waste.

In reality, this data is the core asset for many online businesses, especially when services are funded by targeted advertising.

Applications (social media, apps, etc.) act as tools to lure users into giving away personal data, which feeds the surveillance infrastructure.

Exploitation of Human Activity

While online services enable creativity and social connections, companies often exploit these human activities to extract more data.

Data Brokers Industry

The existence of data brokers (who buy, aggregate, analyze, and resell personal data) proves data‚Äôs economic value.

Startups are valued based on user numbers (or ‚Äúeyeballs‚Äù), which essentially reflect their surveillance capabilities.

Who Wants the Data?

Companies: collect data as their main resource.

Governments: obtain it via secret deals, legal compulsion, coercion, or even hacking.

When companies go bankrupt, their user data gets sold as a valuable asset.

Security problems: data is hard to secure, leading to frequent breaches.

Data as a Toxic Asset

Critics argue data is not just valuable but also toxic or hazardous.

Risks:

Criminals or foreign spies may steal it.

Insiders may leak it.

Management could misuse it.

Authoritarian regimes could demand it.

Therefore, collecting data means weighing benefits vs. risks of misuse.

Civic Hygiene and Political Context

We must consider future governments, not just current ones.

Technology that enables mass surveillance could support a police state in the wrong hands.

Knowledge and Power

Old saying: ‚ÄúKnowledge is power.‚Äù

To observe others without being observed is one of the strongest forms of power.

Totalitarian governments use surveillance for control.

Tech companies may not seek political power, but their data ownership still gives them immense hidden influence.



######   Remembering the Industrial Revolution



Data and the Information Age

Data defines the information age‚Äîenabled by the internet, data storage, processing, and automation.

Its social and economic impact is often compared to the Industrial Revolution.

The Industrial Revolution: Benefits and Problems

Benefits: economic growth, better living standards.

Problems: pollution, unsafe housing, harsh working conditions, child labor.

Solution: regulation (environmental laws, workplace safety, banning child labor, food inspections).

Though regulation increased costs, society benefited greatly.

Parallel to the Information Age

Just as pollution was the dark side of the industrial era, data misuse and surveillance are the dark side of today‚Äôs digital era.

Bruce Schneier‚Äôs View

‚ÄúData is the pollution problem of the information age.‚Äù

Privacy protection is like environmental protection.

Future generations will judge us on how we handle data misuse and surveillance, just as we judge the past for ignoring industrial pollution.



######   Legislation and Self-Regulation



Data Protection Laws

Example: 1995 European Data Protection Directive requires:

Data must be collected only for specific, explicit, and legitimate purposes.

Data must be adequate, relevant, and not excessive.

Limitations in Today‚Äôs Internet Context

These laws conflict with Big Data philosophy, which values:

Maximizing collection.

Combining datasets.

Experimentation and exploration (using data in unforeseen ways).

This undermines meaningful user consent.

Regulation vs. Innovation

Companies resist regulation, arguing it slows innovation.

Example: Medical data

Risks: privacy violations.

Benefits: better diagnostics, new treatments.

Overregulation could block life-saving advances.

Need for a Culture Shift

Tech industry should stop treating users as metrics and treat them as humans with rights.

Companies should:

Self-regulate data practices.

Educate users about how data is used.

Respect privacy, dignity, and agency.

Privacy as a Commons

Privacy is like a national park‚Äîif not protected, it will be destroyed.

Without protection, surveillance will spread, hurting everyone.

But surveillance is not inevitable‚Äîwe can still stop it.

Possible Solutions

Purge data once it‚Äôs no longer needed.

Explore cryptographic access controls (enforcing privacy via technology, not just policy).

Most importantly: culture and attitude changes in tech.



######   Summary



Technical Approaches to Data Systems

No single tool fits all cases ‚Üí need to compose multiple systems.

Use batch processing + event streams to integrate data.

Designate some systems as sources of truth, derive others via transformations.

Loosely coupled, asynchronous dataflows ‚Üí robust, fault-tolerant systems.

Benefits of Dataflow Design

Easier evolution: re-run transformations when code changes.

Easier recovery: reprocess data after errors.

Similar to how databases internally function, but more flexible and scalable.

Updates can flow all the way to user interfaces, even supporting offline work.

Correctness and Fault Handling

Ensure correctness with:

Asynchronous event processing.

Idempotent operations (safe to retry).

Asynchronous constraint checking.

Clients can either wait for checks or proceed (with risk of needing to apologize later).

This is more scalable than distributed transactions.

Ethical Aspects

Data can do good (better services, insights).

But it can also cause harm:

Discrimination, exploitation.

Normalizing surveillance.

Data breaches.

Unintended consequences.

Final Thought

Engineers carry responsibility to shape the world we want.

That world should treat people with humanity, respect, and dignity.

We must consciously work toward it together.




