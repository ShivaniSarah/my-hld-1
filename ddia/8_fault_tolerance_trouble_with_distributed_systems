CHAPTER 8 ‚Äì The Trouble with Distributed Systems
Introduction

This chapter builds on earlier discussions of failures in distributed systems like:

Replica failover (what happens when a node goes down)

Replication lag (inconsistencies when replicas are behind)

Concurrency control (problems when multiple transactions run at the same time).

So far, the picture was already concerning. But here, the author says: we were still too optimistic.

In reality, anything that can go wrong will go wrong.

Engineers working with distributed systems often adopt this pessimistic view because experience shows failures happen in strange, unpredictable ways.

Key Difference: Single Computer vs. Distributed System

On a single computer, behavior is deterministic: if the hardware works, the same input gives the same output.

On a distributed system, things are messier because computers must communicate over a network, and many new types of failures appear.

The chapter‚Äôs aim:

Expose problems that arise in real distributed systems.

Understand what can and cannot be relied on.

Set the stage for Chapter 9, which introduces algorithms to handle these challenges.



###### Faults and Partial Failures



Behavior of a Single Computer

Normally: it works or doesn‚Äôt work.

Software bugs might cause weird behavior (like ‚Äúbad day‚Äù crashes), but:

Good hardware + good software = deterministic execution.

Hardware problems usually cause total failure (e.g., kernel panic, blue screen).

üëâ So, computers are designed to fail completely rather than return wrong results.

Returning wrong results is dangerous because it causes confusion.

Instead, systems crash deterministically.

Example: CPU instructions always behave the same way; memory/disk writes remain intact.

This design principle of always-correct computation traces back to the very first digital computers.

Behavior of Distributed Systems

Multiple computers connected by a network cannot guarantee the same idealized model.

The messy physical world introduces:

Network partitions (some nodes can‚Äôt reach others).

Hardware failures at different levels (switches, PDUs, racks, even entire data centers).

Unexpected accidents (like the anecdote of a truck crashing into a data center‚Äôs HVAC system).

üëâ In short, some parts of the system may fail while others work fine.

Partial Failures

Definition: When some parts of the system fail, but others continue working.

Problem: These failures are nondeterministic.

A request involving multiple nodes may sometimes succeed and sometimes fail in unpredictable ways.

Example: You might send a message but never know if it:

Reached the destination,

Got lost in the network, or

Arrived late after a timeout.

üëâ Message travel time is also nondeterministic, making it even harder to reason about success or failure.

Why Distributed Systems Are Hard

On a single computer, failures are simple: either the whole thing works or it doesn‚Äôt.

In a distributed system, failures can be:

Partial ‚Üí some nodes respond, others don‚Äôt.

Unpredictable ‚Üí the same operation can behave differently each time.

Invisible ‚Üí you may not know whether an operation succeeded.

Thus, partial failure + nondeterminism = the core difficulty of distributed systems.






I‚Äôll explain this section with the same headings as in your text so you don‚Äôt miss any point.

######  Cloud Computing and Supercomputing

There are different philosophies for building large-scale computing systems.

High-Performance Computing (HPC) / Supercomputing

Used for computationally intensive scientific tasks (e.g., weather forecasting, molecular dynamics).

Built from supercomputers with thousands of CPUs.

Cloud Computing

Not sharply defined, but usually refers to:

Multi-tenant datacenters (many customers share resources).

Commodity machines connected with IP networks (Ethernet).

Elastic/on-demand resource allocation (scale up or down easily).

Metered billing (pay for what you use).

Enterprise Datacenters

Sit between HPC and cloud computing in design and philosophy.

Fault Handling Approaches
Supercomputers (HPC)

Strategy: checkpoint and restart.

Jobs periodically checkpoint state to durable storage.

If one node fails ‚Üí entire cluster stops.

After fixing the node, computation restarts from last checkpoint.

This makes supercomputers more like single-node systems:

Partial failure is escalated to total failure (just like a single machine crash).

Internet Services (Focus of the Book)

Very different requirements:

Always online ‚Üí services must be available with low latency; cannot stop the cluster for repairs.

Hardware differences:

Supercomputers use specialized, reliable hardware and communication methods like shared memory or RDMA.

Cloud systems use commodity hardware (cheaper but less reliable).

Network differences:

Cloud datacenters: IP/Ethernet networks in Clos topologies (for high bandwidth).

Supercomputers: specialized topologies (meshes, toruses) optimized for known communication patterns.

Scale and likelihood of failure:

The bigger the system, the higher the chance something is broken.

In huge clusters, something is always broken.

If your only fault strategy is ‚Äústop everything,‚Äù most of the system‚Äôs time will be wasted recovering instead of doing useful work.

Fault tolerance as a feature:

Systems that tolerate node failures without stopping are very valuable.

Enables rolling upgrades ‚Üí restart nodes one at a time while service keeps running.

In the cloud: if a VM is slow, just kill it and request a new one.

Geographical distribution:

Internet services often replicate data across the globe to reduce latency for users.

This means relying on the internet ‚Üí slower and less reliable than local supercomputer networks.

Supercomputers assume all nodes are in the same place.

Accepting Partial Failures

To build distributed systems, we must accept partial failures.

The goal: build reliable systems out of unreliable components.

No such thing as perfect reliability, so we must know the limits of guarantees we can realistically provide.

Even in Small Systems

Partial failure still happens, just less often.

Over time, some component will fail, so software must handle it gracefully.

Fault handling must be designed into the software.

Operators must know how the software behaves during faults.

üëâ Assuming faults are rare and just ‚Äúhoping for the best‚Äù is dangerous.

Must consider a wide range of possible faults.

Even unlikely faults should be tested ‚Üí artificially create them in test environments.

In distributed systems, suspicion, pessimism, and paranoia pay off.



######  Building a Reliable System from Unreliable Components



At first glance, this seems impossible (a chain is only as strong as its weakest link).
But computing has long used the idea of building reliability on top of unreliable foundations.

Examples

Error-Correcting Codes (ECC)

Used to transmit data accurately over channels that occasionally flip bits.

Example: wireless networks affected by radio interference.

TCP over IP

IP (Internet Protocol) is unreliable: can drop, delay, duplicate, or reorder packets.

TCP (Transmission Control Protocol) adds reliability:

Retransmits lost packets.

Eliminates duplicates.

Reassembles packets in correct order.

Limits of Reliability

The higher-level system is more reliable, but not infinitely so.

Example:

ECC can handle a small number of errors, but if the signal is overwhelmed by interference, it fails.

TCP can hide loss/duplication/reordering, but cannot hide network delays.

Why This Matters

Even though reliability is not perfect, it‚Äôs still valuable.

By handling low-level failures, these mechanisms make higher-level systems easier to reason about.

The book will explore this further in ‚ÄúThe End-to-End Argument‚Äù (Chapter 9, p. 519).





######  Unreliable Networks


Shared-Nothing Systems

The distributed systems we study are usually shared-nothing systems:

Each machine has its own memory and disk.

One machine cannot directly access another‚Äôs memory/disk.

Communication happens only via the network.

Reasons this model is dominant for internet services:

Cheap ‚Üí no special hardware needed.

Commodity cloud computing can be used.

High reliability possible via redundancy across multiple datacenters.

Asynchronous Packet Networks

Internet and datacenter networks (Ethernet) are asynchronous packet networks.

Meaning:

One node can send a packet to another.

But the network makes no guarantees:

When the packet will arrive.

Whether it will arrive at all.

Things That Can Go Wrong (if you send a request and expect a response):

Request lost (e.g., cable unplugged).

Request delayed in queue (network/recipient overloaded).

Remote node failed (crash or power down).

Remote node temporarily paused (e.g., long garbage collection stop).

Request processed but response lost (e.g., misconfigured switch).

Request processed but response delayed (network overload or sender overload).

üëâ From the sender‚Äôs perspective, you cannot distinguish the cause‚Äîall you know is ‚Äúno response yet.‚Äù

Timeouts

Usual solution: use a timeout.

Wait for some period.

If no response ‚Üí assume failure.

Problem:

Even after timeout, you don‚Äôt know if the request was delivered.

Request might still be queued and eventually reach recipient even though sender gave up.


######  Network Faults in Practice


Reality Check

We‚Äôve built networks for decades, but they are still unreliable.

Studies show network problems are common, even in controlled datacenters.

Evidence

Medium-sized datacenter: ~12 network faults/month.

Half ‚Üí disconnect a single machine.

Half ‚Üí disconnect an entire rack.

Failures in top-of-rack switches, aggregation switches, load balancers are common.

Redundancy helps less than expected ‚Üí human error (misconfigured switches) is a big cause.

Cloud vs Private Datacenters

Public clouds (e.g., EC2) ‚Üí frequent transient glitches.

Well-managed private datacenters ‚Üí usually more stable.

Examples of Surprising Faults

Switch software upgrade ‚Üí network topology reconfig, causing packet delays > 1 minute.

Sharks biting undersea cables damaging connectivity.

One-way failure ‚Üí network interface drops inbound packets but still sends outbound packets.

Network Partitions

If part of the network is cut off from the rest, this is called a network partition (or netsplit).

Book prefers ‚Äúnetwork fault‚Äù to avoid confusion with data partitions (shards).

Importance of Handling Network Faults

Even if rare, they will happen, so software must handle them.

Without fault handling ‚Üí system may:

Deadlock permanently (unable to serve requests even after recovery).

Do disastrous things (e.g., accidentally delete all data).

Handling faults ‚â† always tolerating them:

If network usually reliable, it may be acceptable to just show users an error temporarily.

But you must:

Know how your software behaves under faults.

Ensure system can recover when network comes back.

Good practice: test by deliberately triggering faults (e.g., Chaos Monkey).


######  Detecting Faults


Many systems must detect faulty nodes automatically. Examples:

Load balancer ‚Üí must stop sending requests to dead nodes.

Distributed database (leader/follower) ‚Üí must promote a new leader if current leader fails.

Why Detection Is Hard

Because of network uncertainty, it‚Äôs hard to know if a node is really dead or just unreachable.

Possible Feedback Mechanisms

TCP-level feedback:

If machine reachable but no process listening ‚Üí OS replies with RST/FIN packet.

Limitation: if process crashed while handling a request, you don‚Äôt know how much work was done.

Crash notification via script:

If process crashed but OS still running ‚Üí script can notify others.

Example: HBase does this to avoid waiting for timeouts.

Switch management interfaces:

Can query hardware to detect link failures (e.g., machine powered off).

Not possible if:

Using the public internet.

Shared datacenter (no access to switches).

Network problem blocks access to management interface.

ICMP Destination Unreachable:

Routers may respond when they know an IP is unreachable.

But routers don‚Äôt have magical failure detection‚Äîstill subject to limitations.

Limits of Detection

Rapid feedback is useful but unreliable.

Even if TCP confirms packet delivery ‚Üí application may crash before processing it.

To be sure a request succeeded ‚Üí need a positive acknowledgment from the application itself.

If something goes wrong ‚Üí you may get an error, or you may get no response at all.

Usual practice:

Retry a few times.

Wait for timeout.

If still no response ‚Üí declare node dead.



######  Timeouts and Unbounded Delays


The Timeout Dilemma

Timeouts are the only reliable way of detecting faults in distributed systems.

But the key question is: how long should the timeout be?

Long timeouts:

System waits longer before declaring a node dead.

Users may experience long delays or error messages.

Short timeouts:

Faults are detected faster.

But higher risk of incorrectly marking a live node as dead (false positives).

Problems with Prematurely Declaring a Node Dead

If a node is falsely declared dead, several issues arise:

If the node was in the middle of an important task (e.g., sending an email), another node may retry it ‚Üí action happens twice.

When a node is marked dead, its responsibilities are transferred to other nodes ‚Üí this increases load.

If the system was already under high load, transferring more work may worsen the problem.

Cascading failure risk:

One overloaded node is marked dead.

Other nodes get its workload ‚Üí they overload and slow down.

Eventually, all nodes may declare each other dead ‚Üí system-wide outage.

A Hypothetical ‚ÄúBounded Delay‚Äù World

Imagine an ideal system:

Network guarantees maximum packet delay = d.

Every packet is either delivered within time d or lost.

Never longer than d.

Nodes guarantee maximum request handling time = r.

In such a case:

Every request should get a response within 2d + r.

If no response in that time, we know for sure that node or network failed.

Timeout = 2d + r would be perfect.

Reality: Asynchronous Networks with Unbounded Delays

Unfortunately, real systems don‚Äôt behave like that:

Networks are asynchronous, meaning no upper limit exists on how long packets might take to arrive.

Servers cannot guarantee they will always process requests within a fixed time.

Even if the system is fast most of the time, a small spike in delays can trigger false failure detection.


######  Network Congestion and Queueing


Network delays often come from queueing, similar to traffic congestion on roads.

1. Network Switch Queueing

If multiple nodes send packets to the same destination at once:

The switch queues them.

On busy links, packets must wait ‚Üí network congestion.

If the queue fills up ‚Üí packets are dropped and must be resent.

2. Destination Machine Queueing

If all CPU cores are busy:

Incoming packets are queued by the operating system until the app is ready.

Delay depends on machine load.

3. Virtualization Delays

In cloud/VM environments:

A VM can be paused for tens of milliseconds while another VM uses CPU.

During the pause, the VM cannot process network data.

Packets are buffered until VM resumes, adding delay.

4. TCP Flow Control (Congestion Avoidance)

TCP avoids overwhelming the network or receiving node.

It slows down sending rate when congestion risk is detected.

This adds extra sender-side queueing before data even enters the network.

Retransmissions in TCP

TCP assumes a packet is lost if it‚Äôs not acknowledged in time.

Lost packets are retransmitted after a timeout.

The application doesn‚Äôt see packet loss directly, but it does experience the extra delay caused by retransmission.


######  TCP Versus UDP


TCP: Reliable but suffers from delays due to retransmission and flow control.

UDP:

Used in latency-sensitive applications like VoIP and video conferencing.

No retransmission, no flow control ‚Üí avoids extra delays.

Still affected by switch queues and CPU scheduling.

Works better when delayed data is useless.

Example: VoIP ‚Äì if a packet arrives too late, it‚Äôs better to skip it and insert silence rather than replay late audio.

Effect of Queueing and Load

Delays are minimal in low-load systems with spare capacity.

Under high utilization, long queues build quickly ‚Üí large delays.

In public clouds or multi-tenant datacenters:

Resources (network links, switches, CPUs) are shared with many tenants.

Noisy neighbors (e.g., heavy MapReduce jobs) can cause high delays for others.

Network performance becomes unpredictable.

Choosing Timeouts in Practice

Since networks have unbounded delays, timeouts cannot be chosen theoretically.

Instead:

Measure response times and round-trip delays across many machines over long periods.

Study their distribution (how delays vary).

Pick a trade-off:

Faster fault detection (shorter timeouts).

Lower risk of false positives (longer timeouts).

Adaptive Timeout Mechanisms

Better than fixed timeouts ‚Üí use adaptive timeouts.

Systems monitor delays and automatically adjust thresholds:

Phi Accrual Failure Detector:

Used in Akka and Cassandra.

Tracks variability (jitter) in response times.

Adjusts timeout dynamically.

TCP retransmission timeouts:

Work similarly by adapting to observed round-trip times.



######  Synchronous Versus Asynchronous Networks



Distributed systems would be simpler if the network always delivered packets reliably, with fixed maximum delay and no packet loss. But this isn‚Äôt how real-world packet networks work. To see why, we compare telephone networks (synchronous) with computer networks (asynchronous).

Telephone Network Reliability

The traditional fixed-line (non-cellular, non-VoIP) telephone network is extremely reliable.

Calls rarely get delayed or dropped.

Why? Because:

A phone call needs low, predictable latency (for natural conversation).

It also needs enough bandwidth to continuously transfer audio.

Circuit Switching in Telephony

When you make a phone call, the telephone network establishes a circuit:

A fixed, guaranteed amount of bandwidth is reserved along the entire route.

This bandwidth is locked until the call ends.

Example:

ISDN runs at 4,000 frames per second.

A call gets 16 bits per frame in each direction.

So each side can always send 16 bits every 250 microseconds, guaranteed.

Result:

Data never waits in queues because the space in each frame is pre-reserved.

End-to-end latency is fixed (bounded delay).



######  Can We Not Make Computer Networks Predictable Too?



A telephone circuit ‚â† a TCP connection.

A circuit = fixed bandwidth reservation.

A TCP connection = opportunistic; uses available bandwidth as needed.

TCP differences:

You can send variable-sized blocks (emails, web pages, files).

TCP tries to finish as fast as possible using available capacity.

When idle, TCP uses no bandwidth (except rare keepalives).

If networks were circuit-switched like telephony:

We could guarantee fixed round-trip times.

But Ethernet and IP are packet-switched, not circuit-switched.

That means queueing happens, creating unbounded delays.

Why Packet Switching in Datacenters and Internet?

Reason: Packet switching works better for bursty traffic.

Circuits are fine for constant streams (voice/video).

But things like web requests, email, file transfers:

No fixed bandwidth needed.

Goal: complete transfer as quickly as possible.

With circuits:

Must guess bandwidth allocation.

If guess too low ‚Üí slow transfer, waste capacity.

If guess too high ‚Üí circuit can‚Äôt be set up (since bandwidth must be guaranteed).

With TCP packet switching:

Rate adjusts dynamically to available network capacity.

Utilization is much better for unpredictable/bursty workloads.

Hybrid Approaches

Some attempts tried mixing both worlds (circuit + packet switching):

ATM (Asynchronous Transfer Mode):

Competed with Ethernet in the 1980s.

Adopted mainly in telephone networks.

Not the same as ATM = cash machine.

InfiniBand:

Provides end-to-end flow control at the link layer.

Reduces queueing, though congestion can still delay packets.

Techniques like:

Quality of Service (QoS) ‚Üí prioritizing packets.

Admission Control ‚Üí limiting senders‚Äô rates.

Can make packet networks behave like circuit-switched ones.

This provides statistical (not strict) delay guarantees.



######   Latency and Resource Utilization


This is about why packet-switched networks allow higher utilization, even though they introduce variable delay.

Static Partitioning in Telephone Networks

Suppose a wire can carry 10,000 calls.

Each call occupies one slot.

The wire is divided statically:

If you‚Äôre the only caller, you still get the same fixed slot.

The unused slots remain wasted.

Advantage: predictable latency.

Disadvantage: poor utilization if not fully loaded.

Dynamic Sharing in Internet Networks

Internet bandwidth is shared dynamically.

Senders push packets as fast as possible.

Routers decide which packet goes next.

Results:

Downside ‚Üí Queueing + variable delay.

Upside ‚Üí Maximized utilization of the wire.

Since the wire has a fixed cost, better utilization makes each byte cheaper.

CPU Analogy

Similar trade-off exists in CPU scheduling:

If CPU cores are shared dynamically:

A thread may wait in the run queue.

Execution time becomes unpredictable.

But CPU utilization is higher.

If CPU cycles were statically allocated:

Each thread gets fixed share.

Predictable, but less efficient (unused capacity wasted).

Same reason virtual machines are used ‚Üí better hardware utilization.

Latency Guarantees vs Cost

Latency guarantees are possible if:

Resources are statically partitioned.

Example: dedicated hardware, exclusive bandwidth.

But this means:

Lower utilization.

More expensive.

Dynamic sharing (multi-tenancy) ‚Üí

Better utilization (cheaper).

But variable delays.

Conclusion: Variable delays are not fundamental laws of physics.

They are a trade-off between cost and reliability.

Current Reality

Today‚Äôs datacenters and public clouds:

Do not enable QoS guarantees.

Internet communication also has no guarantees.

Hence:

Congestion, queueing, unbounded delays are real.

No ‚Äúcorrect‚Äù timeout values exist.

Timeouts must be determined experimentally.

‚úÖ Summary in one line:
Networks could be perfectly reliable (like telephony circuits), but packet-switched internet and datacenter networks trade guaranteed
latency for higher utilization and lower cost‚Äîwhich is why software must always assume delays, drops, and retries.



######  Unreliable Clocks

Clocks and time are crucial in applications because many operations depend on them. They are used to measure durations (intervals between two events) and to record points in time (specific timestamps).

Examples of Clock Usage

Applications use clocks for different types of questions:

Has this request timed out yet?

What‚Äôs the 99th percentile response time of this service?

How many queries per second did this service handle in the last five minutes?

How long did the user spend on our site?

When was this article published?

At what date and time should the reminder email be sent?

When does this cache entry expire?

What is the timestamp on this error message in the log file?

Examples 1‚Äì4 ‚Üí measure durations (time intervals).

Examples 5‚Äì8 ‚Üí deal with points in time (calendar dates and timestamps).

The Challenge of Time in Distributed Systems

Communication takes time: messages don‚Äôt arrive instantly across a network.

The receive time is always later than the send time, but due to variable network delays, we cannot know exactly how much later.

This makes it hard to determine the true order of events across multiple machines.

Each machine has its own hardware clock (based on quartz oscillators), which are not perfectly accurate ‚Üí some run slightly fast, others slightly slow.

Synchronization is possible using NTP (Network Time Protocol), which adjusts a computer‚Äôs clock based on servers (often synced with GPS).

Still, synchronization is imperfect and introduces challenges.



######  Monotonic Versus Time-of-Day Clocks



Modern systems have two main types of clocks that serve different purposes:



######   1. Time-of-Day Clocks (Wall-Clock Time)

Returns the current date and time according to the calendar.

Examples:

clock_gettime(CLOCK_REALTIME) in Linux

System.currentTimeMillis() in Java

Measured as seconds or milliseconds since a fixed epoch (e.g., Jan 1, 1970 UTC).

Typically synchronized with NTP so that timestamps are meaningful across machines.

Problems:

Can jump backward (if NTP resets a fast clock).

Can jump forward suddenly.

May ignore leap seconds, leading to inaccuracies.

Historically had low resolution (e.g., 10 ms steps on older Windows).

‚Üí Unsuitable for measuring elapsed time (durations), but useful for recording actual timestamps.



######    2. Monotonic Clocks

Used to measure durations (e.g., timeouts, response times).

Examples:

clock_gettime(CLOCK_MONOTONIC) in Linux

System.nanoTime() in Java

Properties:

Always move forward (unlike time-of-day clocks).

Useful for checking elapsed time between two events.

Absolute value is meaningless (e.g., could be time since machine boot).

Cannot be compared across different computers.

Challenges:

Multi-CPU servers may have unsynchronized timers per CPU. OS tries to smooth this, but glitches may still occur.

NTP can ‚Äúslew‚Äù the monotonic clock speed slightly (¬±0.05%) to adjust for drift, but cannot make it jump.

Usually has high resolution (microseconds or better).

‚Üí Best for measuring durations locally, but not suitable for global ordering of events across nodes.



######   Clock Synchronization and Accuracy



 Monotonic clocks: do not need synchronization (they‚Äôre local).

Time-of-day clocks: must be synchronized (e.g., using NTP).

But achieving accurate synchronization is harder than expected:

Problems with Clocks and NTP

Quartz clock drift

Clocks naturally drift due to temperature and hardware imperfections.

Google assumes 200 ppm drift (~6 ms error every 30 seconds, ~17 s per day if not resynced).

Large adjustments

If a clock is too far off, NTP may reset it abruptly ‚Üí time jumps backward or forward.

Network issues

If a server is cut off from NTP (e.g., firewall misconfig), drift grows unnoticed.

Synchronization depends on network delays:

Best achievable over internet: ~35 ms error.

But spikes can cause ~1 second errors.

In some cases, clients may give up entirely.

Bad/misconfigured NTP servers

Some servers report incorrect times (off by hours).

Clients usually cross-check with multiple servers, but risk remains.

Leap seconds

Create 59- or 61-second minutes.

Many systems fail if not designed for them ‚Üí have caused real-world crashes.

Some providers ‚Äúsmear‚Äù leap seconds (gradually apply adjustment).

Virtual machines

Virtualized hardware clocks add complexity.

VM pauses (when CPU is given to another VM) appear as sudden clock jumps forward.

Untrusted devices

On user-controlled devices (mobiles, embedded), clocks may be manually set wrong (e.g., to cheat in games).

Achieving High Accuracy (Special Cases)

With enough effort, extremely accurate clocks can be maintained.

Example: MiFID II regulation in Europe ‚Üí financial firms must sync clocks within 100 microseconds of UTC to detect market anomalies.

Achieved using:

GPS receivers

PTP (Precision Time Protocol)

Careful monitoring and setup.

But:

This requires significant cost and expertise.

Any misconfig (e.g., broken NTP daemon, firewall issue) can quickly lead to large clock errors.

‚úÖ Summary:

Durations ‚Üí use monotonic clocks (local, don‚Äôt jump).

Timestamps ‚Üí use time-of-day clocks (but beware drift, jumps, leap seconds, and synchronization issues).

Distributed systems cannot fully rely on clocks for correctness ‚Üí they must assume that clocks can be wrong, out of sync, or misleading.



