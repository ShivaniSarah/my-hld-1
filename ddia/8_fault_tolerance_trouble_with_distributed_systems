CHAPTER 8 ‚Äì The Trouble with Distributed Systems
Introduction

This chapter builds on earlier discussions of failures in distributed systems like:

Replica failover (what happens when a node goes down)

Replication lag (inconsistencies when replicas are behind)

Concurrency control (problems when multiple transactions run at the same time).

So far, the picture was already concerning. But here, the author says: we were still too optimistic.

In reality, anything that can go wrong will go wrong.

Engineers working with distributed systems often adopt this pessimistic view because experience shows failures happen in strange, 
unpredictable ways.

Key Difference: Single Computer vs. Distributed System

On a single computer, behavior is deterministic: if the hardware works, the same input gives the same output.

On a distributed system, things are messier because computers must communicate over a network, and many new types of failures appear.

The chapter‚Äôs aim:

Expose problems that arise in real distributed systems.

Understand what can and cannot be relied on.

Set the stage for Chapter 9, which introduces algorithms to handle these challenges.



###### Faults and Partial Failures



Behavior of a Single Computer

Normally: it works or doesn‚Äôt work.

Software bugs might cause weird behavior (like ‚Äúbad day‚Äù crashes), but:

Good hardware + good software = deterministic execution.

Hardware problems usually cause total failure (e.g., kernel panic, blue screen).

üëâ So, computers are designed to fail completely rather than return wrong results.

Returning wrong results is dangerous because it causes confusion.

Instead, systems crash deterministically.

Example: CPU instructions always behave the same way; memory/disk writes remain intact.

This design principle of always-correct computation traces back to the very first digital computers.

Behavior of Distributed Systems

Multiple computers connected by a network cannot guarantee the same idealized model.

The messy physical world introduces:

Network partitions (some nodes can‚Äôt reach others).

Hardware failures at different levels (switches, PDUs, racks, even entire data centers).

Unexpected accidents (like the anecdote of a truck crashing into a data center‚Äôs HVAC system).

üëâ In short, some parts of the system may fail while others work fine.

Partial Failures

Definition: When some parts of the system fail, but others continue working.

Problem: These failures are nondeterministic.

A request involving multiple nodes may sometimes succeed and sometimes fail in unpredictable ways.

Example: You might send a message but never know if it:

Reached the destination,

Got lost in the network, or

Arrived late after a timeout.

üëâ Message travel time is also nondeterministic, making it even harder to reason about success or failure.

Why Distributed Systems Are Hard

On a single computer, failures are simple: either the whole thing works or it doesn‚Äôt.

In a distributed system, failures can be:

Partial ‚Üí some nodes respond, others don‚Äôt.

Unpredictable ‚Üí the same operation can behave differently each time.

Invisible ‚Üí you may not know whether an operation succeeded.

Thus, partial failure + nondeterminism = the core difficulty of distributed systems.






I‚Äôll explain this section with the same headings as in your text so you don‚Äôt miss any point.

######  Cloud Computing and Supercomputing

There are different philosophies for building large-scale computing systems.

High-Performance Computing (HPC) / Supercomputing

Used for computationally intensive scientific tasks (e.g., weather forecasting, molecular dynamics).

Built from supercomputers with thousands of CPUs.

Cloud Computing

Not sharply defined, but usually refers to:

Multi-tenant datacenters (many customers share resources).

Commodity machines connected with IP networks (Ethernet).

Elastic/on-demand resource allocation (scale up or down easily).

Metered billing (pay for what you use).

Enterprise Datacenters

Sit between HPC and cloud computing in design and philosophy.

Fault Handling Approaches
Supercomputers (HPC)

Strategy: checkpoint and restart.

Jobs periodically checkpoint state to durable storage.

If one node fails ‚Üí entire cluster stops.

After fixing the node, computation restarts from last checkpoint.

This makes supercomputers more like single-node systems:

Partial failure is escalated to total failure (just like a single machine crash).

Internet Services (Focus of the Book)

Very different requirements:

Always online ‚Üí services must be available with low latency; cannot stop the cluster for repairs.

Hardware differences:

Supercomputers use specialized, reliable hardware and communication methods like shared memory or RDMA.

Cloud systems use commodity hardware (cheaper but less reliable).

Network differences:

Cloud datacenters: IP/Ethernet networks in Clos topologies (for high bandwidth).

Supercomputers: specialized topologies (meshes, toruses) optimized for known communication patterns.

Scale and likelihood of failure:

The bigger the system, the higher the chance something is broken.

In huge clusters, something is always broken.

If your only fault strategy is ‚Äústop everything,‚Äù most of the system‚Äôs time will be wasted recovering instead of doing useful work.

Fault tolerance as a feature:

Systems that tolerate node failures without stopping are very valuable.

Enables rolling upgrades ‚Üí restart nodes one at a time while service keeps running.

In the cloud: if a VM is slow, just kill it and request a new one.

Geographical distribution:

Internet services often replicate data across the globe to reduce latency for users.

This means relying on the internet ‚Üí slower and less reliable than local supercomputer networks.

Supercomputers assume all nodes are in the same place.

Accepting Partial Failures

To build distributed systems, we must accept partial failures.

The goal: build reliable systems out of unreliable components.

No such thing as perfect reliability, so we must know the limits of guarantees we can realistically provide.

Even in Small Systems

Partial failure still happens, just less often.

Over time, some component will fail, so software must handle it gracefully.

Fault handling must be designed into the software.

Operators must know how the software behaves during faults.

üëâ Assuming faults are rare and just ‚Äúhoping for the best‚Äù is dangerous.

Must consider a wide range of possible faults.

Even unlikely faults should be tested ‚Üí artificially create them in test environments.

In distributed systems, suspicion, pessimism, and paranoia pay off.



######  Building a Reliable System from Unreliable Components



At first glance, this seems impossible (a chain is only as strong as its weakest link).
But computing has long used the idea of building reliability on top of unreliable foundations.

Examples

Error-Correcting Codes (ECC)

Used to transmit data accurately over channels that occasionally flip bits.

Example: wireless networks affected by radio interference.

TCP over IP

IP (Internet Protocol) is unreliable: can drop, delay, duplicate, or reorder packets.

TCP (Transmission Control Protocol) adds reliability:

Retransmits lost packets.

Eliminates duplicates.

Reassembles packets in correct order.

Limits of Reliability

The higher-level system is more reliable, but not infinitely so.

Example:

ECC can handle a small number of errors, but if the signal is overwhelmed by interference, it fails.

TCP can hide loss/duplication/reordering, but cannot hide network delays.

Why This Matters

Even though reliability is not perfect, it‚Äôs still valuable.

By handling low-level failures, these mechanisms make higher-level systems easier to reason about.

The book will explore this further in ‚ÄúThe End-to-End Argument‚Äù (Chapter 9, p. 519).





######  Unreliable Networks


Shared-Nothing Systems

The distributed systems we study are usually shared-nothing systems:

Each machine has its own memory and disk.

One machine cannot directly access another‚Äôs memory/disk.

Communication happens only via the network.

Reasons this model is dominant for internet services:

Cheap ‚Üí no special hardware needed.

Commodity cloud computing can be used.

High reliability possible via redundancy across multiple datacenters.

Asynchronous Packet Networks

Internet and datacenter networks (Ethernet) are asynchronous packet networks.

Meaning:

One node can send a packet to another.

But the network makes no guarantees:

When the packet will arrive.

Whether it will arrive at all.

Things That Can Go Wrong (if you send a request and expect a response):

Request lost (e.g., cable unplugged).

Request delayed in queue (network/recipient overloaded).

Remote node failed (crash or power down).

Remote node temporarily paused (e.g., long garbage collection stop).

Request processed but response lost (e.g., misconfigured switch).

Request processed but response delayed (network overload or sender overload).

üëâ From the sender‚Äôs perspective, you cannot distinguish the cause‚Äîall you know is ‚Äúno response yet.‚Äù

Timeouts

Usual solution: use a timeout.

Wait for some period.

If no response ‚Üí assume failure.

Problem:

Even after timeout, you don‚Äôt know if the request was delivered.

Request might still be queued and eventually reach recipient even though sender gave up.


######  Network Faults in Practice


Reality Check

We‚Äôve built networks for decades, but they are still unreliable.

Studies show network problems are common, even in controlled datacenters.

Evidence

Medium-sized datacenter: ~12 network faults/month.

Half ‚Üí disconnect a single machine.

Half ‚Üí disconnect an entire rack.

Failures in top-of-rack switches, aggregation switches, load balancers are common.

Redundancy helps less than expected ‚Üí human error (misconfigured switches) is a big cause.

Cloud vs Private Datacenters

Public clouds (e.g., EC2) ‚Üí frequent transient glitches.

Well-managed private datacenters ‚Üí usually more stable.

Examples of Surprising Faults

Switch software upgrade ‚Üí network topology reconfig, causing packet delays > 1 minute.

Sharks biting undersea cables damaging connectivity.

One-way failure ‚Üí network interface drops inbound packets but still sends outbound packets.

Network Partitions

If part of the network is cut off from the rest, this is called a network partition (or netsplit).

Book prefers ‚Äúnetwork fault‚Äù to avoid confusion with data partitions (shards).

Importance of Handling Network Faults

Even if rare, they will happen, so software must handle them.

Without fault handling ‚Üí system may:

Deadlock permanently (unable to serve requests even after recovery).

Do disastrous things (e.g., accidentally delete all data).

Handling faults ‚â† always tolerating them:

If network usually reliable, it may be acceptable to just show users an error temporarily.

But you must:

Know how your software behaves under faults.

Ensure system can recover when network comes back.

Good practice: test by deliberately triggering faults (e.g., Chaos Monkey).


######  Detecting Faults


Many systems must detect faulty nodes automatically. Examples:

Load balancer ‚Üí must stop sending requests to dead nodes.

Distributed database (leader/follower) ‚Üí must promote a new leader if current leader fails.

Why Detection Is Hard

Because of network uncertainty, it‚Äôs hard to know if a node is really dead or just unreachable.

Possible Feedback Mechanisms

TCP-level feedback:

If machine reachable but no process listening ‚Üí OS replies with RST/FIN packet.

Limitation: if process crashed while handling a request, you don‚Äôt know how much work was done.

Crash notification via script:

If process crashed but OS still running ‚Üí script can notify others.

Example: HBase does this to avoid waiting for timeouts.

Switch management interfaces:

Can query hardware to detect link failures (e.g., machine powered off).

Not possible if:

Using the public internet.

Shared datacenter (no access to switches).

Network problem blocks access to management interface.

ICMP Destination Unreachable:

Routers may respond when they know an IP is unreachable.

But routers don‚Äôt have magical failure detection‚Äîstill subject to limitations.

Limits of Detection

Rapid feedback is useful but unreliable.

Even if TCP confirms packet delivery ‚Üí application may crash before processing it.

To be sure a request succeeded ‚Üí need a positive acknowledgment from the application itself.

If something goes wrong ‚Üí you may get an error, or you may get no response at all.

Usual practice:

Retry a few times.

Wait for timeout.

If still no response ‚Üí declare node dead.



######  Timeouts and Unbounded Delays


The Timeout Dilemma

Timeouts are the only reliable way of detecting faults in distributed systems.

But the key question is: how long should the timeout be?

Long timeouts:

System waits longer before declaring a node dead.

Users may experience long delays or error messages.

Short timeouts:

Faults are detected faster.

But higher risk of incorrectly marking a live node as dead (false positives).

Problems with Prematurely Declaring a Node Dead

If a node is falsely declared dead, several issues arise:

If the node was in the middle of an important task (e.g., sending an email), another node may retry it ‚Üí action happens twice.

When a node is marked dead, its responsibilities are transferred to other nodes ‚Üí this increases load.

If the system was already under high load, transferring more work may worsen the problem.

Cascading failure risk:

One overloaded node is marked dead.

Other nodes get its workload ‚Üí they overload and slow down.

Eventually, all nodes may declare each other dead ‚Üí system-wide outage.

A Hypothetical ‚ÄúBounded Delay‚Äù World

Imagine an ideal system:

Network guarantees maximum packet delay = d.

Every packet is either delivered within time d or lost.

Never longer than d.

Nodes guarantee maximum request handling time = r.

In such a case:

Every request should get a response within 2d + r.

If no response in that time, we know for sure that node or network failed.

Timeout = 2d + r would be perfect.

Reality: Asynchronous Networks with Unbounded Delays

Unfortunately, real systems don‚Äôt behave like that:

Networks are asynchronous, meaning no upper limit exists on how long packets might take to arrive.

Servers cannot guarantee they will always process requests within a fixed time.

Even if the system is fast most of the time, a small spike in delays can trigger false failure detection.


######  Network Congestion and Queueing


Network delays often come from queueing, similar to traffic congestion on roads.

1. Network Switch Queueing

If multiple nodes send packets to the same destination at once:

The switch queues them.

On busy links, packets must wait ‚Üí network congestion.

If the queue fills up ‚Üí packets are dropped and must be resent.

2. Destination Machine Queueing

If all CPU cores are busy:

Incoming packets are queued by the operating system until the app is ready.

Delay depends on machine load.

3. Virtualization Delays

In cloud/VM environments:

A VM can be paused for tens of milliseconds while another VM uses CPU.

During the pause, the VM cannot process network data.

Packets are buffered until VM resumes, adding delay.

4. TCP Flow Control (Congestion Avoidance)

TCP avoids overwhelming the network or receiving node.

It slows down sending rate when congestion risk is detected.

This adds extra sender-side queueing before data even enters the network.

Retransmissions in TCP

TCP assumes a packet is lost if it‚Äôs not acknowledged in time.

Lost packets are retransmitted after a timeout.

The application doesn‚Äôt see packet loss directly, but it does experience the extra delay caused by retransmission.


######  TCP Versus UDP


TCP: Reliable but suffers from delays due to retransmission and flow control.

UDP:

Used in latency-sensitive applications like VoIP and video conferencing.

No retransmission, no flow control ‚Üí avoids extra delays.

Still affected by switch queues and CPU scheduling.

Works better when delayed data is useless.

Example: VoIP ‚Äì if a packet arrives too late, it‚Äôs better to skip it and insert silence rather than replay late audio.

Effect of Queueing and Load

Delays are minimal in low-load systems with spare capacity.

Under high utilization, long queues build quickly ‚Üí large delays.

In public clouds or multi-tenant datacenters:

Resources (network links, switches, CPUs) are shared with many tenants.

Noisy neighbors (e.g., heavy MapReduce jobs) can cause high delays for others.

Network performance becomes unpredictable.

Choosing Timeouts in Practice

Since networks have unbounded delays, timeouts cannot be chosen theoretically.

Instead:

Measure response times and round-trip delays across many machines over long periods.

Study their distribution (how delays vary).

Pick a trade-off:

Faster fault detection (shorter timeouts).

Lower risk of false positives (longer timeouts).

Adaptive Timeout Mechanisms

Better than fixed timeouts ‚Üí use adaptive timeouts.

Systems monitor delays and automatically adjust thresholds:

Phi Accrual Failure Detector:

Used in Akka and Cassandra.

Tracks variability (jitter) in response times.

Adjusts timeout dynamically.

TCP retransmission timeouts:

Work similarly by adapting to observed round-trip times.



######  Synchronous Versus Asynchronous Networks



Distributed systems would be simpler if the network always delivered packets reliably, with fixed maximum delay and no packet loss. 
But this isn‚Äôt how real-world packet networks work. To see why, we compare telephone networks (synchronous) with computer networks 
(asynchronous).

Telephone Network Reliability

The traditional fixed-line (non-cellular, non-VoIP) telephone network is extremely reliable.

Calls rarely get delayed or dropped.

Why? Because:

A phone call needs low, predictable latency (for natural conversation).

It also needs enough bandwidth to continuously transfer audio.

Circuit Switching in Telephony

When you make a phone call, the telephone network establishes a circuit:

A fixed, guaranteed amount of bandwidth is reserved along the entire route.

This bandwidth is locked until the call ends.

Example:

ISDN runs at 4,000 frames per second.

A call gets 16 bits per frame in each direction.

So each side can always send 16 bits every 250 microseconds, guaranteed.

Result:

Data never waits in queues because the space in each frame is pre-reserved.

End-to-end latency is fixed (bounded delay).



######  Can We Not Make Computer Networks Predictable Too?



A telephone circuit ‚â† a TCP connection.

A circuit = fixed bandwidth reservation.

A TCP connection = opportunistic; uses available bandwidth as needed.

TCP differences:

You can send variable-sized blocks (emails, web pages, files).

TCP tries to finish as fast as possible using available capacity.

When idle, TCP uses no bandwidth (except rare keepalives).

If networks were circuit-switched like telephony:

We could guarantee fixed round-trip times.

But Ethernet and IP are packet-switched, not circuit-switched.

That means queueing happens, creating unbounded delays.

Why Packet Switching in Datacenters and Internet?

Reason: Packet switching works better for bursty traffic.

Circuits are fine for constant streams (voice/video).

But things like web requests, email, file transfers:

No fixed bandwidth needed.

Goal: complete transfer as quickly as possible.

With circuits:

Must guess bandwidth allocation.

If guess too low ‚Üí slow transfer, waste capacity.

If guess too high ‚Üí circuit can‚Äôt be set up (since bandwidth must be guaranteed).

With TCP packet switching:

Rate adjusts dynamically to available network capacity.

Utilization is much better for unpredictable/bursty workloads.

Hybrid Approaches

Some attempts tried mixing both worlds (circuit + packet switching):

ATM (Asynchronous Transfer Mode):

Competed with Ethernet in the 1980s.

Adopted mainly in telephone networks.

Not the same as ATM = cash machine.

InfiniBand:

Provides end-to-end flow control at the link layer.

Reduces queueing, though congestion can still delay packets.

Techniques like:

Quality of Service (QoS) ‚Üí prioritizing packets.

Admission Control ‚Üí limiting senders‚Äô rates.

Can make packet networks behave like circuit-switched ones.

This provides statistical (not strict) delay guarantees.



######   Latency and Resource Utilization


This is about why packet-switched networks allow higher utilization, even though they introduce variable delay.

Static Partitioning in Telephone Networks

Suppose a wire can carry 10,000 calls.

Each call occupies one slot.

The wire is divided statically:

If you‚Äôre the only caller, you still get the same fixed slot.

The unused slots remain wasted.

Advantage: predictable latency.

Disadvantage: poor utilization if not fully loaded.

Dynamic Sharing in Internet Networks

Internet bandwidth is shared dynamically.

Senders push packets as fast as possible.

Routers decide which packet goes next.

Results:

Downside ‚Üí Queueing + variable delay.

Upside ‚Üí Maximized utilization of the wire.

Since the wire has a fixed cost, better utilization makes each byte cheaper.

CPU Analogy

Similar trade-off exists in CPU scheduling:

If CPU cores are shared dynamically:

A thread may wait in the run queue.

Execution time becomes unpredictable.

But CPU utilization is higher.

If CPU cycles were statically allocated:

Each thread gets fixed share.

Predictable, but less efficient (unused capacity wasted).

Same reason virtual machines are used ‚Üí better hardware utilization.

Latency Guarantees vs Cost

Latency guarantees are possible if:

Resources are statically partitioned.

Example: dedicated hardware, exclusive bandwidth.

But this means:

Lower utilization.

More expensive.

Dynamic sharing (multi-tenancy) ‚Üí

Better utilization (cheaper).

But variable delays.

Conclusion: Variable delays are not fundamental laws of physics.

They are a trade-off between cost and reliability.

Current Reality

Today‚Äôs datacenters and public clouds:

Do not enable QoS guarantees.

Internet communication also has no guarantees.

Hence:

Congestion, queueing, unbounded delays are real.

No ‚Äúcorrect‚Äù timeout values exist.

Timeouts must be determined experimentally.

‚úÖ Summary in one line:
Networks could be perfectly reliable (like telephony circuits), but packet-switched internet and datacenter networks trade guaranteed
latency for higher utilization and lower cost‚Äîwhich is why software must always assume delays, drops, and retries.



######  Unreliable Clocks

Clocks and time are crucial in applications because many operations depend on them. They are used to measure durations (intervals 
between two events) and to record points in time (specific timestamps).

Examples of Clock Usage

Applications use clocks for different types of questions:

Has this request timed out yet?

What‚Äôs the 99th percentile response time of this service?

How many queries per second did this service handle in the last five minutes?

How long did the user spend on our site?

When was this article published?

At what date and time should the reminder email be sent?

When does this cache entry expire?

What is the timestamp on this error message in the log file?

Examples 1‚Äì4 ‚Üí measure durations (time intervals).

Examples 5‚Äì8 ‚Üí deal with points in time (calendar dates and timestamps).

The Challenge of Time in Distributed Systems

Communication takes time: messages don‚Äôt arrive instantly across a network.

The receive time is always later than the send time, but due to variable network delays, we cannot know exactly how much later.

This makes it hard to determine the true order of events across multiple machines.

Each machine has its own hardware clock (based on quartz oscillators), which are not perfectly accurate ‚Üí some run slightly fast, 
others slightly slow.

Synchronization is possible using NTP (Network Time Protocol), which adjusts a computer‚Äôs clock based on servers (often synced with GPS).

Still, synchronization is imperfect and introduces challenges.



######  Monotonic Versus Time-of-Day Clocks



Modern systems have two main types of clocks that serve different purposes:



######   1. Time-of-Day Clocks (Wall-Clock Time)

Returns the current date and time according to the calendar.

Examples:

clock_gettime(CLOCK_REALTIME) in Linux

System.currentTimeMillis() in Java

Measured as seconds or milliseconds since a fixed epoch (e.g., Jan 1, 1970 UTC).

Typically synchronized with NTP so that timestamps are meaningful across machines.

Problems:

Can jump backward (if NTP resets a fast clock).

Can jump forward suddenly.

May ignore leap seconds, leading to inaccuracies.

Historically had low resolution (e.g., 10 ms steps on older Windows).

‚Üí Unsuitable for measuring elapsed time (durations), but useful for recording actual timestamps.



######    2. Monotonic Clocks

Used to measure durations (e.g., timeouts, response times).

Examples:

clock_gettime(CLOCK_MONOTONIC) in Linux

System.nanoTime() in Java

Properties:

Always move forward (unlike time-of-day clocks).

Useful for checking elapsed time between two events.

Absolute value is meaningless (e.g., could be time since machine boot).

Cannot be compared across different computers.

Challenges:

Multi-CPU servers may have unsynchronized timers per CPU. OS tries to smooth this, but glitches may still occur.

NTP can ‚Äúslew‚Äù the monotonic clock speed slightly (¬±0.05%) to adjust for drift, but cannot make it jump.

Usually has high resolution (microseconds or better).

‚Üí Best for measuring durations locally, but not suitable for global ordering of events across nodes.



######   Clock Synchronization and Accuracy



 Monotonic clocks: do not need synchronization (they‚Äôre local).

Time-of-day clocks: must be synchronized (e.g., using NTP).

But achieving accurate synchronization is harder than expected:

Problems with Clocks and NTP

Quartz clock drift

Clocks naturally drift due to temperature and hardware imperfections.

Google assumes 200 ppm drift (~6 ms error every 30 seconds, ~17 s per day if not resynced).

Large adjustments

If a clock is too far off, NTP may reset it abruptly ‚Üí time jumps backward or forward.

Network issues

If a server is cut off from NTP (e.g., firewall misconfig), drift grows unnoticed.

Synchronization depends on network delays:

Best achievable over internet: ~35 ms error.

But spikes can cause ~1 second errors.

In some cases, clients may give up entirely.

Bad/misconfigured NTP servers

Some servers report incorrect times (off by hours).

Clients usually cross-check with multiple servers, but risk remains.

Leap seconds

Create 59- or 61-second minutes.

Many systems fail if not designed for them ‚Üí have caused real-world crashes.

Some providers ‚Äúsmear‚Äù leap seconds (gradually apply adjustment).

Virtual machines

Virtualized hardware clocks add complexity.

VM pauses (when CPU is given to another VM) appear as sudden clock jumps forward.

Untrusted devices

On user-controlled devices (mobiles, embedded), clocks may be manually set wrong (e.g., to cheat in games).

Achieving High Accuracy (Special Cases)

With enough effort, extremely accurate clocks can be maintained.

Example: MiFID II regulation in Europe ‚Üí financial firms must sync clocks within 100 microseconds of UTC to detect market anomalies.

Achieved using:

GPS receivers

PTP (Precision Time Protocol)

Careful monitoring and setup.

But:

This requires significant cost and expertise.

Any misconfig (e.g., broken NTP daemon, firewall issue) can quickly lead to large clock errors.

‚úÖ Summary:

Durations ‚Üí use monotonic clocks (local, don‚Äôt jump).

Timestamps ‚Üí use time-of-day clocks (but beware drift, jumps, leap seconds, and synchronization issues).

Distributed systems cannot fully rely on clocks for correctness ‚Üí they must assume that clocks can be wrong, out of sync, or misleading.




######  Relying on Synchronized Clocks



Clocks appear simple, but in distributed systems they come with many pitfalls. Time may drift, jump backward, or differ between nodes, 
which can cause subtle failures if software depends on synchronized clocks.

The Problem with Clocks

A day is not always exactly 86,400 seconds (because of leap seconds).

Time-of-day clocks may jump backward or forward when corrected.

Different nodes often have different clock values, despite synchronization.

Just like networks can delay or drop packets, clocks can be faulty. Robust software must assume that clocks are sometimes wrong and
handle this gracefully.

The challenge is that clock problems often go unnoticed:

If a CPU or network is broken ‚Üí the machine fails obviously.

If a clock drifts or NTP is misconfigured ‚Üí the system appears normal, but time slowly drifts out of sync.

The result is not a crash, but silent data loss or incorrect ordering.

‚û°Ô∏è Solution: Systems that rely on synchronized clocks must monitor clock offsets across machines. If a node drifts too far from the 
cluster‚Äôs clocks, it should be declared dead and removed.



######  Timestamps for Ordering Events


One tempting (but dangerous) use of synchronized clocks is to order events across nodes.

Example: Multi-Leader Replication

Client A writes x = 1 on Node 1, replicated to Node 3.

Client B increments x on Node 3 ‚Üí now x = 2.

Both writes are replicated to Node 2.

Problem:

Node 1‚Äôs write has timestamp 42.004s,

Node 3‚Äôs write has timestamp 42.003s,

Even though B‚Äôs write happened later, it looks earlier due to clock skew.

Node 2 concludes wrongly: x = 1 is more recent, so it discards B‚Äôs update.

This is the Last Write Wins (LWW) conflict resolution strategy.

Problems with Last Write Wins (LWW)

Writes can disappear silently

A node with a slow clock cannot overwrite a value written by a node with a faster clock until skew is corrected.

This causes arbitrary data loss, with no error reported.

Cannot distinguish sequential vs. concurrent writes

In the example, B‚Äôs increment definitely happened after A‚Äôs write.

But LWW treats it the same as if both writes were concurrent.

To fix this, we need causality tracking (e.g., version vectors).

Same timestamps possible

If two nodes generate writes at the same millisecond, timestamps collide.

Systems add a tiebreaker (random number, unique ID).

But this can still violate causality.

Why NTP Can‚Äôt Fully Solve This

Even with tight synchronization, clock errors persist.

A packet can be sent at 100 ms (sender‚Äôs clock) but arrive at 99 ms (receiver‚Äôs clock).

This makes it look like the message arrived before it was sent, which is impossible.

Conclusion: Relying only on physical clocks (time-of-day, monotonic) is unsafe for ordering.

Logical vs Physical Clocks

Physical clocks: measure real time (wall-clock or monotonic).

Logical clocks: measure event ordering, not real time.

Use counters instead of quartz crystals.

Guarantee causal ordering without relying on synchronization.

Safer for distributed systems.




######   Clock Readings Have a Confidence Interval



Even if you read a clock with microsecond or nanosecond resolution, it‚Äôs not that accurate:

Quartz clocks drift several ms per minute.

Best-case NTP (local network) = tens of ms accuracy.

Internet NTP = hundreds of ms error under congestion.

So, a clock reading is not a single exact time ‚Üí it‚Äôs a range with uncertainty.

Example:

System may be 95% confident that the time is between 10.3s and 10.5s.

If uncertainty is ¬±100 ms, then microsecond digits in a timestamp are meaningless.

Uncertainty =

Quartz drift since last sync.

NTP server‚Äôs uncertainty.

Network round-trip delay.

‚ö†Ô∏è Most systems don‚Äôt expose uncertainty. For example, clock_gettime() just returns a number, not an error bound.

Google‚Äôs TrueTime API (Spanner)

Google‚Äôs Spanner database addresses this problem:

TrueTime returns an interval: [earliest, latest].

Actual time is guaranteed to be within this range.

Interval width depends on:

Quartz drift since last sync,

Sync frequency,

Clock source (GPS or atomic clocks).

Using Confidence Intervals

If intervals don‚Äôt overlap ‚Üí ordering is guaranteed.

If intervals overlap ‚Üí ordering is uncertain.

Commit-Wait Protocol

Spanner waits for the uncertainty interval before committing.

Ensures transaction timestamps reflect causality.

To keep wait short, Spanner minimizes uncertainty using GPS receivers or atomic clocks.

This keeps uncertainty down to about 7 ms.


######   Synchronized Clocks for Global Snapshots


Snapshot isolation allows long-running read-only queries to see a consistent snapshot.

On a single-node DB ‚Üí transaction IDs are simple counters.

In distributed DBs ‚Üí generating global monotonically increasing IDs is hard.

Can we use synchronized clocks as transaction IDs?

If clocks were accurate enough ‚Üí yes.

But uncertainty prevents correctness.

Spanner‚Äôs Solution

Uses TrueTime intervals as transaction IDs.

By waiting out uncertainty, it ensures consistent ordering across datacenters.

Research & Outlook

Using synchronized clocks for transaction semantics is still an active research area.

Outside of Google‚Äôs Spanner, mainstream databases do not yet implement such approaches.

‚úÖ That‚Äôs the complete breakdown of Relying on Synchronized Clocks, explained with all key points intact.

Would you like me to also make a diagram/flowchart showing:

Physical clocks vs Logical clocks,

Where confidence intervals come in,

And how Spanner‚Äôs TrueTime avoids overlap?





###### Process Pauses



In distributed systems, using clocks for leader election or leases can be dangerous because processes may be unexpectedly paused 
for long periods. Let‚Äôs go step by step.

Leader Leases and Clock Reliance

In a system with a single leader per partition, only the leader can accept writes.

To remain leader, a node may hold a lease (like a lock with an expiry time).

A lease guarantees leadership for a fixed period (e.g., 30 seconds).

To continue being leader, the node must periodically renew the lease before it expires.

If it fails, another node can safely take over.

Example loop (simplified):

Continuously process incoming requests.

Before handling, check lease expiration.

Renew lease if too close to expiry.

Process request if lease is still valid.

What‚Äôs wrong with this?

Synchronized clock problem:

Lease expiry is often set by another machine.

Comparing it with the local clock assumes clocks are perfectly synchronized.

If clocks are skewed, the node may think it‚Äôs still leader when it‚Äôs not.

Process pause problem:

Even if only the local monotonic clock is used, the code assumes that very little time passes between checking the lease and processing
the request.

If the program unexpectedly pauses (e.g., 15 seconds), the lease may expire mid-processing, meaning another leader has taken over ‚Äî
but the paused node doesn‚Äôt know.

Why Can Threads Pause for So Long?

It‚Äôs not just a hypothetical risk. Several real-world factors can pause execution:

Garbage Collection (GC) in runtimes

Languages like Java use stop-the-world GC.

Threads may be paused for seconds or even minutes.

Even concurrent GCs (like CMS) still need occasional global pauses.

Virtual Machine suspension

Virtual machines can be paused and resumed (e.g., live migration).

Duration depends on how fast memory state can be copied.

Laptop or end-user devices

Closing a laptop lid suspends execution arbitrarily.

Context switching

OS or hypervisor may switch execution to another thread or VM.

In VMs, this is known as steal time.

Under load, threads may wait a long time to resume.

Disk I/O pauses

Blocking disk operations can stall a thread.

Sometimes hidden (e.g., Java classloader loads files lazily).

Worse if the disk is network-based (e.g., Amazon EBS).

Paging (swapping to disk)

If RAM is low, memory pages get swapped to disk.

Page faults can cause long pauses.

Severe case = thrashing (system spends all time swapping).

Signals like SIGSTOP

A Unix process can be paused (e.g., via Ctrl-Z).

It won‚Äôt resume until explicitly continued.

Key point: Any thread may pause at any line of code without warning. Meanwhile, the world (other nodes, clients, network) moves on, 
possibly declaring the node dead.



######  Response Time Guarantees




Some systems must respond predictably within deadlines (e.g., airbag systems, aircraft, robotics).

These are hard real-time systems.

If a deadline is missed, the system may fail catastrophically.

Characteristics of real-time systems:

Use a real-time operating system (RTOS) with guaranteed CPU scheduling.

Libraries must state worst-case execution times.

Dynamic memory allocation may be restricted.

Testing ensures deadlines are always met.


‚ö†Ô∏è Note:

Real-time ‚â† High performance.

Real-time prioritizes predictability, even at the cost of throughput.

Developing such systems is expensive and limited to safety-critical domains.

For server-side distributed systems, real-time is not economical. They run in non-real-time environments and thus must tolerate pauses.




######   Limiting the Impact of Garbage Collection

Even without full real-time guarantees, some strategies reduce GC‚Äôs negative impact:

Treat GC pauses like planned outages

Runtime can warn when a GC is about to happen.

The node stops receiving new requests, finishes ongoing work, then pauses.

Other nodes temporarily handle client traffic.

This approach reduces high tail latencies.

Used in latency-sensitive domains like financial trading.

GC only for short-lived objects + periodic restarts

GC of short-lived objects is quick.

Instead of letting long-lived objects accumulate, restart processes before a full GC is required.

Rolling restarts allow shifting traffic away before restart, like upgrades.

Result: These techniques can‚Äôt eliminate pauses but can make their impact much smaller.





######   Knowledge, Truth, and Lies

Distributed systems behave very differently from single-computer programs. There‚Äôs:

No shared memory ‚Äî only message passing across unreliable networks.

Failures, pauses, and clock issues that make reasoning tricky.

A node never knows for sure what‚Äôs true ‚Äî it can only make guesses based on messages it receives (or fails to receive). For example:

If a remote node doesn‚Äôt respond, you can‚Äôt tell if it crashed, paused, or if the network dropped the messages.

Because perception itself is unreliable, ‚Äútruth‚Äù in a distributed system isn‚Äôt absolute.

Instead of philosophy, distributed systems rely on system models (assumptions about behavior) and algorithms proven correct within
those models. This allows reliable behavior despite unreliable foundations.



######   The Truth Is Defined by the Majority



Imagine different failure scenarios:

Asymmetric fault: A node receives messages but all its outgoing messages are dropped.

From its view, everything works.

From others‚Äô views, it‚Äôs dead (no responses).

Others declare it dead even though it‚Äôs alive.

Semi-disconnected awareness: A node notices its outgoing messages aren‚Äôt acknowledged, so it realizes something is wrong.

But still, the majority declares it dead.

Stop-the-world pause: A node freezes for a long GC pause (e.g., 1 min).

During that pause, others stop hearing from it and declare it dead.

When it resumes, it thinks no time passed ‚Äî but it‚Äôs already been replaced as ‚Äúdead.‚Äù

Lesson:

A node‚Äôs self-judgment cannot be trusted.

Relying on a single node leads to risk of permanent failure.

Quorums (majority voting among nodes) are used instead.

A decision requires votes from a majority.

A node may believe it‚Äôs alive, but if the quorum disagrees, it must accept being ‚Äúdead.‚Äù

Majority ensures progress while avoiding conflicting ‚Äútruths‚Äù (there can only be one majority).



######    The Leader and the Lock



Some distributed system rules require only one actor at a time:

Only one leader per partition (to avoid split brain).

Only one client holding a lock to prevent corrupting shared data.

Only one user registered with a specific username.

Problem:
Even if a node thinks it is ‚Äúthe chosen one‚Äù (leader, lock holder, etc.), it might not be. For example:

It was leader before but paused (GC, network).

Other nodes declared it dead and elected a new leader.

When it resumes, it still thinks it‚Äôs leader, even though the quorum disagrees.

This can cause errors. Example:

A client acquires a lock to write a file.

GC pause ‚Üí lock expires ‚Üí another client acquires it ‚Üí starts writing.

Paused client wakes up, still thinks it holds the lock ‚Üí writes as well.

Result: file corruption.

This is not theoretical: HBase had this bug.



######   Fencing Tokens



To prevent false leaders/lock holders from causing harm, we use fencing:

Every time a lock/lease is granted, the lock server issues a fencing token.

The token is a monotonically increasing number (increments with each new grant).

How it works (example):

Client 1 acquires lock ‚Üí gets token 33.

Client 1 pauses.

Client 2 acquires same lock ‚Üí gets token 34.

Client 2 writes to storage, including token 34.

Client 1 resumes ‚Üí sends write with token 33.

Storage server sees it already processed a higher token (34) ‚Üí rejects 33.

Thus, the paused client cannot corrupt data.

In ZooKeeper, zxid (transaction ID) or cversion (node version) can serve as fencing tokens.

But this requires the resource itself (e.g., storage server) to validate tokens and reject outdated ones.

It‚Äôs unsafe to let clients rely only on themselves to check lock validity.

For resources without built-in fencing, workarounds exist:

Example: include fencing tokens in filenames.

But some server-side check is always needed.

Key benefit:

The service is protected even if clients misbehave.

Since clients are run by different people with different priorities, servers should never blindly trust them.

‚úÖ In summary:

In distributed systems, truth is determined collectively (quorums), not by individual nodes.

Leaders and locks must be carefully managed ‚Äî pauses and network issues can create ‚Äúfalse leaders.‚Äù

Fencing tokens are a simple and powerful safeguard against stale nodes corrupting the system.




######   Byzantine Faults


Fencing tokens: Used to detect and block nodes that act incorrectly due to errors (e.g., node‚Äôs lease expired but it still tries to act).

Problem: If a node is malicious, it could forge fencing tokens and bypass this safeguard.

Assumption in most distributed systems:

Nodes may be unreliable (slow, unresponsive, outdated due to GC pauses or delays).

But they are honest ‚Üí they don‚Äôt deliberately lie.

If nodes can lie (send wrong or corrupted responses), things get much harder.

This type of behavior is called a Byzantine fault.

The problem of reaching agreement despite such behavior is called the Byzantine Generals Problem.



######   The Byzantine Generals Problem


Origin: A generalization of the Two Generals Problem.

Two generals want to coordinate a battle.

Communication is only via messengers, who may be delayed or lost (like network packets).

Hard to guarantee agreement.

Byzantine version:

Now there are n generals.

Some generals may be traitors.

Loyal generals send truthful messages.

Traitors send false or confusing messages to mislead others.

Problem: Nobody knows in advance who the traitors are.

Naming:

Byzantium = ancient Greek city (later Constantinople, now Istanbul).

The name reflects ‚ÄúByzantine‚Äù meaning complex, bureaucratic, devious.

Leslie Lamport chose it to avoid offending anyone (he considered ‚ÄúAlbanian Generals Problem‚Äù but was advised against it).

Byzantine Fault Tolerance (BFT)

A system is Byzantine fault-tolerant if it works correctly even when:

Some nodes misbehave, don‚Äôt follow protocol.

Or if malicious attackers interfere with communication.

Relevant Scenarios:

Aerospace systems:

Radiation may corrupt memory/CPU registers.

Node may send arbitrary/unpredictable responses.

Failures here are catastrophic (plane crash, rocket collision).

Flight control systems must tolerate Byzantine faults.

Multi-organization systems:

Some participants may cheat/mislead others.

Can‚Äôt just trust another node‚Äôs messages.

Example: Bitcoin/blockchains ‚Üí consensus among untrusted parties about whether a transaction happened.

Practical Relevance in Datacenter Systems

In datacenters:

All nodes are controlled by one organization.

Radiation not a big problem.

So Byzantine faults are rare.

BFT protocols are complex and expensive.

Embedded aerospace systems use hardware-level support for BFT.

In most server-side systems, BFT cost is impractical.

Web Applications:

Must expect malicious clients (e.g., browsers under user control).

Defense:

Input validation

Sanitization

Output escaping

Prevent SQL injection, XSS, etc.

But here, we don‚Äôt use BFT ‚Üí Instead, server is the authority.

In peer-to-peer networks (no central authority), BFT is much more important.

Bugs and Byzantine Faults

A bug could look like a Byzantine fault.

But if all nodes run the same buggy software, BFT algorithms don‚Äôt help.

BFT typically requires:

Supermajority of 2/3+ correct nodes.

Example: With 4 nodes, at most 1 can be faulty.

To use BFT against bugs ‚Üí need independent implementations (so one bug doesn‚Äôt affect all).

Security and Byzantine Faults

It would be nice if protocols protected against attacks or compromises.

But not realistic:

If one node is compromised, likely all can be (since same software stack).

Defense still relies on:

Authentication

Access control

Encryption

Firewalls

etc.



######  Weak Forms of Lying



Even though full BFT isn‚Äôt practical, systems can guard against weaker forms of lying:

Examples:

Packet corruption:

Happens due to hardware faults, OS bugs, driver bugs, router bugs.

Normally detected by TCP/UDP checksums, but not always.

Defense: application-level checksums.

User input validation:

Public apps must sanitize inputs.

Prevent DOS via oversized payloads.

Internal services (behind firewall) can be less strict, but still should check protocol parsing sanity.

NTP (Network Time Protocol):

Clients configured with multiple servers.

Contact all, estimate errors, find majority agreement.

Detects misconfigured server as an outlier and ignores it.

More robust than relying on a single server.

‚úÖ Summary in one line:
Byzantine faults occur when nodes can lie or misbehave arbitrarily; while crucial in high-stakes systems (aerospace, blockchains),
most datacenter systems avoid the cost of BFT and instead rely on traditional security + lightweight sanity checks for weaker failure modes.




######   System Model and Reality

Distributed algorithms are designed to tolerate the faults we discussed earlier (network unreliability, node crashes, pauses, etc.).
To reason about them, we need to abstract away messy hardware/software details by defining a system model‚Äîa set of 
assumptions about timing and failures.

Timing Assumptions in System Models

There are three main models regarding timing:

1. Synchronous Model

Assumes bounded limits for:

Network delay

Process pauses

Clock error (drift)

This does not mean perfect synchronization or zero delay, just that delays never exceed a fixed upper bound.

Unrealistic in practice because real-world systems can have unbounded delays and pauses.

2. Partially Synchronous Model

Behaves like synchronous most of the time but may occasionally exceed bounds.

Example: usually stable network and processes, but sometimes delays or pauses become arbitrarily large.

Most realistic for practical systems, since things usually work fine but can temporarily break timing assumptions.

3. Asynchronous Model

Makes no timing assumptions at all.

No clocks, no timeouts.

Very restrictive, though some algorithms can be designed for it.

Node Failure Models

Besides timing, we must consider node behavior. Three common models:

1. Crash-Stop Faults

Node may suddenly stop responding forever.

Once down, it never comes back.

2. Crash-Recovery Faults

Node may crash, but can recover later.

Assumes stable storage (disk data survives) but in-memory state is lost.

3. Byzantine (Arbitrary) Faults

Nodes may behave in completely arbitrary or malicious ways (e.g., lying, tricking others).

Most Useful Real-World Model

Partially synchronous model with crash-recovery faults

Matches reality: nodes usually behave, but sometimes fail; they can recover with disk persistence.



######  Correctness of an Algorithm



Like sorting algorithms have correctness properties (e.g., sorted order), distributed algorithms must also satisfy certain properties.

Example: Fencing Tokens

When generating fencing tokens (unique numbers for locks):

Uniqueness: No two requests return the same token.

Monotonic sequence: If request x finished before y started, then tx < ty.

Availability: A non-crashed node that requests a token eventually gets a response.

An algorithm is correct in a system model if it always satisfies such properties under that model‚Äôs assumptions.



######   Safety and Liveness



Two categories of properties:

Safety Properties

Definition: Nothing bad happens.

If violated, we can pinpoint the exact moment (e.g., two clients got the same fencing token).

Once broken, cannot be undone.

Liveness Properties

Definition: Something good eventually happens.

Example: A request may not yet be answered, but eventually a response will come.

Always about the future (‚Äúeventually‚Äù).

Eventual consistency is a liveness property.

Rule of Thumb

Safety must always hold, even if everything crashes.

Liveness is conditional: e.g., guaranteed only if most nodes survive and the network eventually recovers.



######  Mapping System Models to Reality



System models are simplified abstractions, but reality is messier:

Stable storage may fail

Data corruption, disk loss, firmware bugs, misconfigurations.

Example: Quorum algorithms fail if a node ‚Äúforgets‚Äù data it promised to store.

Models assume things never happen (e.g., data loss), but in real systems, they sometimes do.

Implementations must handle these ‚Äúimpossible‚Äù cases anyway‚Äîsometimes just by crashing (exit(666)) and letting humans clean up.

Why Models Still Matter

Abstract models help reduce complexity and make reasoning about correctness possible.

Proving correctness in theory doesn‚Äôt guarantee a perfect real-world implementation.

But theoretical proofs are valuable:

They uncover hidden problems earlier.

They prevent surprises in unusual failure conditions.

Both theory and empirical testing are essential in distributed systems.




## Summary


This chapter explored the challenges and problems in distributed systems, highlighting why they are inherently difficult and
how faults must be tolerated.

Network Issues

When sending a packet:

It may be lost or arbitrarily delayed.

Replies may also be lost or delayed.

If no reply arrives, you cannot tell whether the message was delivered.

Clock Problems

A node‚Äôs clock may be out of sync with others.

Despite synchronization protocols like NTP, clocks may:

Drift significantly.

Jump forward or backward suddenly.

It is dangerous to rely on clocks because the error interval is unknown.

Process Pauses

A process may pause unexpectedly (e.g., due to stop-the-world garbage collection).

During this pause:

Other nodes may declare it dead.

When it resumes, it may not even realize it was paused.

Defining Characteristic: Partial Failures

Distributed systems differ from single-computer systems because of partial failures.

Any action involving another node may:

Fail,

Slow down randomly, or

Not respond at all (leading to a timeout).

Goal: build fault tolerance so the system continues working even when some nodes fail.

Fault Detection is Hard

First step: detect failures.

Problem: most systems cannot accurately detect node failure.

Common method: timeouts.

But timeouts cannot distinguish between:

Network failures (message lost in transit), and

Node failures (machine crashed).

Variable delays cause false suspicions of crashes.

A degraded node (‚Äúlimping‚Äù):

Example: a Gigabit NIC suddenly drops to 1 Kb/s due to a bug.

Harder to handle than a completely failed node.

Tolerating Faults is Hard

After detecting a fault, tolerating it is also difficult:

No global variable or shared memory exists.

No common knowledge or universally agreed state.

Even nodes cannot agree on time.

Only way to exchange information: send over the unreliable network.

Major decisions: cannot be made by a single node ‚Üí require protocols involving multiple nodes and quorums.

Single-Computer vs. Distributed Reality

On a single computer:

Operations are deterministic and predictable.

In distributed systems:

Reality is messy, unpredictable, and failure-prone.

Distributed systems engineers often dismiss problems that could be solved on a single machine as trivial.

If possible, keep everything on one machine to avoid unnecessary complexity.

Why Distributed Systems?

Sometimes, a single machine is not enough.

Goals beyond scalability include:

Fault tolerance (continue running despite failures).

Low latency (place data closer to users).

These goals cannot be achieved on a single node.

Reliability vs. Cost Tradeoff

Is unreliability of networks, clocks, and processes inevitable?

Not exactly. Hard real-time systems can guarantee bounded delays, but:

They are expensive.

They result in lower hardware utilization.

Most non-safety-critical systems choose cheap and unreliable over expensive and reliable.

Supercomputers vs. Distributed Systems

Supercomputers:

Assume components are reliable.

If one fails, the entire system stops and restarts.

Distributed systems:

Can keep running indefinitely.

Faults and maintenance are handled node by node.

‚ö†Ô∏è Exception: if a bad configuration change is rolled out to all nodes, the whole system can fail.

Final Note of the Chapter

This chapter focused mainly on problems in distributed systems, painting a bleak outlook.

The next chapter shifts to solutions, introducing algorithms that help cope with these challenges.



