questions asked on kafka

Here‚Äôs a list of common interview questions asked on Apache Kafka, categorized by difficulty and topic. These cover fundamental concepts, architecture, operations, and use cases.



Basic Kafka Interview Questions  :: =>

What is Apache Kafka?

What are the key components of Kafka?

What is a Kafka topic?

What is a Kafka partition?

What is a Kafka broker?

Explain the concept of Producer and Consumer in Kafka.

What is the difference between Kafka and a traditional message queue?

What is a Kafka consumer group?

How does Kafka achieve message durability?

What is the default retention period of Kafka messages?




Intermediate Kafka Questions  :: =>

How does Kafka guarantee message ordering?

What is the role of ZooKeeper in Kafka?

Explain the process of message consumption in Kafka.

How does Kafka handle message offsets?

What is the difference between at-least-once, at-most-once, and exactly-once semantics?

How can Kafka handle a scenario of message loss or duplication?

What is the purpose of partitions in Kafka?

Explain the replication mechanism in Kafka.

How does Kafka handle fault tolerance?

What is ISR (In-Sync Replica) in Kafka?

How can Kafka be integrated with Spark, Flink, or Hadoop?

What are Kafka Streams?





Advanced Kafka Questions :: =>

Explain Kafka‚Äôs internal storage mechanism.

How does Kafka ensure high throughput and low latency?

What is the difference between log compaction and log retention?

How would you design a Kafka cluster for high availability?

How can Kafka be secured? What authentication and authorization mechanisms does Kafka support?

How do Kafka transactions work?

Explain how Kafka manages backpressure.

How do you monitor Kafka performance and health?

What are Kafka Connect and Kafka MirrorMaker?

How does Kafka handle schema evolution?

Explain Kafka‚Äôs exactly-once semantics and how it is implemented.

What are the different types of Kafka partitioners?

How do you tune Kafka for optimal performance?




Kafka Use-Case and Scenario-Based Questions  :: =>


How would you implement a real-time analytics pipeline using Kafka?

What design considerations are needed for a Kafka-based messaging system?

How do you handle schema compatibility with evolving data?

How would you deal with slow consumers in Kafka?

How would you handle reprocessing messages in Kafka?

Describe a scenario where Kafka is not a good fit.

How would you implement a request-response messaging pattern using Kafka?

How to guarantee message order for a particular key?

Explain strategies to prevent data loss in Kafka.

How do you migrate data from legacy systems into Kafka?

If you want, I can also provide detailed answers for any of these questions or a focused list for a specific Kafka topic (e.g., Kafka architecture, Kafka security, Kafka Streams). Would you like that?



--------------------



tell me ans of first 10 questions


Here are clear and concise answers to the first 10 Apache Kafka interview questions:

Q1. What is Apache Kafka?

Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It allows you to publish, subscribe to, store, and process streams of records in a fault-tolerant and scalable way.

Q2. What are the key components of Kafka?

Producer: Sends data to Kafka topics.

Consumer: Reads data from topics.

Broker: A Kafka server that stores and serves data.

Topic: A logical stream to which records are sent.

Partition: Sub-divisions of a topic for parallelism.

ZooKeeper: Used for managing Kafka's cluster metadata (phased out in newer Kafka versions with KRaft).

Q3. What is a Kafka topic?

A topic is a logical channel or category to which records (messages) are sent by producers and from which consumers read. Topics can have multiple partitions to allow for parallel processing.

Q4. What is a Kafka partition?

A partition is a log that stores an ordered sequence of messages. Each topic is split into partitions, and each message in a partition has a unique offset. Partitions allow Kafka to scale horizontally and support parallel processing.

Q5. What is a Kafka broker?

A broker is a Kafka server that stores data and serves client requests (produce, consume, etc.). A Kafka cluster consists of one or more brokers.

Apache Kafka brokers do store data, and they do so in their own queues, but the term "queue" can be misleading in Kafka's architecture. Let me break it down clearly:
‚úÖ Kafka Broker: What it is
A Kafka broker is a server in a Kafka cluster that:
Accepts and stores messages from producers.
Serves messages to consumers.
Handles topic partitions.
‚úÖ Does a broker store data?
Yes, a Kafka broker stores data locally on disk. Each broker:
Stores the partitions of topics it is responsible for.
Writes messages to log files on the local file system (these are append-only files, not traditional queues).

Q6. Explain the concept of Producer and Consumer in Kafka.

Producer: A client application that sends messages to Kafka topics.

Consumer: A client application that reads messages from Kafka topics. Consumers can belong to consumer groups to divide the workload.

Q7. What is the difference between Kafka and a traditional message queue?

Kafka is distributed, durable, and supports message replay. Unlike traditional queues, Kafka stores messages for a configurable period and allows multiple consumers to read independently. Kafka also emphasizes high throughput and horizontal scalability.

Q8. What is a Kafka consumer group?
A consumer group is a group of consumers working together to consume messages from a topic. Each partition is consumed by only one consumer in a group, ensuring load balancing and parallelism.

Q9. How does Kafka achieve message durability?
Kafka persists messages to disk using a write-ahead log. Data is replicated across brokers to ensure fault tolerance, and messages remain even after being consumed (until they expire based on retention policy).

Q10. What is the default retention period of Kafka messages?
By default, Kafka retains messages for 7 days. This can be changed using the log.retention.hours configuration. You can also retain messages based on size or indefinitely (if needed).

Let me know if you want detailed answers with examples, diagrams, or follow-up questions on these topics.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



Q11. How does Kafka guarantee message ordering?

Kafka guarantees message ordering within a partition. That means all messages sent to a specific partition are written and read in the same order. However, Kafka does not guarantee order across multiple partitions of the same topic.

üëâ Tip: If you need strict ordering for related events, use a message key to ensure all related messages go to the same partition.

Q12. What is the role of ZooKeeper in Kafka?

ZooKeeper (used in older Kafka versions) performs the following roles:

Maintains metadata about brokers (e.g., which brokers are alive).

Handles leader election for partitions.

Manages cluster configuration and topic-related changes.

üìå Note: As of Kafka 2.8, a new mode called KRaft (Kafka Raft) removes the dependency on ZooKeeper for simpler operations and improved scalability.

Q13. Explain the process of message consumption in Kafka.

Here‚Äôs a simplified message consumption flow:

A consumer subscribes to one or more topics.

Kafka assigns partitions to consumers within a consumer group.

The consumer polls Kafka to fetch data.

Kafka returns messages starting from the last committed offset.

After processing, the consumer commits the new offset (automatically or manually).

Kafka provides APIs to control offset commits and reprocessing behavior.

Q14. How does Kafka handle message offsets?

Kafka uses offsets to uniquely identify each message within a partition. Consumers track their progress using these offsets. There are two offset handling modes:

Automatic commit: Kafka commits offsets at intervals (default every 5 seconds).

Manual commit: Applications explicitly commit offsets after processing, providing more control (important for reliability).

Offsets can be stored in Kafka (default) or externally (e.g., in a database).

Q15. What is the difference between at-least-once, at-most-once, and exactly-once semantics?

At-least-once (default): Messages may be delivered more than once if a failure occurs. Ensures no data loss.

At-most-once: Messages are delivered at most one time. There is no retry on failure ‚Äî data loss may occur.

Exactly-once: Each message is delivered and processed once with no duplication. Kafka supports this through idempotent producers and transactional writes.

üß† Kafka‚Äôs exactly-once semantics require configuring both producers and consumers carefully (e.g., using Kafka Streams or the transactions API).

Let me know if you‚Äôd like practical examples or diagrams for any of these concepts.



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


Q16. How can Kafka handle a scenario of message loss or duplication?

Message Loss Handling:

Replication: Kafka replicates messages across multiple brokers (replicas) to avoid loss.

Acknowledgment (acks): Producers can specify how many brokers must acknowledge a message before it is considered ‚Äúwritten‚Äù.

acks=0: No guarantee.

acks=1: Leader must ack.

acks=all: All ISR replicas must ack (safest).

Durability: Kafka writes messages to disk before acknowledging.

Message Duplication Handling:

Idempotent Producer: Ensures exactly-once semantics by assigning sequence numbers to messages (enable.idempotence=true).

Consumer Offsets: If a consumer crashes after processing but before committing an offset, it may reprocess (duplicate).

Solution: Use transactional API or external idempotent stores.
( transactional API  ->  all steps of API should be atomic to ensure -> @Transactional )
(idempotent stores -> check the composite key exits in DB or not then insert)

Q17. What is the purpose of partitions in Kafka?

Partitions serve two primary purposes:

Parallelism & Scalability: Partitions allow Kafka to scale horizontally by distributing data across multiple brokers and enabling parallel consumption.

Ordering Guarantee: Kafka guarantees order within a single partition, but not across partitions.

Each topic can have multiple partitions, and each partition is an ordered, immutable sequence of records.

Q18. Explain the replication mechanism in Kafka.

Kafka ensures fault tolerance via replication:

Each partition has:

Leader: Handles all reads/writes.

Followers: Replicate data from the leader.

Key concepts:

Replication Factor: Number of copies of each partition.

In-Sync Replicas (ISR): Set of replicas that are fully caught up with the leader.

If the leader fails:

Kafka elects a new leader from ISR using ZooKeeper or the newer KRaft mode.

Q19. How does Kafka handle fault tolerance?

Kafka‚Äôs fault tolerance is achieved through:

Replication: Each partition is replicated across multiple brokers.

Leader Election: If a broker hosting the leader partition fails, Kafka elects a new leader from ISR.

Acknowledgment Policies: Producers can wait for multiple acknowledgments (acks=all) before considering a message committed.

ZooKeeper or KRaft: Tracks cluster metadata and helps with leader elections (moving towards KRaft mode without ZooKeeper).

Consumer Groups: Allow other consumers in the group to take over if one fails.



Q20. What is ISR (In-Sync Replica) in Kafka?

ISR (In-Sync Replica) is the set of replicas that:

Are fully caught up with the leader (i.e., have the latest messages).

Are eligible to become leader in case the current leader fails.

Example:

Topic T1 with partition P1 has replication factor 3: brokers 1 (leader), 2, and 3.

If brokers 1 and 2 are in sync, ISR = {1, 2}.

If broker 3 lags too much or goes down, it is removed from ISR until it catches up.

Kafka only acknowledges messages to producers once all ISR members have received the data (if acks=all).



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


Q21. How can Kafka be integrated with Spark, Flink, or Hadoop?

Kafka can be integrated into big data ecosystems for real-time and batch processing:

Apache Spark (Structured Streaming):

Read from Kafka using spark.readStream.format("kafka").

Supports checkpointing, exactly-once processing, and writing back to Kafka.

Apache Flink:

Native Kafka connectors for consuming/producing with exactly-once semantics.

Flink manages state and offset checkpoints for fault tolerance.

Hadoop:

Use Kafka Connect with the HDFS Sink Connector to push data from Kafka to HDFS.

Or use batch ingestion tools like Gobblin or Camus.

Q22. What are Kafka Streams?

Kafka Streams is a Java library for building real-time applications and microservices that:

Process data directly from Kafka topics.

Transform, aggregate, filter, and join data.

Output results to new Kafka topics.

Key features:

Lightweight and embedded in your app (no cluster required).

Supports stateful processing using local RocksDB-backed state stores.

Enables exactly-once processing, windowing, joins, and KTable (changelog stream).

Example use case:

StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> input = builder.stream("input-topic");
KStream<String, String> upper = input.mapValues(String::toUpperCase);
upper.to("output-topic");


Q23. Explain Kafka‚Äôs internal storage mechanism.

Kafka uses a highly optimized append-only log storage model:

Key Concepts:
Partition as Log: Each topic partition is a log file where new records are always appended.

Segmented Logs: A partition's log is split into segments (e.g., 1 GB files).

Each segment is a separate file on disk (e.g., 000000000000.log).

Index Files: Kafka maintains:

Offset index (.index) ‚Üí maps message offsets to file positions.

Time index (.timeindex) ‚Üí maps timestamps to offsets.

All the 3 above techniques are used.

How Storage Works:

New messages are written to the active segment (one only).

Old segments are eligible for deletion or compaction based on retention policy.

Durability:
Kafka uses write-ahead logs.

Messages are flushed to disk periodically or after a configurable number of messages.

OS page cache is used for performance; fsync ensures durability on flush.



Q24. How does Kafka ensure high throughput and low latency?
Kafka is built for high throughput and low latency using several design principles:

1. Sequential I/O (Append-only Log):

Kafka writes data sequentially, avoiding disk seeks.
Uses zero-copy transfer (sendfile) to transfer data from file to network socket efficiently.
(Every layer has its own buffer from disk to  OS buffer to memory buffer to network buffer.
Zero copy means kafka will skip the process and directly transfer from disk to network.
)

2. Batching:
Both producers and brokers batch records to reduce overhead.

Batching reduces the number of network and disk operations.

3. OS Page Cache:
Kafka relies on the Linux page cache to cache file system operations, avoiding costly disk reads.

4. Compression:
Kafka supports message compression (gzip, snappy, lz4, zstd), reducing bandwidth and disk usage.

5. Efficient Networking:
Kafka uses asynchronous, non-blocking I/O (NIO) for high concurrency.
Non-blocking I/O (NIO) allows a program to perform other tasks while waiting for I/O operations to complete,
rather than blocking the thread until the I/O is finished. This is particularly useful in scenarios 
like network programming where handling multiple connections concurrently is essential. NIO uses techniques 
like I/O multiplexing and event-driven programming to achieve this. 

6. Partitioning:
Data is partitioned across brokers, enabling parallelism in reads and writes.

7. Log Segmentation:
Log files are split into segments, allowing efficient deletion and index maintenance.


Q25. What is the difference between log compaction and log retention?

Both are Kafka strategies for data management and cleanup, but serve different purposes.

Log Retention:
Time or size-based deletion of messages.

Once the data exceeds the configured time or size threshold, Kafka deletes it.

Config:

retention.ms=604800000      # Keep logs for 7 days
retention.bytes=1073741824  # Or max 1GB per partition


Use Case: Event streams where old data isn't needed, e.g., logs, metrics.

Log Compaction:
Kafka retains the latest message per key by removing older ones with the same key.

Keeps the latest state for each key indefinitely.

Config:

cleanup.policy=compact


| Feature          | Log Retention                      | Log Compaction                         |
| ---------------- | ---------------------------------- | -------------------------------------- |
| Basis            | Time/Size                          | Message Key                            |
| Purpose          | Delete old messages                | Retain the latest update per key       |
| Data Loss        | Deletes all messages beyond limits | Retains most recent message per key    |
| Example Use Case | Logs, metrics, telemetry           | User profile state, configuration data |


----------------------------------------------------------------------------------------------------------------------------------------



Q26. How would you design a Kafka cluster for high availability?
To design a highly available Kafka cluster, you must eliminate single points of failure and ensure data durability and service continuity.

‚úÖ Key Strategies:
Replication Factor ‚â• 3:

Each topic's partitions should be replicated to at least 3 brokers.

Ensures that even if 1 or 2 brokers fail, data is still safe.

Multiple Brokers:

Spread partitions and replicas across multiple brokers (min 3‚Äì5 for HA).

Leader Election from ISR:

Kafka automatically elects a new leader from In-Sync Replicas (ISR) if the current leader fails.

ack=all & min.insync.replicas=2:

Producers wait until all ISR replicas ack messages ‚Üí ensures durability.

Prevents data loss if one broker goes down.

Multiple ZooKeeper Nodes / KRaft Controllers:

Use a ZooKeeper ensemble (odd number ‚â• 3) or KRaft quorum for metadata durability.

Rack Awareness:

Configure broker.rack to place replicas across different racks/availability zones.

Enables fault tolerance against rack failure.

Monitoring & Alerts:

Use Kafka monitoring tools (Prometheus, Grafana, Confluent Control Center) to detect failures early.

Backup & Disaster Recovery:

MirrorMaker 2 or custom solutions for cross-cluster replication.

Q27. How can Kafka be secured? What authentication and authorization mechanisms does Kafka support?
Kafka supports several security features to ensure authentication, authorization, and encryption.

üîí Security Mechanisms:

Encryption (TLS/SSL):

Use TLS for encrypting data in-transit (between brokers, producers, consumers).

Configure ssl.keystore, ssl.truststore.

Authentication:

Kafka supports:

SSL authentication (client certificates)

SASL mechanisms:

SASL/PLAIN (username/password)

SASL/SCRAM (encrypted password storage)

SASL/GSSAPI (Kerberos)

Authorization (ACLs):

Kafka‚Äôs built-in ACL system (ZK or KRaft-based) can restrict:

Topic access

Group access

Producer/consumer permissions

Example:
kafka-acls.sh --add --allow-principal User:alice --operation Read --topic my-topic


Audit Logging:

Kafka can log security events and access patterns for auditing purposes.

KRaft Mode:

Newer deployments can use KRaft (Kafka without ZooKeeper), with built-in security management.

Q28. How do Kafka transactions work?

Kafka supports exactly-once semantics (EOS) using transactions.

‚úÖ Use Case:
To atomically write to multiple partitions or topics, or read-process-write reliably without duplication.

üîÑ Steps in Kafka Transactions:
1.Enable idempotence:

Producer must set: enable.idempotence=true (auto-enabled with transactions)

2.Init transactional producer:

Properties props = new Properties();
props.put("transactional.id", "my-tx-id");

3.Begin, send, and commit:
producer.initTransactions();
producer.beginTransaction();
producer.send(...); // multiple topic-partition writes
producer.commitTransaction(); // or abortTransaction()

4.Read-process-write (with EOS):

Use read_committed isolation level to consume only committed messages.

‚ö† Notes:
Transactional ID must be unique per producer instance.

Requires internal topic __transaction_state.


Q29. Explain how Kafka manages backpressure.
Kafka doesn‚Äôt have a strict backpressure mechanism but handles it using indirect controls at different stages:

üîÉ Producer Side:
Buffer size: If producer buffer is full (buffer.memory), it blocks or throws an exception based on max.block.ms.

linger.ms: Can delay sending small batches to accumulate more records, smoothing spikes.

üßØ Broker Side:
Kafka brokers queue incoming messages in memory.

If overloaded, brokers can throttle producers using quota configuration.

üîÅ Consumer Side:
Consumers control backpressure by controlling offset commits.

If a consumer is too slow, lag increases, which can be monitored and alerted.

üîÑ Flow Control Tools:
Use concurrency controls (e.g., thread pools) in consumers.

Apply rate limiting or batch processing to control processing throughput.

Q30. How do you monitor Kafka performance and health?

Monitoring Kafka is critical for performance tuning and fault detection.

üîç Key Metrics to Monitor:
Broker Metrics:

Under-replicated partitions

Offline partitions

Request latency

ISR shrink/expand events

Producer Metrics:

Record send rate / failure rate

Batch size and compression rate

Retries / errors

Consumer Metrics:

Consumer lag (difference between committed and latest offset)

Rebalance frequency

Throttled fetch rate

Cluster Health:

Disk usage

JVM heap usage

Controller health

ZooKeeper or KRaft status

üìä Tools for Monitoring:
Prometheus + Grafana: Open-source combo to scrape and visualize metrics.

Confluent Control Center: Enterprise tool for deep Kafka insights.

Datadog / New Relic / Splunk: Commercial options with Kafka plugins.

JMX Exporter: Kafka exposes metrics over JMX, which Prometheus can scrape.



----------------------------------------------------------------------------------------------------------------------------------------



Q31. How do consumers handle message ordering in Kafka?

‚úÖ Kafka guarantees message order within a partition, but not across partitions.
üî∏ How Consumers Maintain Ordering:
A single consumer thread per partition ensures strict ordering.

If multiple partitions are assigned to a consumer, messages from different partitions may be processed out of global order.

Use key-based partitioning to ensure that all events with the same key go to the same partition, preserving order for that key.

üß† Example:
ProducerRecord<String, String> record = new ProducerRecord<>("my-topic", "user123", "event1");

All events with key "user123" will go to the same partition, ensuring ordered processing.

Q32. What is a dead-letter queue (DLQ) and how is it used in Kafka?

A Dead-Letter Queue (DLQ) is a Kafka topic used to store messages that cannot be processed successfully by consumer after multiple attempts.

üîç Use Cases:

Malformed messages (bad schema, missing fields)

Business logic failures (e.g., division by zero, missing user in DB)

Deserialization errors

üîÅ DLQ Implementation Approaches:

Manual Handling in Consumer:
try {
    processMessage(record);
} catch (Exception e) {
    producer.send(new ProducerRecord<>("dead-letter-topic", record.key(), record.value()));
}

Kafka Connect DLQ Support:

Kafka Connect supports built-in DLQ configuration:
errors.tolerance=all
errors.deadletterqueue.topic.name=dlq-topic


Schema Registry Integration:

If using Confluent Schema Registry, DLQ can capture schema evolution/deserialization issues.

Schema Registry provides a centralized repository for managing and validating schemas for topic message data, and
for serialization and deserialization of the data over the network. 
Producers and consumers to Kafka topics can use schemas to ensure data consistency and compatibility as schemas evolve.


Q33. How do Kafka Connect and Kafka Streams differ?

Kafka Connect API

| Feature         | **Kafka Connect**                                       | **Kafka Streams**                                 |
| --------------- | ------------------------------------------------------- | ------------------------------------------------- |
| Purpose         | Data integration between Kafka and external systems     | Real-time stream processing on Kafka data         |
| Use Case        | Source/sink connectors (e.g., DB ‚Üí Kafka, Kafka ‚Üí HDFS) | Transform, aggregate, join, filter Kafka topics   |
| Model           | Configuration-driven (no code)                          | Java library (embedded in apps)                   |
| Fault Tolerance | Built-in with distributed mode                          | Requires handling via checkpoints or state stores |
| Scalability     | Scales using worker nodes (standalone/distributed mode) | Scales via app instances + Kafka partitions       |
| Example         | Kafka ‚Üí MySQL using JDBC Sink Connector                 | Join 2 topics to compute enriched event stream    |


Q34. What are Kafka‚Äôs delivery semantics?

Kafka supports three delivery semantics:

‚úÖ 1. At Most Once
Messages may be lost, but never duplicated.

Producer sends without waiting for acknowledgment.

Fast but risky.

‚úÖ 2. At Least Once
Messages are retried until acknowledged ‚Üí no loss, but duplicates possible.

Common default for Kafka.

‚úÖ 3. Exactly Once (EOS)
No duplicates, no message loss.

Requires:

enable.idempotence=true on producer

Transactional producer API

Consumers using read_committed

Highest reliability, but with more overhead.

Q35. What is idempotence in Kafka?

Idempotence means processing the same message multiple times results in the same outcome ‚Äî crucial for avoiding duplicates.

üõ° In Kafka:
Producers can be made idempotent using:

enable.idempotence=true

Ensures that resending the same message (e.g., due to retry) does not create duplicates.

üîß How It Works:

Kafka assigns a producer ID and sequence number for each message.

Broker uses these to discard duplicate writes.

‚ö† Important:
Idempotence works within a single producer session.

It is required for exactly-once delivery (acks=all, retries allowed).

At the consumer end , composite key of the data is used to check whether this data exits in database or not. then insert or not.



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



scenario based questions


36. What is the role of ZooKeeper in Kafka? Is it still required?
‚úÖ In older Kafka versions (pre-2.8):
ZooKeeper was essential for Kafka cluster management.

üîπ ZooKeeper‚Äôs Responsibilities:
Leader election (e.g., controller broker)

Maintaining metadata (topic configs, broker list, ISR)

Cluster state and configuration management

Node registration and health checks

üö´ In Kafka 2.8+ and 3.x, ZooKeeper is being phased out:
üîÑ Replacement: KRaft (Kafka Raft Metadata Mode)
Kafka now includes its own metadata quorum using Raft consensus protocol.

Benefits:

No external dependency (no ZooKeeper)

Simplified deployment and management

Better scalability and resilience

37. How do you perform schema evolution in Kafka?
Schema evolution allows changing the structure of Kafka messages over time while maintaining compatibility.

‚úÖ Tools: Confluent Schema Registry
üîÑ Supported Compatibility Modes:
BACKWARD: New schema can read old data.

FORWARD: Old schema can read new data.

FULL: Both forward and backward compatible.

NONE: No compatibility checks.

üîß Best Practices:
Use Avro, Protobuf, or JSON Schema with the Schema Registry.

Avoid removing fields or changing field types.

Use default values for new fields.

üîç Example:
Old schema:
{"type": "record", "name": "User", "fields": [{"name": "name", "type": "string"}]}

New schema (backward compatible):
{"type": "record", "name": "User", "fields": [
  {"name": "name", "type": "string"},
  {"name": "email", "type": "string", "default": ""}
]}

38. How does Kafka handle data replication and consistency?
Kafka ensures high availability and durability using partition replication.

üîÅ Replication Mechanism:
Each partition has a leader and followers (replicas).

Leader handles all reads/writes, followers replicate data from it.

‚úÖ Consistency Rules:
A message is considered committed when:

It is written to all in-sync replicas (ISR)

Producer uses acks=all

üîß Key Settings:
replication.factor: Number of replicas per partition.

min.insync.replicas: Minimum replicas that must acknowledge for successful write.

üß† Behavior:
If a follower lags, it is removed from ISR.

Leader election only happens from ISR (to maintain consistency).

If leader fails, a follower from ISR is elected.

39. Explain how Kafka handles consumer group rebalancing.
üîÅ Rebalancing = reassigning partitions to consumers in a group.
üß© When does it happen?
A new consumer joins or leaves the group.

A topic is added or partition count changes.

A consumer fails (heartbeat timeout).

üß† Rebalance Process:
Group Coordinator is selected among brokers.

Consumers send heartbeat and join group requests.

One consumer becomes group leader.

Group leader uses a PartitionAssignor (e.g., range, round-robin, sticky) to assign partitions.

All consumers receive their new partition assignments.

üß± PartitionAssignor Types:
RangeAssignor: Splits partitions sequentially.

RoundRobinAssignor: Distributes evenly.

StickyAssignor: Tries to minimize movement across rebalances.

‚ö† Impacts:
During rebalance, consumers pause processing ‚Üí temporary unavailability.

Use cooperative rebalancing (Kafka 2.4+) to reduce downtime.

40. What are some best practices for using Kafka in production?
‚úÖ Key Kafka Production Best Practices:
Design Topics Properly:

Use logical partitioning by key (e.g., user ID, order ID)

Avoid excessive partitions (balance performance and overhead)

Tune Producer Settings:

acks=all, enable.idempotence=true for durability

Use compression (snappy or lz4) for better throughput

Manage Consumer Lag:

Monitor lag metrics

Use auto offset commit cautiously

Prefer manual commit after successful processing

Secure the Cluster:

Use SSL, SASL, ACLs

Encrypt data in transit and at rest

Monitor and Alert:

Track:

Under-replicated partitions

Consumer group lags

Broker disk usage and GC

Request latency

Use Schema Registry:

Enforce schema compatibility

Avoid breaking changes

Set Retention Policies:

Define appropriate retention.ms and cleanup.policy

Use compaction for state-changes and retention for event logs

Backup and Disaster Recovery:

Use MirrorMaker 2 for cross-region/cluster replication

Snapshot critical data if needed

Use Dedicated Storage:

Use fast disks (SSD/NVMe)

Separate Kafka logs from OS disks

Enable Quotas:

Protect brokers from overload using producer/consumer quotas



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------




‚úÖ 1. What are Kafka Connect and Kafka MirrorMaker?
Kafka Connect:
A tool for integrating Kafka with external systems (DBs, filesystems, cloud services).

Supports source connectors (to Kafka) and sink connectors (from Kafka).

Handles scalability, fault-tolerance, and offset management.

Example: MySQL ‚Üí Kafka (source), Kafka ‚Üí Elasticsearch (sink)

Kafka MirrorMaker:
A tool for replicating Kafka topics across clusters.

Use cases:

Cross-data center replication

Migration between Kafka clusters

MirrorMaker 2.0 (part of Kafka Connect) supports active-active replication, offset sync, and topic renaming.

‚úÖ 2. How does Kafka handle schema evolution?
Kafka uses Confluent Schema Registry to enforce schema validation and compatibility.

Supports formats like Avro, Protobuf, JSON Schema.

Compatibility Modes:
BACKWARD: New readers can read old data.

FORWARD: Old readers can read new data.

FULL: Both forward and backward.

NONE: No checks.

Best Practices:
Add fields with default values

Avoid deleting or changing field types

‚úÖ 3. Explain Kafka‚Äôs exactly-once semantics and how it is implemented.
Kafka supports Exactly-Once Semantics (EOS) using:

Key Components:
Idempotent Producer (enable.idempotence=true)

Prevents duplicate writes due to retries.

Transactional Producer

Groups multiple writes into an atomic unit.

Uses initTransactions(), beginTransaction(), send(), commitTransaction().

Kafka Consumer Isolation

isolation.level=read_committed ensures consumers read only committed records.

Use Case:
End-to-end exactly-once between producer ‚Üí Kafka ‚Üí consumer ‚Üí DB.

‚úÖ 4. What are the different types of Kafka partitioners?
Default Partitioner:
Uses hashing on the message key to determine partition.

Custom Partitioner:
Implement org.apache.kafka.clients.producer.Partitioner interface.

Round-Robin Partitioner:
Distributes messages evenly when no key is specified.

Sticky Partitioner (Kafka 2.4+):
Sticks to one partition for a batch, reducing overhead and improving batching efficiency.

‚úÖ 5. How do you tune Kafka for optimal performance?
Broker Configs:
num.network.threads, num.io.threads: Tune based on core count.

log.segment.bytes, log.retention.hours: Tune for storage size and durability.

message.max.bytes: Match with producer max.request.size.

Producer Tuning:
batch.size, linger.ms: Increase batching.

compression.type: Use snappy or lz4.

acks=all, retries, enable.idempotence=true

Consumer Tuning:
fetch.min.bytes, fetch.max.wait.ms

Increase concurrency (consumer threads or consumer groups)

Hardware:
Use SSDs, high IOPS disks

Separate disk for logs

High network throughput (10Gbps+)

‚úÖ 6. How would you implement a real-time analytics pipeline using Kafka?
Architecture:
Producers: Collect real-time events (web logs, transactions).

Kafka Topics: Event streams (e.g., clicks, purchases).

Stream Processing:

Kafka Streams or Flink/Spark for processing.

Transformations, aggregations, joins.

Sink:

Output to data warehouse (e.g., BigQuery, Redshift) or dashboards (e.g., Elasticsearch).

Tools:
Kafka Connect for ingestion/export

Kafka Streams/Flink for logic

Schema Registry for data contracts

‚úÖ 7. What design considerations are needed for a Kafka-based messaging system?
Topic Design:

Partition count = parallelism

Avoid hot partitions

Message Size:

Keep messages small (<1MB ideally)

Retention Policy:

Based on use case: time-based or compacted

Throughput vs Latency:

Tune producer/consumer for batch size and flush frequency

Schema Management:

Use Schema Registry to enforce contracts

Consumer Group Strategy:

Ensure each partition has only one active consumer

Monitoring:

Lag, ISR, disk usage, GC, throughput

‚úÖ 8. How do you handle schema compatibility with evolving data?
Register all schemas in Schema Registry

Set compatibility mode (BACKWARD, FORWARD, FULL)

Add new fields with default values

Avoid:

Changing existing field types

Removing fields

Use tools like avro-tools to validate changes before deployment

‚úÖ 9. How would you deal with slow consumers in Kafka?
Increase partitions: More consumers can be added for parallelism

Pause/resume specific partitions using consumer.pause()

Use a buffer or queue between Kafka and downstream system

Adjust consumer settings:

max.poll.records, fetch.max.bytes

Monitor consumer lag

Use alerting if lag increases

Avoid long processing per message: Offload heavy processing elsewhere

‚úÖ 10. How would you handle reprocessing messages in Kafka?
Approaches:
Reset Offsets:

Use kafka-consumer-groups.sh --reset-offsets

Reset to earliest/latest/specific timestamp

Use a new consumer group:

Kafka retains messages based on retention, not consumption

Reprocessing with a new group ID gets all data again

Dead-letter queues (DLQs):

For retrying failed messages

Idempotent consumers:

Ensure processing doesn‚Äôt duplicate side effects

‚úÖ 11. Describe a scenario where Kafka is not a good fit.
Kafka is NOT ideal when:
Low-latency message delivery is critical (e.g., sub-ms latency)

Small-scale systems with few messages

Need strong ordering across all messages (Kafka guarantees order only within a partition)

Transactional semantics across systems are required (Kafka‚Äôs EOS doesn't cover cross-system)

Very high fan-out without consumer scalability (each consumer group scales per topic/partition)( Real-Time Sports Score Updates)

‚úÖ 12. How would you implement a request-response messaging pattern using Kafka?
Kafka is not inherently request-response, but it can be emulated:

Strategy:
Producer sends a request to a Kafka topic with a correlation ID and a reply topic.

Consumer processes the request, writes response to the reply topic with same correlation ID.

Producer listens on the reply topic, filters by correlation ID.

Considerations:
Use unique reply topics per client or share with filtering

Implement timeouts on the client side

Libraries like spring-kafka offer templates for this

‚úÖ 13. How to guarantee message order for a particular key?
Key-based Partitioning: All messages with the same key go to the same partition.

Kafka guarantees in-order delivery per partition.

Ensure:

key != null

Single producer thread per key (or use key-based routing)

Consumer reads from a single partition

‚úÖ 14. Explain strategies to prevent data loss in Kafka.
Producer Side:
acks=all ensures write to all in-sync replicas

retries, max.in.flight.requests.per.connection=1 to avoid duplication

enable.idempotence=true

Broker Side:
Use replication factor ‚â• 3

min.insync.replicas > 1

Monitor ISR and disk space

Use reliable storage (RAID, SSDs)

Consumer Side:
Use enable.auto.commit=false, manually commit after successful processing

Design idempotent consumers

Monitor lag and failures

‚úÖ 15. How do you migrate data from legacy systems into Kafka?
Strategies:
Kafka Connect + JDBC Source Connector:

Pull data from legacy DB into Kafka

Custom Producers:

Create scripts or services that read legacy data and publish to Kafka

Batch Migration + Streaming:

First migrate historical data in bulk

Then stream new updates using CDC tools (like Debezium)

Schema Mapping:

Convert legacy data formats to Avro/JSON/Protobuf

Validation:

Ensure data consistency

Monitor lag and failures during migration



-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


1. What does it mean if a replica is not an In-Sync Replica for a long time?
A replica that has been out of ISR for a long period of time indicates that the follower is unable to fetch data at the same rate
as the leader.

2. What are the traditional methods of message transfer? How is Kafka better from them?
Following are the traditional methods of message transfer:-

Message Queuing:- 
A point-to-point technique is used in the message queuing pattern. A message in the queue will be destroyed once it has been consumed,
similar to how a message is removed from the server once it has been delivered in the Post Office Protocol.
Asynchronous messaging is possible with these queues.
If a network problem delays a message's delivery, such as if a consumer is unavailable, the message will be held in the queue
until it can be sent. This means that messages aren't always sent in the same order. Instead, they are given on a first-come,
first-served basis, which can improve efficiency in some situations.


Publisher - Subscriber Model:- 
The publish-subscribe pattern entails publishers producing ("publishing") messages in multiple categories and subscribers 
consuming published messages from the various categories to which they are subscribed. Unlike point-to-point texting, a
message is only removed once it has been consumed by all category subscribers.
Kafka caters to a single consumer abstraction that encompasses both of the aforementioned- the consumer group.
Following are the benefits of using Kafka over the traditional messaging transfer techniques:
Scalable: A cluster of devices is used to partition and streamline the data thereby, scaling up the storage capacity.
Faster: Thousands of clients can be served by a single Kafka broker as it can manage megabytes of reads and writes per second.
Durability and Fault-Tolerant: The data is kept persistent and tolerant to any hardware failures by copying the data in the clusters.


3. What are the major components of Kafka?
Following are the major components of Kafka:-

Topic:
A Topic is a category or feed in which records are saved and published.
Topics are used to organize all of Kafka's records. Consumer apps read data from topics, whereas producer applications
write data to them. Records published to the cluster remain in the cluster for the duration of a configurable retention period.
Kafka keeps records in the log, and it's up to the consumers to keep track of where they are in the log (the "offset"). 
As messages are read, a consumer typically advances the offset in a linear fashion. The consumer, on the other hand,
is in charge of the position, as he or she can consume messages in any order. When reprocessing records, for example, 
a consumer can reset to an older offset.
Producer:
A Kafka producer is a data source for one or more Kafka topics that optimizes, writes, and publishes messages.
Partitioning allows Kafka producers to serialize, compress, and load balance data among brokers.
Consumer:
Data is read by consumers by reading messages from topics to which they have subscribed. Consumers will be divided into groups.
Each consumer in a consumer group will be responsible for reading a subset of the partitions of each subject to which they have 
subscribed.
Broker:
A Kafka broker is a server that works as part of a Kafka cluster (in other words, a Kafka cluster is made up of a number of brokers).
Multiple brokers typically work together to build a Kafka cluster, which provides load balancing, reliable redundancy, and failover.
The cluster is managed and coordinated by brokers using Apache ZooKeeper. Without sacrificing performance, each broker instance
can handle read and write volumes of hundreds of thousands per second (and gigabytes of messages). Each broker has its own ID and 
can be in charge of one or more topic log divisions.
ZooKeeper is also used by Kafka brokers for leader elections, in which a broker is chosen to lead the handling of client requests
for a certain partition of a topic. Connecting to any broker will bring a client up to speed with the entire Kafka cluster.
A minimum of three brokers should be used to achieve reliable failover; the higher the number of brokers,
the more reliable the failover.

4. Explain the four core API architecture that Kafka uses.
Following are the four core APIs that Kafka uses:

Producer API:
The Producer API in Kafka allows an application to publish a stream of records to one or more Kafka topics.
Consumer API:
An application can subscribe to one or more Kafka topics using the Kafka Consumer API. It also enables the application to
process streams of records generated in relation to such topics.
Streams API:
The Kafka Streams API allows an application to use a stream processing architecture to process data in Kafka.
An application can use this API to take input streams from one or more topics, process them using streams operations, and 
generate output streams to transmit to one or more topics. The Streams API allows you to convert input streams into output streams
in this manner.
Connect API:
The Kafka Connector API connects Kafka topics to applications. This opens up possibilities for constructing and managing 
the operations of producers and consumers, as well as establishing reusable links between these solutions. A connector,
for example, may capture all database updates and ensure that they are made available in a Kafka topic.



5. What do you mean by a Partition in Kafka?
Kafka topics are separated into partitions, each of which contains records in a fixed order. A unique offset is assigned and
attributed to each record in a partition. Multiple partition logs can be found in a single topic.
This allows several users to read from the same topic at the same time. Topics can be parallelized via partitions,
which split data into a single topic among numerous brokers.

Replication in Kafka is done at the partition level. A replica is the redundant element of a topic partition.
Each partition often contains one or more replicas, which means that partitions contain messages that are duplicated across many
Kafka brokers in the cluster.

One server serves as the leader of each partition (replica), while the others function as followers.
The leader replica is in charge of all read-write requests for the partition, while the followers replicate the leader.
If the lead server goes down, one of the followers takes over as the leader. To disperse the burden, we should aim for a good balance
of leaders, with each broker leading an equal number of partitions.


6. What do you mean by zookeeper in Kafka and what are its uses?
Apache ZooKeeper is a naming registry for distributed applications as well as a distributed, open-source configuration and
synchronization service. It keeps track of the Kafka cluster nodes' status, as well as Kafka topics, partitions, and so on.

ZooKeeper is used by Kafka brokers to maintain and coordinate the Kafka cluster. When the topology of the Kafka cluster changes,
such as when brokers and topics are added or removed, ZooKeeper notifies all nodes. When a new broker enters the cluster,
for example, ZooKeeper notifies the cluster, as well as when a broker fails. ZooKeeper also allows brokers and topic partition pairs
to elect leaders, allowing them to select which broker will be the leader for a given partition (and server read and write operations
from producers and consumers), as well as which brokers contain clones of the same data. 
When the cluster of brokers receives a notification from ZooKeeper, they immediately begin to coordinate with one another and
elect any new partition leaders that are required. This safeguards against the unexpected absence of a broker.

7. Can we use Kafka without Zookeeper?
Kafka can now be used without ZooKeeper as of version 2.8. The release of Kafka 2.8.0 in April 2021 gave us all the opportunity to
try it out without ZooKeeper. However, this version is not yet ready for production and lacks some key features.
In the previous versions, bypassing Zookeeper and connecting directly to the Kafka broker was not possible.
This is because when the Zookeeper is down, it is unable to fulfill client requests.


8. Explain the concept of Leader and Follower in Kafka.
In Kafka, each partition has one server that acts as a Leader and one or more servers that operate as Followers.
The Leader is in charge of all read and writes requests for the partition, while the Followers are responsible 
for passively replicating the leader. In the case that the Leader fails, one of the Followers will assume leadership.
The server's load is balanced as a result of this.

9. Why is Topic Replication important in Kafka? What do you mean by ISR in Kafka?
Topic replication is critical for constructing Kafka deployments that are both durable and highly available. When one broker fails,
topic replicas on other brokers remain available to ensure that data is not lost and that the Kafka deployment is not disrupted.
The replication factor specifies the number of copies of a topic that are kept across the Kafka cluster.
It takes place at the partition level and is defined at the subject level. A replication factor of two, for example, 
will keep two copies of a topic for each partition.

Each partition has an elected leader, and other brokers store a copy that can be used if necessary. Logically,
the replication factor cannot be more than the cluster's total number of brokers. An In-Sync Replica (ISR) is a replica that is up
to date with the partition's leader.


10. What do you understand about a consumer group in Kafka?

A consumer group in Kafka is a collection of consumers who work together to ingest data from the same topic or range of topics.
The name of an application is essentially represented by a consumer group. Consumers in Kafka often fall into one of several categories.
The ‚Äò-group' command must be used to consume messages from a consumer group. 

11. What is the maximum size of a message that Kafka can receive?
By default, the maximum size of a Kafka message is 1MB (megabyte). The broker settings allow you to modify the size. 
Kafka, on the other hand, is designed to handle 1KB messages as well.

12. What are some of the features of Kafka?
Following are the key features of Kafka:-
Kafka is a messaging system built for high throughput and fault tolerance.
Kafka has a built-in patriation system known as a Topic.
Kafka Includes a replication feature as well.
Kafka provides a queue that can handle large amounts of data and move messages from one sender to another.
Kafka can also save the messages to storage and replicate them across the cluster.
For coordination and synchronization with other services, Kafka collaborates with Zookeeper.
Apache Spark is well supported by Kafka.


13. How do you start a Kafka server?
Firstly, we extract Kafka once we have downloaded the most recent version. We must make sure that our local environment has Java 8+ 
installed in order to run Kafka.
The following commands must be done in order to start the Kafka server and ensure that all services are started in the correct order:
Start the ZooKeeper service by doing the following:
$bin/zookeeper-server-start.sh config/zookeeper.properties
To start the Kafka broker service, open a new terminal and type the following commands:
$ bin/kafka-server-start.sh config/server.properties

14. What do you mean by geo-replication in Kafka?
Geo-Replication is a Kafka feature that allows messages in one cluster to be copied across many data centers or cloud regions.
Geo-replication entails replicating all of the files and storing them throughout the globe if necessary.
Geo-replication can be accomplished with Kafka's MirrorMaker Tool. Geo-replication is a technique for ensuring data backup.

15. What are some of the disadvantages of Kafka?

Following are the disadvantages of Kafka :
Kafka performance degrades if there is message tweaking. When the message does not need to be updated, Kafka works well.
Wildcard topic selection is not supported by Kafka. It is necessary to match the exact topic name.
Brokers and consumers reduce Kafka's performance when dealing with huge messages by compressing and decompressing the messages.
This has an impact on Kafka's throughput and performance.
Certain message paradigms, including point-to-point queues and request/reply, are not supported by Kafka.
Kafka does not have a complete set of monitoring tools.

16. Tell me about some of the real-world usages of Apache Kafka.

Following are some of the real-world usages of Apache Kafka:
As a Message Broker: Due to its high throughput value, Kafka is capable of managing a huge amount of comparable types of messages or data.
Kafka can be used as a publish-subscribe messaging system that allows data to be read and published in a convenient manner.
To Monitor operational data: Kafka can be used to keep track of metrics related to certain technologies, such as security logs.
Website activity tracking: Kafka can be used to check that data is transferred and received successfully by websites.
Kafka can handle the massive amounts of data created by websites for each page and for the activities of users.
Data logging: Kafka's data replication between nodes functionality can be used to restore data on nodes that have failed.
Kafka may also be used to collect data from a variety of logs and make it available to consumers.
Stream Processing with Kafka: Kafka may be used to handle streaming data, which is data that is read from one topic, processed,
and then written to another. Users and applications will have access to a new topic containing the processed data.


17. What are the use cases of Kafka monitoring?
Following are the use cases of Kafka monitoring :

Track System Resource Consumption: It can be used to keep track of system resources such as memory, CPU, and disk utilization over time.
Monitor threads and JVM usage: Kafka relies on the Java garbage collector to free up memory, ensuring that it runs frequently thereby
guaranteeing that the Kafka cluster is more active.
Keep an eye on the broker, controller, and replication statistics so that the statuses of partitions and replicas can be modified
as needed.
Finding out which applications are causing excessive demand and identifying performance bottlenecks might help solve performance issues
rapidly. 

18. What do you mean by Kafka schema registry?

 A Schema Registry is present for both producers and consumers in a Kafka cluster, and it holds Avro schemas.
For easy serialization and de-serialization, Avro schemas enable the configuration of compatibility parameters between producers 
and consumers. The Kafka Schema Registry is used to ensure that the schema used by the consumer and the schema used by the producer
are identical. The producers just need to submit the schema ID and not the whole schema when using the Confluent schema registry in Kafka.
The consumer looks up the matching schema in the Schema Registry using the schema ID.



19. What are the benefits of using clusters in Kafka?
Kafka cluster is basically a group of multiple brokers. They are used to maintain load balance. Because Kafka brokers are stateless,
they rely on Zookeeper to keep track of their cluster state. A single Kafka broker instance can manage hundreds of thousands of reads
and writes per second, and each broker can handle TBs of messages without compromising performance.
Zookeeper can be used to choose the Kafka broker leader. Thus having a cluster of Kafka brokers heavily increases the performance.


20. Describe partitioning key in Kafka.
In Kafka terminology, messages are referred to as records. Each record has a key and a value, with the key being optional.
For record partitioning, the record's key is used. There will be one or more partitions for each topic.
Partitioning is a straightforward data structure. It's the append-only sequence of records, which is arranged chronologically 
by the time they were attached. Once a record is written to a partition, it is given an offset ‚Äì a sequential id that
reflects the record's position in the partition and uniquely identifies it inside it.

Partitioning is done using the record's key. By default, Kafka producer uses the record's key to determine which partition the
record should be written to. The producer will always choose the same partition for two records with the same key. 

This is important because we may have to deliver records to customers in the same order that they were made.
You want these events to come in the order they were created when a consumer purchases an eBook from your webshop and 
subsequently cancels the transaction. If you receive a cancellation event before a buy event, the cancellation will be rejected
as invalid (since the purchase has not yet been registered in the system), and the system will then record the purchase and send
the product to the client (and lose you money). You might use a customer id as the key of these Kafka records to solve this problem 
and assure ordering. This will ensure that all of a customer's purchase events are grouped together in the same partition.

21. What is the purpose of partitions in Kafka?
Partitions allow a single topic to be partitioned across numerous servers from the perspective of the Kafka broker.
This allows you to store more data in a single topic than a single server can. If you have three brokers and need to store 10TB
of data in a topic, one option is to construct a topic with only one partition and store all 10TB on one broker.
Another alternative is to build a three-partitioned topic and distribute 10 TB of data among all brokers. 
A partition is a unit of parallelism from the consumer's perspective.


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Kafka Interview Questions for Experienced


1. Tell me about some of the use cases where Kafka is not suitable.

Following are some of the use cases where Kafka is not suitable :

Kafka is designed to manage large amounts of data. Traditional messaging systems would be more appropriate if only a small number of messages need to be processed every day.
Although Kafka includes a streaming API, it is insufficient for executing data transformations. For ETL (extract, transform, load) jobs, Kafka should be avoided.
There are superior options, such as RabbitMQ, for scenarios when a simple task queue is required.
If long-term storage is necessary, Kafka is not a good choice. It simply allows you to save data for a specific retention period and no longer.


2. What is a Replication Tool in Kafka? Explain some of the replication tools available in Kafka.

The Kafka Replication Tool is used to create a high-level design for the replica maintenance process. The following are some of the replication tools available:

Preferred Replica Leader Election Tool: Partitions are spread to many brokers in a cluster, each copy known as a replica, using the Preferred Replica Leader Election Tool. The leader is frequently referred to as the favored replica. The brokers normally spread the leader position equitably across the cluster for various partitions, but owing to failures, planned shutdowns, and other factors, an imbalance can develop over time. This tool can be used to preserve the balance in these situations by reassigning the preferred replicas, and hence the leaders.
Topics tool: The Kafka topics tool is in charge of all administration operations relating to topics, including:
Listing and describing the topics.
Topic generation.
Modifying Topics.
Adding a topic's dividers.
Disposing of topics.
Tool to reassign partitions: The replicas assigned to a partition can be changed with this tool. This refers to adding or removing followers from a partition.
StateChangeLogMerger tool: The StateChangeLogMerger tool collects data from brokers in a cluster, formats it into a central log, and aids in the troubleshooting of state change issues. Sometimes there are issues with the election of a leader for a particular partition. This tool can be used to figure out what's causing the issue.
Change topic configuration tool: used to create new configuration choices, modify current configuration options, and delete configuration options.

3. Differentiate between Rabbitmq and Kafka.

 Following are the differences between Kafka and Rabbitmq:

Based on Architecture :

Rabbitmq: 

Rabbitmq is a general-purpose message broker and request/reply, point-to-point, and pub-sub communication patterns are all used by it.
It has a smart broker/ dumb consumer model. There is the consistent transmission of messages to consumers at about the same speed as the broker monitors the consumer's status.
It is a mature platform and is well supported for Java, client libraries, .NET, Ruby, and Node.js. It offers a variety of plugins as well.
The communication can be synchronous or asynchronous. It also provides options for distributed deployment.
Kafka:

Kafka is a message and stream platform for high-volume publish-subscribe messages and streams. It is durable, quick, and scalable.
It is a durable message store, similar to a log, and it runs in a server cluster and maintains streams of records in topics (categories).
In this, messages are made up of three components: a value, a key, and a timestamp.
It has a dumb broker / smart consumer model as it does not track which messages are viewed by customers and only maintains unread messages. Kafka stores all messages for a specific amount of time.
In this, external services are required to run, including Apache Zookeeper in some circumstances.
Manner of Handling Messages :

Basis	Rabbitmq	Kafka
Ordering of messages	The ordering of messages is not supported here. 	Partitions in Kafka enable message ordering. Message keys are used while sending the messages to the topic.
Lifetime of messages	Since Rabbitmq is a message queue, messages are done away with once consumed and the acknowledgement is sent.	Since Kafka is a log, the messages are always present there. We can have a message retention policy for the same.
Prioritizing the messages 	In this, priorities can be specified for the messages and the messages can be consumed according to their priority.	Prioritising the messages is not possible in  Kafka.
Guarantee of delivering the messages 	Atomicity is not guaranteed in this case, even when the transaction involves a single queue.	In Kafka, it is guaranteed that the whole batch of messages in a partition is either sent successfully or failed.
Based on Approach :

Kafka: The pull model is used by Kafka. Batches of messages from a given offset are requested by consumers. When there are no messages past the offset, Kafka allows for long-pooling, which eliminates tight loops.
Because of Kafka's partitions, a pull model makes sense. In a partition with no competing customers, Kafka provides message orders. This allows users to take advantage of message batching for more efficient message delivery and higher throughput.
Rabbitmq: RabbitMQ operates on a push paradigm, which prevents users from becoming overwhelmed by imposing a prefetch limit on them. This can be used for messaging with low latency. The push model's goal is to distribute messages individually and promptly, ensuring that work is parallelized equitably and messages are handled roughly in the order they came in the queue.
Based on Performance:

Kafka: Compared to message brokers like RabbitMQ, Kafka provides significantly better performance. It boosts performance by using sequential disc I/O, making it a good choice for queue implementation. With limited resources, it can achieve high throughput (millions of messages per second), which is essential for large data use cases.
Rabbitmq: RabbitMQ can also handle a million messages per second, but it does so at the expense of more resources (around 30 nodes). RabbitMQ can be used for many of the same applications as Kafka, however, it must be used in conjunction with other technologies such as Apache Cassandra.


4. What are the parameters that you should look for while optimising kafka for optimal performance?
Two major measurements are taken into account while tuning for optimal performance: latency measures, which relate to the amount of time it takes to process one event, and throughput measures, which refer to the number of events that can be processed in a given length of time. Most systems are tuned for one of two things: delay or throughput, whereas Kafka can do both. 

The following stages are involved in optimizing Kafka's performance:

Kafka producer tuning: Data that producers must provide to brokers is kept in a batch. The producer transmits the batch to the broker when it's ready. To adjust the producers for latency and throughput, two parameters must be considered: batch size and linger time. The batch size must be chosen with great care. If the producer is constantly delivering messages, a bigger batch size is recommended to maximize throughput. However, if the batch size is set to a huge value, it may never fill up or take a long time to do so, affecting the latency. The batch size must be selected based on the nature of the volume of messages transmitted by the producer. The linger duration is included to create a delay while more records are added to the batch, allowing for larger records to be transmitted. More messages can be transmitted in one batch with a longer linger period, but latency may suffer as a result. A shorter linger time, on the other hand, will result in fewer messages being transmitted faster, resulting in lower latency but also lower throughput.
Tuning the Kafka broker: Each partition in a topic has a leader, and each leader has 0 or more followers. It's critical that the leaders are appropriately balanced, and that some nodes aren't overworked in comparison to others.
Tuning Kafka Consumers: To ensure that consumers keep up with producers, the number of partitions for a topic should be equal to the number of consumers. The divisions are divided among the consumers in the same consumer group.

5. Differentiate between Redis and Kafka.

The following table illustrates the differences between Redis and Kafka:

Redis	Kafka
Push-based message delivery is supported by Redis. This means that messages published to Redis will be distributed to consumers automatically.	Pull-based message delivery is supported by Kafka. The messages published to the Kafka broker are not automatically sent to the consumers; instead, consumers must pull the messages when they are ready.
Message retention is not supported by Redis. The communications are destroyed once they have been delivered to the recipients.	In its log, Kafka allows for message preservation.
Parallel processing is not supported by Redis.	Multiple consumers in a consumer group can consume partitions of the topic concurrently because of the Kafka's partitioning feature.
Redis can not manage vast amounts of data because it's an in-memory database.	Kafka can handle massive amounts of data since it uses disc space as its primary storage.
Because Redis is an in-memory store, it is much faster than Kafka.	Because Kafka stores data on disc, it is slower than Redis.


6. Describe in what ways Kafka enforces security.

The security given by Kafka is made up of three parts:
Encryption: All communications sent between the Kafka broker and its many clients are encrypted. This prevents data from being intercepted by other clients. All messages are shared in an encrypted format between the components.
Authentication: Before being able to connect to Kafka, apps that use the Kafka broker must be authenticated. Only approved applications will be able to send or receive messages. To identify themselves, authorized applications will have unique ids and passwords.
After authentication, authorization is carried out. It is possible for a client to publish or consume messages once it has been validated. The permission ensures that write access to apps can be restricted to prevent data contamination.
7. Differentiate between Kafka and Java Messaging Service(JMS).
The following table illustrates the differences between Kafka and Java Messaging Service:


Java Messaging Service(JMS)	Kafka
The push model is used to deliver the messages. Consumers receive messages on a regular basis.	A pull mechanism is used in the delivery method. When consumers are ready to receive the messages, they pull them.
When the JMS queue receives confirmation from the consumer that the message has been received, it is permanently destroyed.	Even after the consumer has viewed the communications, they are maintained for a specified length of time.
JMS is better suited to multi-node clusters in very complicated systems.	Kafka is better suited to handling big amounts of data.
JMS is a FIFO queue that does not support any other type of ordering.	Kafka ensures that partitions are sent in the order in which they appeared in the message.


8. What do you understand about Kafka MirrorMaker?
The MirrorMaker is a standalone utility for copying data from one Apache Kafka cluster to another. The MirrorMaker reads data from original cluster topics and writes it to a destination cluster with the same topic name. The source and destination clusters are separate entities that can have various partition counts and offset values.

9. Differentiate between Kafka and Flume.

Apache Flume is a dependable, distributed, and available software for aggregating, collecting, and transporting massive amounts of log data quickly and efficiently. Its architecture is versatile and simple, based on streaming data flows. It's written in the Java programming language. It features its own query processing engine, allowing it to alter each fresh batch of data before sending it to its intended sink. It is designed to be adaptable.


The following table illustrates the differences between Kafka and Flume :

Kafka	Flume
Kafka is a distributed data system.	Apache Flume is a system that is available, dependable, and distributed.
It essentially functions as a pull model.	It essentially functions as a push model.
It is made for absorbing and analysing real-time streaming data.	It collects, aggregates, and moves massive amounts of log data from a variety of sources to a centralised data repository in an efficient manner.
If it is resilient to node failure, it facilitates automatic recovery.	If the flume-agent fails, you will lose events in the channel.
Kafka operates as a cluster that manages incoming high-volume data streams in real-time.	Flume is a tool for collecting log data from web servers that are spread.
It is a messaging system that is fault-tolerant, efficient, and scalable.	It is made specifically for Hadoop.
It's simple to scale.	In comparison to Kafka, it is not scalable.

10. What do you mean by confluent kafka? What are its advantages?

Confluent is an Apache Kafka-based data streaming platform: a full-scale streaming platform capable of not just publish-and-subscribe but also data storage and processing within the stream. Confluent Kafka is a more comprehensive Apache Kafka distribution. It enhances Kafka's integration capabilities by including tools for optimizing and managing Kafka clusters, as well as ways for ensuring the streams' security. Kafka is easy to construct and operate because of the Confluent Platform. Confluent's software comes in three varieties: 

A free, open-source streaming platform that makes it simple to get started with real-time data streams;
An enterprise-grade version with more administration, operations, and monitoring tools;
A premium cloud-based version.
Following are the advantages of Confluent Kafka :

It features practically all of Kafka's characteristics, as well as a few extras.
It greatly simplifies the administrative operations procedures.
It relieves data managers of the burden of thinking about data relaying.

11. Describe message compression in Kafka. What is the need of message compression in Kafka? Also mention if there are any disadvantages of it.

Producers transmit data to brokers in JSON format in Kafka. The JSON format stores data in string form, which can result in several duplicate records being stored in the Kafka topic. As a result, the amount of disc space used increases. As a result, before delivering messages to Kafka, compression or delaying of data is performed to save disk space. Because message compression is performed on the producer side, no changes to the consumer or broker setup are required. 

It is advantageous because of the following factors:

It decreases the latency of messages transmitted to Kafka by reducing their size.
Producers can send more net messages to the broker with less bandwidth.
When data is saved in Kafka using cloud platforms, it can save money in circumstances where cloud services are paid.
Message compression reduces the amount of data stored on disk, allowing for faster read and write operations.
Message Compression has the following disadvantages :

Producers must use some CPU cycles to compress their work.
Decompression takes up several CPU cycles for consumers.
Compression and decompression place a higher burden on the CPU.

12. What do you mean by multi-tenancy in Kafka?

Multi-tenancy is a software operation mode in which many instances of one or more programs operate in a shared environment independently of one another. The instances are considered to be physically separate yet logically connected. The level of logical isolation in a system that supports multi-tenancy must be comprehensive, but the level of physical integration can vary. Kafka is multi-tenant because it allows for the configuration of many topics for data consumption and production on the same cluster.

13. What do you understand about log compaction and quotas in Kafka?

Log compaction is a way through which Kafka assures that for each topic partition, at least the last known value for each message key within the log of data is kept. This allows for the restoration of state following an application crash or a system failure. During any operational maintenance, it allows refreshing caches after an application restarts. Any consumer processing the log from the beginning will be able to see at least the final state of all records in the order in which they were written, because of the log compaction.

A Kafka cluster can apply quotas on producers and fetch requests as of Kafka 0.9. Quotas are byte-rate limits that are set for each client-id. A client-id is a logical identifier for a request-making application. A single client-id can therefore link to numerous producers and client instances. The quota will be applied to them all as a single unit. Quotas prevent a single application from monopolizing broker resources and causing network saturation by consuming extremely large amounts of data.

14. What are the guarantees that Kafka provides?
Following are the guarantees that Kafka assures :

The messages are displayed in the same order as they were published by the producers. The order of the messages is maintained.
The replication factor determines the number of replicas. If the replication factor is n, the Kafka cluster has fault tolerance for up to n-1 servers.
Per partition, Kafka can provide "at least one" delivery semantics. This means that if a partition is given numerous times, Kafka assures that it will reach a customer at least once.

15. What do you mean by an unbalanced cluster in Kafka? How can you balance it?

It's as simple as assigning a unique broker id, listeners, and log directory to the server.properties file to add new brokers to an existing Kafka cluster. However, these brokers will not be allocated any data partitions from the cluster's existing topics, so they won't be performing much work unless the partitions are moved or new topics are formed. A cluster is referred to as unbalanced if it has any of the following problems :

Leader Skew: 

Consider the following scenario: a topic with three partitions and a replication factor of three across three brokers. 


The leader receives all reads and writes on a partition. Followers send fetch requests to the leaders in order to receive their most recent messages. Followers exist solely for redundancy and fail-over purposes.

Consider the case of a broker who has failed. It's possible that the failed broker was a collection of numerous leader partitions. Each unsuccessful broker's leader partition is promoted as the leader by its followers on the other brokers. Because fail-over to an out-of-sync replica is not allowed, the follower must be in sync with the leader in order to be promoted as the leader.

If another broker goes down, all of the leaders are on the same broker, therefore there is no redundancy.

When both brokers 1 and 3 go live, the partitions gain some redundancy, but the leaders stay focused on broker 2.


As a result, the Kafka brokers have a leader imbalance. When a node is a leader for more partitions than the number of partitions/number of brokers, the cluster is in a leader skewed condition.

Solving the leader skew problem:

Kafka offers the ability to reassign leaders to the desired replicas in order to tackle this problem. This can be accomplished in one of two ways:

The auto.leader.rebalance.enable=true broker option allows the controller node to transfer leadership to the preferred replica leaders, restoring the even distribution.
When Kafka-preferred-replica-election.sh is run, the preferred replica is selected for all partitions: The utility requires a JSON file containing a mandatory list of zookeeper hosts and an optional list of topic partitions. If no list is provided, the utility uses a zookeeper to retrieve all of the cluster's topic partitions. The Kafka-preferred-replica-election.sh utility can be time-consuming to use. Custom scripts can render only the topics and partitions that are required, automating the process across the cluster.
Broker Skew:

Let us consider a Kafka cluster with nine brokers. Let the topic name be "sample_topic." The following is how the brokers are assigned to the topic in our example:

On brokers 3,4 and 5, the topic ‚Äúsample_topic‚Äù is skewed. This is because if the number of partitions per broker on a given issue is more than the average, the broker is considered to be skewed.

Solving the broker skew problem :

The following steps can be used to solve it:

Generate the candidate assignment configuration using the partition reassignment tool (Kafka-reassign-partition.sh) with the ‚Äìgenerate option. The current and intended replica allocations are shown here.
Create a JSON file with the suggested assignment.
To update the metadata for balancing, run the partition reassignment tool.
Run the ‚ÄúKafka-preferred-replica-election.sh‚Äù tool to complete the balancing after the partition reassignment is complete.

16. How will you expand a cluster in Kafka?

To add a server to a Kafka cluster, it only needs to be given a unique broker id and Kafka must be started on that server. However, until a new topic is created, a new server will not be given any of the data partitions. As a result, when a new machine is introduced to the cluster, some existing data must be migrated to these new machines. To relocate some partitions to the new broker, we use the partition reassignment tool. Kafka will make the new server a follower of the partition it is migrating to, allowing it to replicate the data on that partition completely. When all of the data has been duplicated, the new server can join the ISR, and one of the current replicas will erase the data it has for that partition.  

17. What do you mean by graceful shutdown in Kafka?

The Apache cluster will automatically identify any broker shutdown or failure. In this instance, new leaders for partitions previously handled by that device will be chosen. This can happen as a result of a server failure or even if it is shut down for maintenance or configuration changes. When a server is taken down on purpose, Kafka provides a graceful method for terminating the server rather than killing it.

When a server is switched off:

To prevent having to undertake any log recovery when Kafka is restarted, it ensures that all of its logs are synced onto a disk. Because log recovery takes time, purposeful restarts can be sped up.
Prior to shutting down, all partitions for which the server is the leader will be moved to the replicas. The leadership transfer will be faster as a result, and the period each partition is inaccessible will be decreased to a few milliseconds.
18. Can the number of partitions for a topic be changed in Kafka?
Currently, Kafka does not allow you to reduce the number of partitions for a topic. The partitions can be expanded but not shrunk. The alter command in Apache Kafka allows you to change the behavior of a topic and its associated configurations. To add extra partitions, use the alter command.

To increase the number of partitions to five, use the following command:

./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic sample-topic --partitions 5
19. What do you mean by BufferExhaustedException and OutOfMemoryException in Kafka?
When the producer can't assign memory to a record because the buffer is full, a BufferExhaustedException is thrown. If the producer is in non-blocking mode, and the rate of production exceeds the rate at which data is transferred from the buffer for long enough, the allocated buffer will be depleted, the exception will be thrown.

If the consumers are sending huge messages or if there is a spike in the number of messages sent at a rate quicker than the rate of downstream processing, an OutOfMemoryException may arise. As a result, the message queue fills up, consuming memory space.

20. How will you change the retention time in Kafka at runtime?

A topic's retention time can be configured in Kafka. A topic's default retention time is seven days. While creating a new subject, we can set the retention time. When a topic is generated, the broker's property log.retention.hours are used to set the retention time. When configurations for a currently operating topic need to be modified, kafka-topic.sh must be used.

The right command is determined on the Kafka version in use.

The command to use up to 0.8.2 is kafka-topics.sh --alter.
Use kafka-configs.sh --alter starting with version 0.9.0.

21. Differentiate between Kafka streams and Spark Streaming.

Kafka Streams	Spark Streaming
Kafka is fault-tolerant because of partitions and their replicas.	Using Cache and RDD (Resilient Distributed Dataset), Spark can restore partitions.
It is only capable of handling real-time streams	It is capable of handling both real-time and batch tasks.
Messages in the Kafka log are persistent.	To keep the data durable, you'll need to utilize a dataframe or another data structure.
There are no interactive modes in Kafka. The data from the producer is simply consumed by the broker, who then waits for the client to read it.	Interactive modes are available.
22. What are Znodes in Kafka Zookeeper? How many types of Znodes are there?
The nodes in a ZooKeeper tree are called znodes. Version numbers for data modifications, ACL changes, and timestamps are kept by Znodes in a structure. ZooKeeper uses the version number and timestamp to verify the cache and guarantee that updates are coordinated. Each time the data on Znode changes, the version number connected with it grows.

There are three different types of Znodes:


Persistence Znode: These are znodes that continue to function even after the client who created them has been disconnected. Unless otherwise specified, all znodes are persistent by default.
Ephemeral Znode: Ephemeral znodes are only active while the client is still alive. When the client who produced them disconnects from the ZooKeeper ensemble, the ephemeral Znodes are automatically removed. They have a significant part in the election of the leader.
Sequential Znode: When znodes are constructed, the ZooKeeper can be asked to append an increasing counter to the path's end. The parent znode's counter is unique. Sequential nodes can be either persistent or ephemeral.
Conclusion:
In this article, we discussed the most frequently asked interview questions on Kafka. It should be clear why Kafka is such an effective streaming platform. Kafka is a useful solution for scenarios that require real-time data processing, application activity tracking, and monitoring. At the same time, Kafka should not be utilized for on-the-fly data conversions, data storage, or when a simple task queue is all that is required.




