Here's a deep dive into Indexing, Partitioning, and Sharding â€” key concepts for optimizing data storage and retrieval in databases and distributed systems.

ðŸ“Œ 1. Indexing
âœ… What is Indexing?
Indexing is a data structure (usually B-Trees or Hash tables) used to quickly locate data without scanning the entire table.

ðŸ” Analogy
Like an index in a book: instead of flipping every page, you jump to the exact page.

ðŸ”§ How It Works
Instead of scanning rows for a match:

Index maps column values to row locations

Index lookup is O(log N) (for B-Tree) vs. O(N) for full scan

horizontal or vertical indexing based on rows or columns

ðŸ’¡ Types of Indexes
| Type              | Description                                               |
| ----------------- | --------------------------------------------------------- |
| **Single-column** | Index on one column                                       |
| **Composite**     | Index on multiple columns (e.g., `INDEX(col1, col2)`)     |
| **Unique**        | Ensures all indexed values are unique                     |
| **Full-Text**     | Supports text search                                      |
| **Bitmap**        | Efficient for columns with low cardinality (e.g., gender) |


âš ï¸ Trade-offs
Speeds up reads

Slows down writes (insert/update/delete)

Consumes additional storage

ðŸ“Œ 2. Partitioning
âœ… What is Partitioning?
Partitioning is splitting a single large table into smaller logical pieces (partitions), but still within the same database/server.

ðŸ§© Why Partition?
Improve query performance

Simplify archival and purging

Better data management and indexing

horizontal or vertical partitioning based on rows or columns

ðŸ§± Types of Partitioning

| Type                       | Description               | Example                          |
| -------------------------- | ------------------------- | -------------------------------- |
| **Range Partitioning**     | Based on a value range    | Dates (`2010â€“2020`, `2020â€“2030`) |
| **List Partitioning**      | Based on explicit values  | Country = â€˜INâ€™, â€˜USâ€™, â€˜UKâ€™       |
| **Hash Partitioning**      | Based on hash of a key    | `hash(user_id) % N`              |
| **Composite Partitioning** | Combines above strategies | Range + Hash                     |


âš ï¸ Partitioning is still local
All data is in the same database instance, not across multiple servers.

ðŸ“Œ 3. Sharding
âœ… What is Sharding?
Sharding is horizontal partitioning across multiple physical databases or servers â€” each shard holds a subset of data.

âš¡ Why Shard?
To handle massive scale:

Break data into independent pieces

Spread load across multiple machines

Avoid single-node bottlenecks

ðŸ”„ Shard Key
The column/attribute used to determine which shard a record belongs to.

horizontal or vertical sharding based on rows or columns

ðŸ“¦ Types of Sharding


| Type                         | Description                                                                            | Example                                     |
| ---------------------------- | -------------------------------------------------------------------------------------- | ------------------------------------------- |
| **Horizontal Sharding**      | Each shard holds different rows (users A-M on one shard, N-Z on another)               | Most common                                 |
| **Vertical Sharding**        | Different shards hold different columns (user profile on one, transactions on another) | Rare                                        |
| **Directory-based Sharding** | A lookup table maps records to shards                                                  | More flexible, but needs a metadata service |



âš–ï¸ Considerations
Query routing: The system must know where to find the data.

Resharding is complex (when load grows or skew happens).

Joins across shards are hard and slow.

High availability and replication are needed per shard.

Consistency is needed

Consistent Hashing is needed to solve the problem of increasing or decreasing the size of shards
Memcached + consistent hashing can solve it. 
Memcached is for larger datasets while Redis works well is higher performant with smaller datasets.

Multi level sharding solves the problem of fixed no of shards
create index on each shard
Master slave to avoid single point of failure

ðŸ†š Comparison Table

| Feature                     | Indexing                   | Partitioning              | Sharding             |
| --------------------------- | -------------------------- | ------------------------- | -------------------- |
| Purpose                     | Speed up lookups           | Organize large tables     | Scale across servers |
| Scope                       | Within a table             | Within a single DB        | Across multiple DBs  |
| Performance                 | Speeds up reads            | Improves query efficiency | Distributes workload |
| Use Case                    | Frequently queried columns | Large fact tables         | Large-scale web apps |
| Involves Multiple Machines? | âŒ                          | âŒ                         | âœ…                    |


ðŸ§  Summary
Indexing = Accelerates search within data (like lookup table , index and address pointers of rows/ columns in case of columnar db)

Partitioning = Logical breakup of a table in one DB ( like 5 rows partition in row db or 5 columns partition on columnar db based on date range may  be )

Sharding = Physical breakup of data across multiple DB instanc or machine. ( geographical division of DB instances )
load balancing on shards need to be performed

âœ… Best Practices
Indexing
Index columns used in WHERE, JOIN, ORDER BY.

Avoid over-indexing â€” adds write overhead.

Use covering indexes (includes all columns in SELECT).

Partitioning
Choose partition key wisely â€” avoid skew.

Maintain partition pruning for performance.

Archive or drop old partitions easily.

Sharding
Pick a good shard key (high cardinality, uniform access).

Avoid cross-shard JOINs and transactions.

Plan for resharding from the start.


ðŸ“Œ 1. Indexing â€“ Internal Implementation
ðŸ”§ Data Structures Used
Most databases use:

B+ Trees (e.g., PostgreSQL, MySQL InnoDB)

Hash Tables (e.g., for exact matches, not range queries)

Inverted Index (for full-text search)

ðŸ§  How It Works Internally
ðŸŸ¦ B+ Tree Index
Index pages (nodes) store keys in a sorted order.

Leaf nodes point to actual data locations (row pointers or disk pages).

Insert/Search/Delete operations navigate the tree to maintain balance (O(log n) time).

During query planning, the optimizer checks if indexes can be used:

If so, index is traversed.

Otherwise, a full table scan is done.

ðŸŸ¨ Maintenance
When a row is inserted/updated, the corresponding index is updated.

Can cause page splits or rebalancing in the tree.

Hence, indexes slow down writes and require vacuuming/defragmentation (e.g., VACUUM in PostgreSQL).

ðŸ“Œ 2. Partitioning â€“ Internal Implementation
Partitioning divides a logical table into physical partitions, but the query engine still sees it as one table.

ðŸ”§ Implementation Internals
Metadata Layer stores mapping of:

Partition key ranges or lists.

Physical location of each partition (files or disk pages).

During a query:

The planner uses the partition key predicate to prune partitions not needed.

Only relevant partitions are scanned (called partition elimination).

Each partition is often implemented as a separate table or file internally.

ðŸ”„ Insert Path
For every insert, the partitioning engine:

Determines the target partition based on the key.

Inserts into the physical table that represents that partition.

ðŸ›  Physical Storage
May use separate:

Heap files or tablespaces (PostgreSQL)

File groups (SQL Server)

Files on disk (MySQL Partitioned Table â†’ separate .ibd files)


ðŸ“Œ Note: If range based partitioning , binary search is used check which partition to fall in based on query
or hashing is used or lookup table to find the partition


ðŸ“Œ 3. Sharding â€“ Internal Implementation
Sharding is implemented above the storage layer, often in a routing and coordination layer that handles:

ðŸ”§ Core Components
Router / Query Router:

Routes queries to the correct shard based on a shard key.

Example: mongos in MongoDB, vtgate in Vitess.

Metadata Store / Config Server:

Stores the mapping of:

Shard keys to ranges or hash slots

Which server stores what data

Used during query planning and routing

Shard Nodes:

Each shard runs an independent DB engine (e.g., MongoDB instance or MySQL replica set).

Data in shards is stored using traditional storage (B-Trees, heap files, etc.)

ðŸ§  Data Distribution Algorithms
Strategy	How It's Done
Range-based sharding	Map ranges of shard key values to shard servers. E.g., Aâ€“M â†’ Shard 1, Nâ€“Z â†’ Shard 2
Hash-based sharding	Compute hash(key) % N to assign data to shard
Directory-based sharding	Use a central mapping table for every key (less scalable, more flexible)

ðŸ”„ Query Flow
Query arrives at router

Router checks config server to find shard(s) responsible for the key

Query is forwarded to shard

Shard executes it using local indexes/storage

Results are optionally merged and returned

| Feature          | Storage Level | Planning Layer   | Routing | Disk Layout                 |
| ---------------- | ------------- | ---------------- | ------- | --------------------------- |
| **Indexing**     | Per table     | Query planner    | âŒ       | B+ Tree, Hash               |
| **Partitioning** | Single DB     | Partition mapper | âŒ       | Files/tables per partition  |
| **Sharding**     | Across DBs    | Router (global)  | âœ…       | Separate DB instances/files |




ðŸ“¦ Example: PostgreSQL Partitioning Internals
pg_class holds table metadata

pg_inherits maps child partitions to parent

Each partition = separate heap table

Query planner uses constraint exclusion to pick relevant partitions

ðŸ“¦ Example: MongoDB Sharding Internals
config.shards = shard server list

config.chunks = shard key ranges and their shards

mongos uses this metadata to route each query

mongod stores data using B-Trees



if there are heavy read operations , then make replicas as storage is cheap we need to optimise time complexity
this will load balance

so a combination sharding, partitioning and indexing => to make  dataset small

Sometimes , if complex queries are there, we have to make intermediate tables
and keep tables de-normalised . and refresh those intermediate tables.
or make better DB choice like colummar db or document db or graph db or rdbms


